{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hjhr-eT8bzyp"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StevenLevine-NOAA/NBM-Verif/blob/notebooks/MDL_Internal_NBM_Reliability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NBM Reliability**\n",
        "This python notebook grabs NBM data as well as station data, and computes reliability for NBM PMaxT, PMinT, PMaxWind, PMaxGust or PQPF thresholds. Note, in the case of PQPF, conditional reliability is generated (stats only computed when precipitation was observed, because reasons, mainly the difficulty of ranking a 0 ob)."
      ],
      "metadata": {
        "id": "TcVdYIIebz_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1 - Install & Import Packages**\n",
        "This section will install and import all the packages we need.\n",
        "\n",
        "This section only needs to run once!"
      ],
      "metadata": {
        "id": "hjhr-eT8bzyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "id": "NfI_qD1eZnwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: After running cell 1, you will get a \"session crashed\" error message.  That's OK and can be ignored.  Just wait a few seconds before running cell 2"
      ],
      "metadata": {
        "id": "yRlZIeMKnLIW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1Z21rU-aFwM"
      },
      "outputs": [],
      "source": [
        "!mamba install -q -c conda-forge cartopy contextily pyproj pyepsg pygrib netCDF4 shapely tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from urllib.request import urlretrieve, urlopen\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "import math\n",
        "#!pip install pygrib\n",
        "import pygrib\n",
        "from netCDF4 import Dataset\n",
        "import requests\n",
        "import os, re, traceback\n",
        "\n",
        "import matplotlib\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.axes as maxes\n",
        "import matplotlib.patheffects as PathEffects\n",
        "from matplotlib.path import Path\n",
        "from matplotlib.textpath import TextToPath\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib.font_manager import FontProperties\n",
        "matplotlib.rcParams['font.sans-serif'] = 'Liberation Sans'\n",
        "matplotlib.rcParams['font.family'] = \"sans-serif\"\n",
        "from sklearn.calibration import calibration_curve\n",
        "import seaborn as sns\n",
        "from pyproj import Proj, transform as pyproj_transform\n",
        "\n",
        "import itertools\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#!pip install tqdm\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "#!pip uninstall shapely --yes\n",
        "#!pip install shapely cartopy==0.19.0.post1 --no-binary shapely --no-binary cartopy\n",
        "from cartopy import crs as ccrs, feature as cfeature"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2 - Edit Form Options and Go!**\n",
        "This is where you set the options. This is the last and only cell you really should have to edit. I recommend testing with a two days to make sure your output is what you expect before doing anything crazy, like queueing up and entire month. Your plots will appear inline, but plots and csvs are also available for download by clicking the folder icon in the bar on the left. It may be better to download this and run it locally if you can, since it can take some time if you selected a lot of dates and/or stations."
      ],
      "metadata": {
        "id": "lxQfbezhb4QK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Options Selection\n",
        "#@markdown Pick the element you want to look at\n",
        "element = \"maxwind\" #@param [\"maxt\", \"mint\", \"qpf\", \"maxwind\", \"maxgust\"]\n",
        "threshold = 11 #@param [\"- QPF -\", \"0.01\", \"0.10\", \"0.25\", \"0.50\", \"1.0\", \"2.0\", \"3.0\", \"4.0\", \"- MinT - \", \"-40\", \"-20\", \"0\", \"28\", \"32\", \"80\", \"- MaxT -\", \"0\", \"28\", \"32\", \"80\", \"90\", \"100\", \"110\", \"120\", \"- MaxWind/Gust -\", \"11\", \"17\", \"22\", \"34\", \"41\", \"48\", \"56\", \"64\"]{type:\"raw\"}\n",
        "#@markdown -------\n",
        "#@markdown Pick a start date and end date\n",
        "start_date = \"2023-08-01\" #@param {type:\"date\"}\n",
        "end_date = \"2023-08-15\" #@param {type:\"date\"}\n",
        "#@markdown -------\n",
        "#@markdown Pick an NBM Lead time in days and QMD Initialization hour (choose carefully - suggested 00Z for Max and 12Z for Min). (3 = ~3-day/72-hr lead time)\n",
        "nbm_lead_time = 2 #@param {type:\"number\"}\n",
        "nbm_init_hour = 12 #@param {type:\"slider\", min:0, max:18, step:6}\n",
        "#@markdown -------\n",
        "#@markdown Pick a region and what you want to compare to the probabilities - NBM Determinstic or Observations\n",
        "region_selection = \"CWA\" #@param [\"WR\", \"SR\", \"CR\", \"ER\", \"CONUS\",\"CWA\"]\n",
        "#@markdown If CWA selected, which one? (i.e. \"SLC\" for Salt Lake City)\n",
        "cwa_id = \"LWX\" #@param {type:\"string\"}\n",
        "cwa_outline = True #@param {type:\"boolean\"}\n",
        "\n",
        "threshold= float(threshold)\n",
        "\n",
        "if os.path.exists(\"nbm\"):\n",
        "  pass\n",
        "else:\n",
        "  os.system('mkdir nbm')\n",
        "\n",
        "if os.path.exists(\"obs\"):\n",
        "  pass\n",
        "else:\n",
        "  os.system('mkdir obs')\n",
        "\n",
        "if os.path.exists(\"csv\"):\n",
        "  pass\n",
        "else:\n",
        "  os.system('mkdir csv')\n",
        "\n",
        "if element == \"maxt\":\n",
        "  if threshold <= 32:\n",
        "    percentile_stat = \"nonexceedance\"\n",
        "    temp = \"ColdMaxT\"\n",
        "  elif threshold >= 80:\n",
        "    percentile_stat = \"exceedance\"\n",
        "    temp = \"HotMaxT\"\n",
        "\n",
        "elif element == \"mint\":\n",
        "  if threshold <= 32:\n",
        "    percentile_stat = \"nonexceedance\"\n",
        "    temp = \"ColdMinT\"\n",
        "  elif threshold >= 80:\n",
        "    percentile_stat = \"exceedance\"\n",
        "    temp= \"HotMinT\"\n",
        "elif element == \"qpf\" or element == \"maxwind\" or element == \"maxgust\":\n",
        "  percentile_stat = \"exceedance\"\n",
        "\n",
        " #@markdown Substitute precip obs with Stage IV values (useful in frozen precip)?\n",
        "use_stageiv = False#@param {type:\"boolean\"}\n",
        "#@markdown Which obs?\n",
        "network_selection = \"NWS\" #@param [\"NWS\", \"RAWS\", \"NWS+RAWS\", \"NWS+RAWS+HADS\", \"ALL\", \"CUSTOM\", \"LIST\"]\n",
        "#@markdown If Custom or List selected for network, enter comma separated network IDs (custom) or siteids (list)  WITH NO SPACES here. For help - https://developers.synopticdata.com/about/station-providers/\n",
        "network_input = \"\"#@param {type:\"string\"}\n",
        "network_dict = {\"NWS+RAWS+HADS\":\"&network=1,2,106\",\"NWS+RAWS\":\"&network=1,2\", \"NWS\":\"&network=1\", \"RAWS\": \"&network=2\", \"ALL\":\"\", \"CUSTOM\": \"&network=\"+network_input, \"LIST\": \"&stid=\"+network_input}\n",
        "network_string = network_dict[network_selection]\n",
        "\n",
        "\n",
        "if region_selection == \"CONUS\":\n",
        "  region_list = [\"WR\", \"CR\", \"SR\", \"ER\"]\n",
        "elif region_selection == \"CWA\":\n",
        "  region_list = [cwa_id]\n",
        "else:\n",
        "  region_list = [region_selection]\n",
        "\n",
        "def cwa_list(input_region):\n",
        "  region_dict ={\"WR\":\"BYZ,BOI,LKN,EKA,FGZ,GGW,TFX,VEF,LOX,MFR,MSO,PDT,PSR,PIH,PQR,REV,STO,SLC,SGX,MTR,HNX,SEW,OTX,TWC\",\n",
        "              \"CR\":\"ABR,BIS,CYS,LOT,DVN,BOU,DMX,DTX,DDC,DLH,FGF,GLD,GJT,GRR,GRB,GID,IND,JKL,EAX,ARX,ILX,LMK,MQT,MKX,MPX,LBF,APX,IWX,OAX,PAH,PUB,UNR,RIW,FSD,SGF,LSX,TOP,ICT\",\n",
        "              \"ER\":\"ALY,LWX,BGM,BOX,BUF,BTV,CAR,CTP,RLX,CHS,ILN,CLE,CAE,GSP,MHX,OKX,PHI,PBZ,GYX,RAH,RNK,AKQ,ILM\",\n",
        "              \"SR\":\"ABQ,AMA,FFC,EWX,BMX,BRO,CRP,EPZ,FWD,HGX,HUN,JAN,JAX,KEY,MRX,LCH,LZK,LUB,MLB,MEG,MAF,MFL,MOB,MRX,OHX,LIX,OUN,SJT,SHV,TAE,TBW,TSA\"}\n",
        "  if (input_region in [\"WR\", \"CR\", \"SR\", \"ER\"]):\n",
        "    cwas_list = region_dict[input_region]\n",
        "  else:\n",
        "    cwas_list = input_region\n",
        "  return cwas_list\n",
        "\n",
        "synoptic_token = \"ea497aaa40464749b903ac1204fd8020\"\n",
        "statistics_api = \"https://api.synopticlabs.org/v2/stations/statistics?\"\n",
        "precipitation_api = \"https://api.synopticdata.com/v2/stations/precipitation?\"\n",
        "metadata_api = \"https://api.synopticdata.com/v2/stations/metadata?\"\n",
        "\n",
        "start_date = datetime.strptime(start_date,'%Y-%m-%d')\n",
        "end_date = datetime.strptime(end_date,'%Y-%m-%d')\n",
        "\n",
        "threshold_dict_qpf = {0.01:\"0.254\",0.10:\"2.54\",0.25:\"6.35\",0.50:\"12.7\",\n",
        "                  1.00:\"25.4\",2.00:\"50.8\",\"3.00\":\"76.2\",\"4.00\":\"101.6\",\n",
        "                  \"5.00\":\"127\",\"6.00\":\"152.4\",\"8.00\":\"203.2\",\"12.00\":\"304.8\",\n",
        "                  \"18.00\":\"457.2\",\"24.00\":\"609.6\",\"30.00\":\"762\"}\n",
        "threshold_dict_mint = {-40:233,-20:244,0:255,28:270,\n",
        "                  32:273,80:299}\n",
        "threshold_dict_maxt = {0:255,28:270,32:273,80:299,\n",
        "                      90:305,100:310,110:316,120:322}\n",
        "threshold_dict_maxwind = {11:\"5.0\",17:\"8.0\",22:\"11.0\",34:\"17.0\",48:\"24.0\",64:\"32.0\"}\n",
        "threshold_dict_maxgust = {22:\"11.0\",34:\"17.0\",41:\"21.0\",48:\"24.0\",56:\"28.0\",64:\"32.0\"}\n",
        "\n",
        "\n",
        "if element == \"qpf\":\n",
        "  threshold_q= threshold_dict_qpf[threshold]\n",
        "elif element == \"mint\":\n",
        "  threshold_q = threshold_dict_mint[threshold]\n",
        "elif element == \"maxt\":\n",
        "  threshold_q = threshold_dict_maxt[threshold]\n",
        "elif element == \"maxwind\":\n",
        "  threshold_q=threshold_dict_maxwind[threshold]\n",
        "elif element == \"maxgust\":\n",
        "  threshold_q=threshold_dict_maxgust[threshold]\n",
        "\n",
        "bulk_stats = {}\n",
        "valid_date = start_date\n",
        "temp_vars = [\"maxt\",\"mint\"]\n",
        "if (region_selection == \"CWA\"):\n",
        "  region_name = cwa_id\n",
        "else:\n",
        "  region_name = region_selection\n",
        "\n",
        "def project(lon, lat, prj):\n",
        "  lon = float(lon)\n",
        "  lat = float(lat)\n",
        "\n",
        "  outproj = prj\n",
        "  inproj = Proj(init='epsg:4326')\n",
        "  nbm_coords = pyproj_transform(inproj, outproj, lon, lat)\n",
        "  coordX = nbm_coords[0]\n",
        "  coordY = nbm_coords[1]\n",
        "  #print(f'Lat: {lat}, Y: {coordY} | Lon: {lon}, X: {coordX}')\n",
        "  return(coordX, coordY)\n",
        "\n",
        "\n",
        "def ll_to_index(datalons, datalats, loclon, loclat):\n",
        "  abslat = np.abs(datalats-loclat)\n",
        "  abslon = np.abs(datalons-loclon)\n",
        "  c = np.maximum(abslon, abslat)\n",
        "  latlon_idx_flat = np.argmin(c)\n",
        "  latlon_idx = np.unravel_index(latlon_idx_flat, datalons.shape)\n",
        "  return(latlon_idx)\n",
        "\n",
        "def project_hrap(lon, lat, s4x, s4y):\n",
        "  lon = float(lon)\n",
        "  lat = float(lat)\n",
        "\n",
        "  globe = ccrs.Globe(semimajor_axis=6371200)\n",
        "  hrap_ccrs = proj = ccrs.Stereographic(central_latitude=90.0,\n",
        "                          central_longitude=255.0,\n",
        "                          true_scale_latitude=60.0, globe=globe)\n",
        "  latlon_ccrs = ccrs.PlateCarree()\n",
        "  hrap_coords = hrap_ccrs.transform_point(lon,lat,src_crs=latlon_ccrs)\n",
        "  hrap_idx = ll_to_index(s4x, s4y, hrap_coords[0], hrap_coords[1])\n",
        "\n",
        "  return hrap_idx\n",
        "\n",
        "\n",
        "\n",
        "def download_subset(remote_url, remote_file, local_filename):\n",
        "  print(\"   > Downloading a subset of NBM gribs\")\n",
        "  local_file = \"nbm/\"+local_filename\n",
        "  if \"qmd\" in remote_file:\n",
        "    if element == \"maxt\":\n",
        "      if (int(nbm_qmd_forecasthour_start) % 24 == 0) and (int(nbm_qmd_forecasthour) % 24 ==0):\n",
        "        search_string = f':TMP:2 m above ground:{str(int(int(nbm_qmd_forecasthour_start)/24))}-{str(int(int(nbm_qmd_forecasthour)/24))} day max fcst:'\n",
        "      else:\n",
        "        search_string = f':TMP:2 m above ground:{str(int(nbm_qmd_forecasthour_start))}-{str(int(nbm_qmd_forecasthour))} hour max fcst:'\n",
        "    elif element == \"mint\":\n",
        "      if (int(nbm_qmd_forecasthour_start) % 24 == 0) and (int(nbm_qmd_forecasthour) % 24 ==0):\n",
        "        search_string = f':TMP:2 m above ground:{str(int(int(nbm_qmd_forecasthour_start)/24))}-{str(int(int(nbm_qmd_forecasthour)/24))} day min fcst:'\n",
        "      else:\n",
        "        search_string = f':TMP:2 m above ground:{str(int(nbm_qmd_forecasthour_start))}-{str(int(nbm_qmd_forecasthour))} hour min fcst:'\n",
        "    elif element == \"qpf\":\n",
        "      if (int(nbm_qmd_forecasthour_start) % 24 == 0) and (int(nbm_qmd_forecasthour) % 24 ==0):\n",
        "        search_string = f':APCP:surface:{str(int(int(nbm_qmd_forecasthour_start)/24))}-{str(int(int(nbm_qmd_forecasthour)/24))} day acc fcst:'\n",
        "      else:\n",
        "        search_string = f':APCP:surface:{str(int(nbm_qmd_forecasthour_start))}-{str(int(nbm_qmd_forecasthour))} hour acc fcst:'\n",
        "    elif element == \"maxwind\":\n",
        "      if (int(nbm_qmd_forecasthour_start) % 24 == 0) and (int(nbm_qmd_forecasthour) % 24 == 0):\n",
        "        search_string = f':WIND:10 m above ground:{str(int(nbm_qmd_forecasthour_start/24))}-{str(int(nbm_qmd_forecasthour/24))} hour max fcst:'\n",
        "      else:\n",
        "        search_string = f':WIND:10 m above ground:{str(int(nbm_qmd_forecasthour_start))}-{str(int(nbm_qmd_forecasthour))} hour max fcst:'\n",
        "    elif element == \"maxgust\":\n",
        "      if (int(nbm_qmd_forecasthour_start) % 24 == 0) and (int(nbm_qmd_forecasthour) % 24 == 0):\n",
        "        search_string = f':GUST:10 m above ground:{str(int(nbm_qmd_forecasthour_start/24))}-{str(int(nbm_qmd_forecasthour/24))} hour max fcst:'\n",
        "      else:\n",
        "        search_string = f':GUST:10 m above ground:{str(int(nbm_qmd_forecasthour_start))}-{str(int(nbm_qmd_forecasthour))} hour max fcst:'\n",
        "  elif \"core\" in remote_file:\n",
        "    if element == \"maxt\":\n",
        "      search_string = f':TMAX:2 m above ground:{str(int(nbm_core_forecasthour_start))}-{str(int(nbm_core_forecasthour))} hour max fcst:'\n",
        "    elif element == \"mint\":\n",
        "      search_string = f':TMIN:2 m above ground:{str(int(nbm_core_forecasthour_start))}-{str(int(nbm_core_forecasthour))} hour min fcst:'\n",
        "    elif element == \"snow\":\n",
        "      search_string = f':ASNOW:surface:{str(int(nbm_core_forecasthour_start))}-{str(int(nbm_core_forecasthour))} hour acc'\n",
        "  #print(\"Search string = \",search_string)\n",
        "  idx = remote_url+\".idx\"\n",
        "  #print(\"IDX file = \" + idx)\n",
        "  r = requests.get(idx)\n",
        "  if not r.ok:\n",
        "    print('     ❌ SORRY! Status Code:', r.status_code, r.reason)\n",
        "    print(f'      ❌ It does not look like the index file exists: {idx}')\n",
        "\n",
        "  lines = r.text.split('\\n')\n",
        "  expr = re.compile(search_string)\n",
        "  expr\n",
        "  byte_ranges = {}\n",
        "  for n, line in enumerate(lines, start=1):\n",
        "    # n is the line number (starting from 1) so that when we call for\n",
        "    # `lines[n]` it will give us the next line. (Clear as mud??)\n",
        "    # Use the compiled regular expression to search the line\n",
        "    #print(\">> Searching throgh this line: \" + line)\n",
        "    if expr.search(line):\n",
        "      # aka, if the line contains the string we are looking for...\n",
        "      # Get the beginning byte in the line we found\n",
        "      parts = line.split(':')\n",
        "      rangestart = int(parts[1])\n",
        "      # Get the beginning byte in the next line...\n",
        "      if n+1 < len(lines):\n",
        "        # ...if there is a next line\n",
        "        parts = lines[n].split(':')\n",
        "        rangeend = int(parts[1])\n",
        "      else:\n",
        "        # ...if there isn't a next line, then go to the end of the file.\n",
        "        rangeend = ''\n",
        "\n",
        "        # Store the byte-range string in our dictionary,\n",
        "        # and keep the line information too so we can refer back to it.\n",
        "      byte_ranges[f'{rangestart}-{rangeend}'] = line\n",
        "      #print(line)\n",
        "    #else:\n",
        "      #print(\">>>  Could not find search string!\")\n",
        "  #print(\">>  Number of items in byteRange:\" + str(len(byte_ranges)))\n",
        "  for i, (byteRange, line) in enumerate(byte_ranges.items()):\n",
        "\n",
        "    if i == 0:\n",
        "      # If we are working on the first item, overwrite the existing file.\n",
        "      curl = f'curl -s --range {byteRange} {remote_url} > {local_file}'\n",
        "      #print(\">>  Adding curl command: \" + curl)\n",
        "    else:\n",
        "      # If we are working on not the first item, append the existing file.\n",
        "      curl = f'curl -s --range {byteRange} {remote_url} >> {local_file}'\n",
        "      #print(\"Adding curl command: \" + curl)\n",
        "    #print('>>  Parsing line: ' + line)\n",
        "    try:\n",
        "      num, byte, date, var, level, forecast, _ = line.split(':')\n",
        "    except:\n",
        "      pass\n",
        "      #print(\">>>  Can't get num/byte/etc from this line, so skipping...\")\n",
        "\n",
        "    #print(f'  Downloading GRIB line [{num:>3}]: variable={var}, level={level}, forecast={forecast}')\n",
        "    #print(f'  Downloading GRIB line: variable={var}, level={level}, forecast={forecast}')\n",
        "    #print(\"Running the curl command...\")\n",
        "    os.system(curl)\n",
        "\n",
        "  if os.path.exists(local_file):\n",
        "    print(f'      ✅ Success! Searched for [{search_string}] and got [{len(byte_ranges)}] GRIB fields and saved as {local_file}')\n",
        "    return local_file\n",
        "  else:\n",
        "    print(print(f'      ❌ Unsuccessful! Searched for [{search_string}] and did not find anything!'))\n",
        "\n",
        "\n",
        "\n",
        "def get_stageiv():\n",
        "  siv_url = \"https://water.weather.gov/precip/downloads/\"+valid_date_end.strftime('%Y')+\"/\"+valid_date_end.strftime('%m')+\"/\"+valid_date_end.strftime('%d')+\"/nws_precip_1day_\"+valid_date_end.strftime('%Y%m%d')+\"_conus.nc\"\n",
        "  data = urlopen(siv_url).read()\n",
        "\n",
        "  nc = Dataset('data', memory=data)\n",
        "  #with Dataset(siv_file, 'r') as nc:\n",
        "  stageIV = nc.variables['observation']\n",
        "  s4x = nc.variables['x']\n",
        "  s4y = nc.variables['y']\n",
        "  return stageIV, s4x, s4y\n",
        "\n",
        "def mps_to_kts(mps):\n",
        "  kts = mps * 1.94384\n",
        "  return kts\n",
        "\n",
        "date_delta = end_date - start_date\n",
        "datelist = [start_date + timedelta(days=d) for d in range(int(date_delta.days)+1)]\n",
        "\n",
        "\n",
        "for valid_date in tqdm(datelist, desc=\"Looping over date range\"):\n",
        "  with tqdm(total=3, leave=False) as pbar:\n",
        "    pbar.set_description(f\"Working on day {valid_date.strftime('%Y-%m-%d')}\")\n",
        "    valid_date_string = valid_date.strftime('%Y%m%d')\n",
        "    nbm_init_date = valid_date - timedelta(days=(nbm_lead_time - 1))\n",
        "    nbm_init = nbm_init_date + timedelta(hours=int(nbm_init_hour))\n",
        "\n",
        "    if element==\"maxt\":\n",
        "      nbm_qmd_valid_hour=\"06\"\n",
        "      valid_date_start = valid_date\n",
        "      valid_date_end = valid_date + timedelta(days=1)\n",
        "      obs_start_hour = \"1200\"\n",
        "      obs_end_hour = \"0600\"\n",
        "      ob_stat = \"maximum\"\n",
        "      valid_end_datetime = valid_date_end + timedelta(hours=(int(obs_end_hour)/100))\n",
        "      nbm_qmd_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_qmd_valid_hour))\n",
        "\n",
        "\n",
        "    elif element==\"mint\":\n",
        "      nbm_qmd_valid_hour=\"18\"\n",
        "      valid_date_start = valid_date\n",
        "      valid_date_end = valid_date\n",
        "      obs_start_hour = \"0000\"\n",
        "      obs_end_hour = \"1800\"\n",
        "      ob_stat = \"minimum\"\n",
        "      valid_end_datetime = valid_date_end + timedelta(hours=(int(obs_end_hour)/100))\n",
        "      nbm_qmd_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_qmd_valid_hour))\n",
        "\n",
        "    elif element==\"qpf\":\n",
        "      nbm_qmd_valid_hour=\"12\"\n",
        "      valid_date_start = valid_date\n",
        "      valid_date_end = valid_date + timedelta(days=1)\n",
        "      obs_start_hour = \"1200\"\n",
        "      obs_end_hour = \"1200\"\n",
        "      ob_stat = \"total\"\n",
        "      valid_end_datetime = valid_date_end + timedelta(hours=(int(obs_end_hour)/100))\n",
        "      nbm_qmd_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_qmd_valid_hour))\n",
        "\n",
        "    elif element==\"maxwind\" or element == \"maxgust\":\n",
        "      nbm_qmd_valid_hour=\"06\"\n",
        "      obs_start_hour=\"0600\"\n",
        "      obs_end_hour=\"0600\"\n",
        "      ob_stat=\"maximum\"\n",
        "      valid_date_start=valid_date\n",
        "      valid_date_end=valid_date + timedelta(days=1)\n",
        "      valid_end_datetime=valid_date_end + timedelta(hours=(int(obs_end_hour)/100))\n",
        "      #valid_date_start = datetime.strptime(valid_date,'%Y-%m-%d')\n",
        "      #valid_date_end = datetime.strptime(valid_date,'%Y-%m-%d') + timedelta(days=1)\n",
        "      #valid_end_datetime=valid_date_end + timedelta(hours=(int(obs_end_hour)/100))\n",
        "      core_init = nbm_init\n",
        "      nbm_core_valid_end_datetime = valid_date_end\n",
        "      nbm_qmd_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_qmd_valid_hour))\n",
        "      nbm_core_fhdelta = valid_end_datetime - nbm_init\n",
        "\n",
        "    valid_input = valid_date_end + timedelta (hours=int(nbm_qmd_valid_hour))\n",
        "    vt = valid_date_start.strftime('%a %m-%d-%Y')\n",
        "    init = nbm_init.strftime('%HZ %Y-%m-%d')\n",
        "\n",
        "\n",
        "    nbm_qmd_fhdelta = nbm_qmd_valid_end_datetime - nbm_init\n",
        "    nbm_qmd_forecasthour = nbm_qmd_fhdelta.total_seconds() / 3600.\n",
        "    if element==\"qpf\" or element == \"maxwind\" or element == \"maxgust\":\n",
        "      nbm_qmd_forecasthour_start = nbm_qmd_forecasthour - 24\n",
        "    else:\n",
        "      nbm_qmd_forecasthour_start = nbm_qmd_forecasthour - 18\n",
        "\n",
        "\n",
        "    obs ={}\n",
        "\n",
        "\n",
        "    for region in region_list:\n",
        "      pbar.set_description(f\"Working on day {valid_date.strftime('%Y%m%d')} (Getting obs for {region})\")\n",
        "      if element == \"qpf\" and use_stageiv:\n",
        "        json_name = \"obs/\"+region+\"_nws_raws_locs.json\"\n",
        "      else:\n",
        "        json_name = \"obs/Obs_\"+element+\"_\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour+\"_\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour+\"_\"+region+\".json\"\n",
        "\n",
        "      if any(te in element for te in temp_vars):\n",
        "        api_token = \"&token=\"+synoptic_token\n",
        "        station_query = \"&cwa=\"+cwa_list(region)\n",
        "        vars_query = \"&vars=air_temp\"\n",
        "        start_query = \"&start=\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour\n",
        "        end_query = \"&end=\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour\n",
        "        stat_type = \"&type=\"+ob_stat\n",
        "        network_query = network_string\n",
        "        api_extras = \"&units=temp%7Cf&within=1440&status=active\"\n",
        "        obs_url = statistics_api + api_token + station_query + vars_query + start_query + end_query + stat_type + network_query + api_extras\n",
        "      elif element == \"maxwind\" or element == \"maxgust\":\n",
        "        api_token = \"&token=\"+synoptic_token\n",
        "        station_query = \"&cwa=\"+cwa_list(region)\n",
        "        if element == \"maxwind\":\n",
        "          vars_query = \"&vars=wind_speed\"\n",
        "        else:\n",
        "          vars_query= \"&vars=wind_gust\"\n",
        "        start_query = \"&start=\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour\n",
        "        end_query = \"&end=\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour\n",
        "        stat_type = \"&type=\"+ob_stat\n",
        "        network_query = network_string\n",
        "        obs_url = statistics_api + api_token + station_query + vars_query + start_query + end_query + stat_type + network_query\n",
        "      elif (element == \"qpf\"):\n",
        "        if use_stageiv:\n",
        "          api_token = \"&token=\"+synoptic_token\n",
        "          station_query = \"&cwa=\"+cwa_list(region)\n",
        "          api_extras = \"&fields=status,latitude,longitude,name,elevation\"\n",
        "          network_query = network_string\n",
        "          obs_url = metadata_api + api_token + station_query + network_query + api_extras\n",
        "          stageIV, s4xs, s4ys = get_stageiv()\n",
        "          s4xs, s4ys = np.meshgrid(s4xs, s4ys)\n",
        "        else:\n",
        "          api_token = \"&token=\"+synoptic_token\n",
        "          station_query = \"&cwa=\"+cwa_list(region)\n",
        "          api_extras = \"&fields=status,latitude,longitude,name,elevation&obtimezone=utc\"\n",
        "          network_query = network_string\n",
        "          vars_query = \"&pmode=totals\"\n",
        "          units_query = \"&units=precip|in\"\n",
        "          start_query = \"&start=\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour\n",
        "          end_query = \"&end=\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour\n",
        "          obs_url = precipitation_api + api_token + station_query + network_query + vars_query + start_query + end_query + units_query + api_extras\n",
        "      #print(obs_url)\n",
        "      if os.path.exists(json_name):\n",
        "        pass\n",
        "      else:\n",
        "        urlretrieve(obs_url, json_name)\n",
        "\n",
        "      if os.path.exists(json_name):\n",
        "          with open(json_name) as json_file:\n",
        "              obs_json = json.load(json_file)\n",
        "              obs_lats = []\n",
        "              obs_lons = []\n",
        "              obs_value = []\n",
        "              obs_elev = []\n",
        "              obs_stid = []\n",
        "              obs_name = []\n",
        "              for stn in obs_json[\"STATION\"]:\n",
        "                  # print(stn.encode('utf-8'))\n",
        "                  if stn[\"STID\"] is None:\n",
        "                    stid = \"N0N3\"\n",
        "                  else:\n",
        "                    stid = stn[\"STID\"]\n",
        "                  #print(f'Processing {region} station {stid}')\n",
        "                  name = stn[\"NAME\"]\n",
        "                  if stn[\"ELEVATION\"]:\n",
        "                    elev = stn[\"ELEVATION\"]\n",
        "                  #elif stn[\"ELEV_DEM\"]:\n",
        "                  #  elev = stn[\"ELEV_DEM\"]\n",
        "                  else:\n",
        "                    elev = -999\n",
        "                  lat = stn[\"LATITUDE\"]\n",
        "                  lon = stn[\"LONGITUDE\"]\n",
        "                  stat= None\n",
        "                  if any(te in element for te in temp_vars):\n",
        "                    if \"air_temp_set_1\" in stn['STATISTICS'] and stn['STATISTICS']['air_temp_set_1']:\n",
        "                      if ob_stat in stn['STATISTICS']['air_temp_set_1']  and float(stn[\"LATITUDE\"]) < 50.924 and float(stn[\"LATITUDE\"]) > 23.377 and float(stn[\"LONGITUDE\"]) > -125.650 and float(stn[\"LONGITUDE\"]) < -66.008:\n",
        "                          stat = stn['STATISTICS']['air_temp_set_1'][ob_stat]\n",
        "                          obs_stid.append(str(stid))\n",
        "                          obs_name.append(str(name))\n",
        "                          obs_elev.append(int(elev))\n",
        "                          obs_lats.append(float(lat))\n",
        "                          obs_lons.append(float(lon))\n",
        "                          obs_value.append(float(stat))\n",
        "                  elif element == \"maxwind\":\n",
        "                    if 'wind_speed_set_1' in stn['STATISTICS'] and stn['STATISTICS']['wind_speed_set_1']:\n",
        "                      if ob_stat in stn['STATISTICS']['wind_speed_set_1'] and float(stn[\"LATITUDE\"]) != 0.:\n",
        "                        stat = stn['STATISTICS']['wind_speed_set_1'][ob_stat]\n",
        "                        obs_stid.append(str(stid))\n",
        "                        obs_name.append(str(name))\n",
        "                        obs_elev.append(float(elev))\n",
        "                        obs_lats.append(float(lat))\n",
        "                        obs_lons.append(float(lon))\n",
        "                        obs_value.append(mps_to_kts(float(stat)))\n",
        "                  elif element == \"maxgust\":\n",
        "                    if \"wind_gust_set_1\" in stn['STATISTICS'] and stn['STATISTICS'][\"wind_gust_set_1\"]:\n",
        "                      if ob_stat in  stn['STATISTICS']['wind_gust_set_1'] and float(stn[\"LATITUDE\"]) != 0.:\n",
        "                        stat = stn['STATISTICS']['wind_gust_set_1'] and float(stn[\"LATITUDE\"]) != 0.\n",
        "                        obs_stid.append(str(stid))\n",
        "                        obs_name.append(str(name))\n",
        "                        obs_elev.append(float(elev))\n",
        "                        obs_lats.append(float(lat))\n",
        "                        obs_lons.append(float(lon))\n",
        "                        obs_value.append(mps_to_kts(float(stat)))\n",
        "                  elif (element == \"qpf\"):\n",
        "                    if (stn[\"STATUS\"] == \"ACTIVE\") and float(stn[\"LATITUDE\"]) < 50.924 and float(stn[\"LATITUDE\"]) > 23.377 and float(stn[\"LONGITUDE\"]) > -125.650 and float(stn[\"LONGITUDE\"]) < -66.008:\n",
        "                      obs_stid.append(str(stid))\n",
        "                      obs_name.append(str(name))\n",
        "                      obs_elev.append(int(elev))\n",
        "                      obs_lats.append(float(lat))\n",
        "                      obs_lons.append(float(lon))\n",
        "                      if use_stageiv:\n",
        "                        coords = project_hrap(lon, lat, s4xs, s4ys)\n",
        "                        siv_value = float(stageIV[coords])\n",
        "                        if (siv_value >= 0.0):\n",
        "                          obs_value.append(siv_value)\n",
        "                        else:\n",
        "                          obs_value.append(np.NaN)\n",
        "                      else:\n",
        "                        if \"precipitation\" in stn[\"OBSERVATIONS\"]:\n",
        "                          if \"total\" in stn[\"OBSERVATIONS\"][\"precipitation\"][0]:\n",
        "                            ptotal = stn[\"OBSERVATIONS\"][\"precipitation\"][0][\"total\"]\n",
        "                            if ptotal >= 0.01:\n",
        "                              obs_value.append(ptotal)\n",
        "                            else:\n",
        "                              obs_value.append(np.nan)\n",
        "                          else:\n",
        "                            obs_value.append(np.nan)\n",
        "                        else:\n",
        "                          obs_value.append(np.nan)\n",
        "\n",
        "\n",
        "              csv_name = \"obs_points_\"+region+\".csv\"\n",
        "              obs[region] = pd.DataFrame()\n",
        "              obs[region][\"stid\"] = obs_stid\n",
        "              obs[region][\"name\"] = obs_name\n",
        "              obs[region][\"elevation\"] = obs_elev\n",
        "              obs[region][\"lat\"] = obs_lats\n",
        "              obs[region][\"lon\"] = obs_lons\n",
        "              obs[region][\"ob_\"+element] = obs_value\n",
        "              #if element == \"qpf\":\n",
        "                #obs[region][obs[region][\"ob_qpf\"] < 0.01] = np.nan\n",
        "              #obs[region].to_csv(csv_name)\n",
        "\n",
        "    pbar.update()\n",
        "    pbar.set_description(f\"Working on day {valid_date.strftime('%Y-%m-%d')} (Getting and processing NBM)\")\n",
        "    if os.path.exists(\"nbm\"):\n",
        "      pass\n",
        "    else:\n",
        "      os.system('mkdir nbm')\n",
        "    nbm_init_filen = nbm_init.strftime('%Y%m%d') + \"_\" + nbm_init.strftime('%H')\n",
        "    nbm_url_base = \"https://noaa-nbm-grib2-pds.s3.amazonaws.com/blend.\"+nbm_init.strftime('%Y%m%d') \\\n",
        "                +\"/\"+nbm_init.strftime('%H')+\"/\"\n",
        "    perc_file = f'blend.t{int(nbm_init_hour):02}z.qmd.f{int(nbm_qmd_forecasthour):03}.co.grib2'\n",
        "    perc_file_subset = f'blend.t{int(nbm_init_hour):02}z.qmd.{nbm_init_filen}{nbm_init_filen}f{int(nbm_qmd_forecasthour):03}.co.{element}_subset_prob.grib2'\n",
        "    perc_url = nbm_url_base+\"qmd/\"+perc_file\n",
        "    if os.path.exists(\"nbm/\"+perc_file_subset):\n",
        "      pbar.set_description(f\"Working on day {valid_date.strftime('%Y-%m-%d')}:NBM probabilistic already exists\")\n",
        "    else:\n",
        "      #urlretrieve(perc_url, \"nbm/\"+perc_file)\n",
        "      try:\n",
        "        download_subset(perc_url, perc_file, perc_file_subset)\n",
        "      except:\n",
        "        print(' GASP! Did not find NBM data. Moving on to the next day')\n",
        "        continue\n",
        "\n",
        "    nbmgrb = pygrib.open(\"nbm/\"+perc_file_subset)\n",
        "\n",
        "    if element==\"qpf\":\n",
        "      nbm_gribobject = nbmgrb.select(name=\"Total Precipitation\",lengthOfTimeRange=24, upperLimit=float(threshold_q))[0]\n",
        "    elif element==\"maxwind\":\n",
        "      nbm_gribobject=nbmgrb.select(name=\"10 metre wind speed\",lengthOfTimeRange=24,stepTypeInternal=\"max\",upperLimit=float(threshold_q))[0]\n",
        "    elif element==\"maxgust\":\n",
        "      nbm_gribobject=nbmgrb.select(name=\"10 metre wind gust\",lengthOfTimeRange=24,stepTypeInternal=\"max\",upperLimit=float(threshold_q))[0]\n",
        "    else:\n",
        "      if \"MaxT\" in temp:\n",
        "        if \"Hot\" in temp:\n",
        "          nbm_gribobject = nbmgrb.select(name=\"2 metre temperature\", stepTypeInternal=\"max\", upperLimit=threshold_q)[0]\n",
        "        elif \"Cold\" in temp:\n",
        "          nbm_gribobject = nbmgrb.select(name=\"2 metre temperature\", stepTypeInternal=\"max\", lowerLimit=threshold_q)[0]\n",
        "      elif \"MinT\" in temp:\n",
        "        if \"Cold\" in temp:\n",
        "          nbm_gribobject = nbmgrb.select(name=\"2 metre temperature\", stepTypeInternal=\"min\", lowerLimit=threshold_q)[0]\n",
        "        elif \"Hot\" in temp:\n",
        "          nbm_gribobject = nbmgrb.select(name=\"2 metre temperature\", stepTypeInternal=\"min\", upperLimit=threshold_q)[0]\n",
        "\n",
        "\n",
        "\n",
        "    nbm = nbm_gribobject.values\n",
        "    nbmlats, nbmlons = nbm_gribobject.latlons()\n",
        "    nbmgrb.close()\n",
        "\n",
        "    try:\n",
        "      for region in region_list:\n",
        "        pbar.update()\n",
        "        pbar.set_description(f\"Working on day {valid_date.strftime('%Y-%m-%d')} (Extracting NBM data)\")\n",
        "        #print(f'    Extracting NBM data for {region}')\n",
        "        var_string = \"ob_\"+element\n",
        "        all_obs = obs[region][[var_string]].values\n",
        "        point_lats = obs[region][\"lat\"].values\n",
        "        point_lons = obs[region][\"lon\"].values\n",
        "        prob_exceedance = []\n",
        "        nbm_idx = []\n",
        "        for i in range(0, len(point_lats)):\n",
        "          coords = ll_to_index(nbmlons, nbmlats, point_lons[i], point_lats[i])\n",
        "          poe_value = nbm[coords]\n",
        "          nbm_idx.append(coords)\n",
        "          prob_exceedance.append(poe_value)\n",
        "        obs[region][\"NBM_idx\"] = nbm_idx\n",
        "        obs[region][\"prob_exceed\"] = prob_exceedance\n",
        "        csv_file = f'NBM_{element}_Reliability_{valid_date_string}_{region}.csv'\n",
        "        #obs[region].to_csv(csv_file)\n",
        "\n",
        "\n",
        "\n",
        "        if valid_date == start_date:\n",
        "          bulk_stats[region] = pd.DataFrame()\n",
        "          bulk_stats[region][\"ob\"] = obs[region][var_string]\n",
        "          bulk_stats[region][\"prob_exceedance\"] = obs[region]['prob_exceed']\n",
        "        else:\n",
        "          bulk_length = len(bulk_stats[region])\n",
        "          for value in range(0, len(obs[region]['stid'].values)):\n",
        "            newdata = [obs[region][var_string].values[value], obs[region]['prob_exceed'].values[value]]\n",
        "            append_series = pd.Series(newdata, index = bulk_stats[region].columns)\n",
        "            bulk_stats[region] = bulk_stats[region].append(append_series, ignore_index=True)\n",
        "\n",
        "        if valid_date == end_date:\n",
        "          bulk_obPerc_filename = region+\"_BulkStats_ProbNExceed_\"+str(threshold)+\"_\"+start_date.strftime('%Y%m%d')+\"_\"+end_date.strftime('%Y%m%d')+\".csv\"\n",
        "          bulk_stats[region].to_csv(bulk_obPerc_filename)\n",
        "\n",
        "          if region_name == \"CONUS\":\n",
        "            bulk_stats_conus = pd.concat([bulk_stats[\"WR\"], bulk_stats[\"CR\"], bulk_stats[\"ER\"], bulk_stats[\"SR\"]])\n",
        "            conus_bulk_obPerc_filename = \"CONUS_BulkStats_ProbNExceed_\"+str(threshold)+\"_\"+start_date.strftime('%Y%m%d')+\"_\"+end_date.strftime('%Y%m%d')+\".csv\"\n",
        "            bulk_stats_conus.to_csv(conus_bulk_obPerc_filename)\n",
        "\n",
        "    except:\n",
        "      pbar.set_description(f'Working on day {valid_date} (Problem with matching NBM/Obs! Skipping this day!)')\n",
        "      continue\n",
        "    pbar.set_description(f\"{valid_date.strftime('%Y-%m-%d')} finished!\")\n",
        "    pbar.update()\n",
        "\n",
        "#########################################################################################"
      ],
      "metadata": {
        "id": "GQ7-fZh2bpg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Make Plots\n",
        "print(\"Making plots...\")\n",
        "\n",
        "summary_valid_title = start_date.strftime('%m/%d/%Y')+\" - \"+end_date.strftime('%m/%d/%Y')\n",
        "title_dict = {\"maxt\":[\"Max T\",\"PMaxT\"],\"mint\":[\"Min T\",\"PMinT\"], \"qpf\":[\"Precip\",\"PQPF\"] , \"maxwind\":[\"Max Sustained Wind\",\"PMaxWind\"], \"maxgust\":[\"Max Wind Gust\",\"PMaxGust\"]}\n",
        "background_color = '#272727'\n",
        "text_color = 'white'\n",
        "\n",
        "\n",
        "if element == \"qpf\":\n",
        "  color1 = ['brown'] * 10\n",
        "  color0 = ['teal'] * 10\n",
        "else:\n",
        "  if percentile_stat == \"nonexceedance\":\n",
        "    color0 = ['b'] * 10\n",
        "    color1 = ['r'] * 10\n",
        "  elif percentile_stat == \"exceedance\":\n",
        "    color0 = ['r'] * 10\n",
        "    color1 = ['b'] * 10\n",
        "\n",
        "\n",
        "if region_selection == \"CONUS\":\n",
        "\n",
        "  matplotlib.rc('axes',facecolor=background_color, edgecolor='w')\n",
        "\n",
        "  valid_title = valid_date.strftime('%a %m-%d-%Y')\n",
        "  nbm_init_title = nbm_init.strftime('%HZ %m-%d-%Y')\n",
        "\n",
        "  def flip(items, ncol):\n",
        "          return itertools.chain(*[items[i::ncol] for i in range(ncol)])\n",
        "  if percentile_stat == \"nonexceedance\":\n",
        "    titlestring = f'NBM v4.0 {title_dict[element][1]} < {threshold} Reliability'\n",
        "  elif percentile_stat == \"exceedance\":\n",
        "    titlestring = f'NBM v4.0 {title_dict[element][1]} > {threshold} Reliability'\n",
        "\n",
        "  fig = plt.figure(constrained_layout=True, figsize=(16,9), facecolor=background_color, frameon=True, dpi=150)\n",
        "  #grid = fig.add_gridspec(4,4, width_ratios=width_ratios, hspace=0.2, wspace=0.2, left=0.1, right=0.9)\n",
        "  width_ratios = [2,1,1]\n",
        "  grid = fig.add_gridspec(2,3, width_ratios=width_ratios)\n",
        "  fig.text(0.5, 1.08, titlestring, horizontalalignment='center', verticalalignment='bottom', weight='bold',fontsize=25,color=text_color)\n",
        "  if use_stageiv:\n",
        "    fig.text(0.5, 1.08,f'Valid: {summary_valid_title} | NBM Lead Time (Days): {nbm_lead_time} | Points: Stage IV @ NWS + RAWS',horizontalalignment='center', verticalalignment='top', fontsize=16, color=text_color)\n",
        "  else:\n",
        "    fig.text(0.5, 1.08,f'Valid: {summary_valid_title} | NBM Lead Time (Days): {nbm_lead_time} | Points: NWS + RAWS',horizontalalignment='center', verticalalignment='top', fontsize=16, color=text_color)\n",
        "\n",
        "  ax1 = fig.add_subplot(grid[:,:-2])\n",
        "  ax2 = fig.add_subplot(grid[0,1])\n",
        "  ax3 = fig.add_subplot(grid[0,2])\n",
        "  ax4 = fig.add_subplot(grid[1,1])\n",
        "  ax5 = fig.add_subplot(grid[1,2])\n",
        "\n",
        "  conus_df = pd.concat([bulk_stats[\"WR\"], bulk_stats[\"CR\"], bulk_stats[\"ER\"], bulk_stats[\"SR\"]])\n",
        "\n",
        "  ys = []\n",
        "  xs = [0.05,0.15,0.25,0.35,0.45,0.55,0.65,0.75,0.85,0.95]\n",
        "  for p in range(10,110,10):\n",
        "    if percentile_stat == \"nonexceedance\":\n",
        "      try:\n",
        "        ys.append(len(conus_df[(conus_df.ob < float(threshold)) & (conus_df.prob_exceedance <= p) & (conus_df.prob_exceedance > (p-10))]) / len(conus_df[(conus_df.prob_exceedance <= p) & (conus_df.prob_exceedance > (p-10))]))\n",
        "      except:\n",
        "        ys.append(np.nan)\n",
        "    else:\n",
        "      try:\n",
        "        ys.append(len(conus_df[(conus_df.ob > float(threshold)) & (conus_df.prob_exceedance <= p) & (conus_df.prob_exceedance > (p-10))]) / len(conus_df[(conus_df.prob_exceedance <= p) & (conus_df.prob_exceedance > (p-10))]))\n",
        "      except:\n",
        "        ys.append(np.nan)\n",
        "  if np.isnan(ys).any():\n",
        "    print(\"Uh-oh, you got some empty bins, might want to double-check some things\")\n",
        "    ys_tointerp = pd.Series(ys)\n",
        "    ys = ys_tointerp.interpolate()\n",
        "\n",
        "  ax1.set_aspect('equal')\n",
        "  ax1.plot(xs, xs, color='grey', linestyle='--')\n",
        "  ax1.plot(xs, ys, color='steelblue', marker='o')\n",
        "  for tick in ax1.get_xticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  for tick in ax1.get_yticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  ax1.tick_params(axis='y',labelsize=8, color=text_color)\n",
        "  ax1.set_title(\"CONUS\", color=text_color, fontsize=14, weight='bold')\n",
        "  ax1.set_xlabel(\"Forecast Probability\", color=text_color, fontsize=10)\n",
        "  ax1.set_ylabel(\"Observed Relative Frequency\", color=text_color, fontsize=10)\n",
        "  ax1.fill_between(xs, ys, xs, where=(np.subtract(ys, xs) > 0), interpolate=True, color=color1, alpha=0.25)\n",
        "  ax1.fill_between(xs, xs, ys, where=(np.subtract(ys, xs) < 0), interpolate=True, color=color0, alpha=0.25)\n",
        "\n",
        "  region_ys = {}\n",
        "  for region in region_list:\n",
        "    if not region in region_ys.keys():\n",
        "      region_ys[region] = []\n",
        "\n",
        "    for p in range(10,110,10):\n",
        "      if percentile_stat == \"nonexceedance\":\n",
        "        try:\n",
        "          region_ys[region].append(len(bulk_stats[region][(bulk_stats[region].ob < float(threshold)) & (bulk_stats[region].prob_exceedance <= p) & (bulk_stats[region].prob_exceedance > (p-10))]) / len(bulk_stats[region][(bulk_stats[region].prob_exceedance <= p) & (bulk_stats[region].prob_exceedance > (p-10))]))\n",
        "        except:\n",
        "          region_ys[region].append(np.nan)\n",
        "      else:\n",
        "        try:\n",
        "          region_ys[region].append(len(bulk_stats[region][(bulk_stats[region].ob > float(threshold)) & (bulk_stats[region].prob_exceedance <= p) & (bulk_stats[region].prob_exceedance > (p-10))]) / len(bulk_stats[region][(bulk_stats[region].prob_exceedance <= p) & (bulk_stats[region].prob_exceedance > (p-10))]))\n",
        "        except:\n",
        "          region_ys[region].append(np.nan)\n",
        "\n",
        "    if np.isnan(region_ys[region]).any():\n",
        "      print(f'  {region} has at least one empty bin')\n",
        "      ys_region_tointerp = pd.Series(region_ys[region])\n",
        "      region_ys[region] = ys_region_tointerp.interpolate()\n",
        "\n",
        "  ax2.set_aspect('equal')\n",
        "  ax2.plot(xs, xs, color='grey', linestyle='--')\n",
        "  ax2.plot(xs, region_ys[\"WR\"], color='steelblue', marker='o')\n",
        "  for tick in ax2.get_xticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  for tick in ax2.get_yticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  ax2.tick_params(axis='y',labelsize=8, color=text_color)\n",
        "  ax2.set_xlabel(\"Forecast Probability\", color=text_color, fontsize=10)\n",
        "  ax2.set_ylabel(\"Observed Relative Frequency\", color=text_color, fontsize=10)\n",
        "  ax2.set_title(\"Western Region\", color=text_color, fontsize=14, weight='bold')\n",
        "  ax2.fill_between(xs, region_ys[\"WR\"], xs, where=(np.subtract(region_ys[\"WR\"], xs) > 0), interpolate=True, color=color1, alpha=0.25)\n",
        "  ax2.fill_between(xs, xs, region_ys[\"WR\"], where=(np.subtract(region_ys[\"WR\"], xs) < 0), interpolate=True, color=color0, alpha=0.25)\n",
        "\n",
        "\n",
        "  ax3.set_aspect('equal')\n",
        "  ax3.plot(xs, xs, color='grey', linestyle='--')\n",
        "  ax3.plot(xs, region_ys[\"CR\"], color='steelblue', marker='o')\n",
        "  for tick in ax3.get_xticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  for tick in ax3.get_yticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  ax3.tick_params(axis='y',labelsize=8, color=text_color)\n",
        "  ax3.set_xlabel(\"Forecast Probability\", color=text_color, fontsize=10)\n",
        "  ax3.set_ylabel(\"Observed Relative Frequency\", color=text_color, fontsize=10)\n",
        "  ax3.set_title(\"Central Region\", color=text_color, fontsize=14, weight='bold')\n",
        "  ax3.fill_between(xs, region_ys[\"CR\"], xs, where=(np.subtract(region_ys[\"CR\"], xs) > 0), interpolate=True, color=color1, alpha=0.25)\n",
        "  ax3.fill_between(xs, xs, region_ys[\"CR\"], where=(np.subtract(region_ys[\"CR\"], xs) < 0), interpolate=True, color=color0, alpha=0.25)\n",
        "\n",
        "\n",
        "  ax4.set_aspect('equal')\n",
        "  ax4.plot(xs, xs, color='grey', linestyle='--')\n",
        "  ax4.plot(xs, region_ys[\"SR\"], color='steelblue', marker='o')\n",
        "  for tick in ax4.get_xticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  for tick in ax4.get_yticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  ax4.tick_params(axis='y',labelsize=8, color=text_color)\n",
        "  ax4.set_xlabel(\"Forecast Probability\", color=text_color, fontsize=10)\n",
        "  ax4.set_ylabel(\"Observed Relative Frequency\", color=text_color, fontsize=10)\n",
        "  ax4.set_title(\"Southern Region\", color=text_color, fontsize=14, weight='bold')\n",
        "  ax4.fill_between(xs, region_ys[\"SR\"], xs, where=(np.subtract(region_ys[\"SR\"], xs) > 0), interpolate=True, color=color1, alpha=0.25)\n",
        "  ax4.fill_between(xs, xs, region_ys[\"SR\"], where=(np.subtract(region_ys[\"SR\"], xs) < 0), interpolate=True, color=color0, alpha=0.25)\n",
        "\n",
        "\n",
        "  ax5.set_aspect('equal')\n",
        "  ax5.plot(xs, xs, color='grey', linestyle='--')\n",
        "  ax5.plot(xs, region_ys[\"ER\"], color='steelblue', marker='o')\n",
        "  for tick in ax5.get_xticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  for tick in ax5.get_yticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  ax5.tick_params(axis='y',labelsize=8, color=text_color)\n",
        "  ax5.set_xlabel(\"Forecast Probability\", color=text_color, fontsize=10)\n",
        "  ax5.set_ylabel(\"Observed Relative Frequency\", color=text_color, fontsize=10)\n",
        "  ax5.set_title(\"Eastern Region\", color=text_color, fontsize=14, weight='bold')\n",
        "  ax5.fill_between(xs, region_ys[\"ER\"], xs, where=(np.subtract(region_ys[\"ER\"], xs) > 0), interpolate=True, color=color1, alpha=0.25)\n",
        "  ax5.fill_between(xs, xs, region_ys[\"ER\"], where=(np.subtract(region_ys[\"ER\"], xs) < 0), interpolate=True, color=color0, alpha=0.25)\n",
        "\n",
        "  if percentile_stat == \"nonexceedance\":\n",
        "    ax1.text(0.98,0.01,f'ob hits = {len(conus_df[(conus_df.ob < float(threshold-0.5))])}', horizontalalignment='right', size=10, color='grey')\n",
        "    ax2.text(0.98,0.01,f'ob hits = {len(bulk_stats[\"WR\"][bulk_stats[\"WR\"].ob < float(threshold)])}', horizontalalignment='right', size=10, color='grey')\n",
        "    ax3.text(0.98,0.01,f'ob hits = {len(bulk_stats[\"CR\"][bulk_stats[\"CR\"].ob < float(threshold)])}', horizontalalignment='right', size=10, color='grey')\n",
        "    ax4.text(0.98,0.01,f'ob hits = {len(bulk_stats[\"SR\"][bulk_stats[\"SR\"].ob < float(threshold)])}', horizontalalignment='right', size=10, color='grey')\n",
        "    ax5.text(0.98,0.01,f'ob hits = {len(bulk_stats[\"ER\"][bulk_stats[\"ER\"].ob < float(threshold)])}', horizontalalignment='right', size=10, color='grey')\n",
        "  else:\n",
        "    ax1.text(0.98,0.01,f'ob hits = {len(conus_df[(conus_df.ob > float(threshold))])}', horizontalalignment='right', size=10, color='grey')\n",
        "    ax2.text(0.98,0.01,f'ob hits = {len(bulk_stats[\"WR\"][bulk_stats[\"WR\"].ob > float(threshold)])}', horizontalalignment='right', size=10, color='grey')\n",
        "    ax3.text(0.98,0.01,f'ob hits = {len(bulk_stats[\"CR\"][bulk_stats[\"CR\"].ob > float(threshold)])}', horizontalalignment='right', size=10, color='grey')\n",
        "    ax4.text(0.98,0.01,f'ob hits = {len(bulk_stats[\"SR\"][bulk_stats[\"SR\"].ob > float(threshold)])}', horizontalalignment='right', size=10, color='grey')\n",
        "    ax5.text(0.98,0.01,f'ob hits = {len(bulk_stats[\"ER\"][bulk_stats[\"ER\"].ob > float(threshold)])}', horizontalalignment='right', size=10, color='grey')\n",
        "\n",
        "else:\n",
        "\n",
        "  if (region_selection == \"CWA\"):\n",
        "    dataframeid = cwa_id\n",
        "  else:\n",
        "    dataframeid = region_selection\n",
        "\n",
        "  matplotlib.rc('axes',facecolor=background_color, edgecolor='w')\n",
        "\n",
        "  valid_title = valid_date.strftime('%a %m-%d-%Y')\n",
        "  nbm_init_title = nbm_init.strftime('%HZ %m-%d-%Y')\n",
        "\n",
        "  def flip(items, ncol):\n",
        "          return itertools.chain(*[items[i::ncol] for i in range(ncol)])\n",
        "  if percentile_stat == \"nonexceedance\":\n",
        "    titlestring = f'NBM v4.1 {title_dict[element][1]} < {threshold} Reliability'\n",
        "  elif percentile_stat == \"exceedance\":\n",
        "    titlestring = f'NBM v4.1 {title_dict[element][1]} > {threshold} Reliability'\n",
        "\n",
        "  fig = plt.figure(constrained_layout=True, figsize=(9,9), facecolor=background_color, frameon=True, dpi=150)\n",
        "  #grid = fig.add_gridspec(4,4, width_ratios=width_ratios, hspace=0.2, wspace=0.2, left=0.1, right=0.9)\n",
        "\n",
        "  fig.text(0.5, 1.08, titlestring, horizontalalignment='center', verticalalignment='bottom', weight='bold',fontsize=18,color=text_color)\n",
        "  if use_stageiv:\n",
        "    fig.text(0.5, 1.08,f'Valid: {summary_valid_title} | NBM Lead Time (Days): {nbm_lead_time} | Points: Stage IV @ NWS + RAWS',horizontalalignment='center', verticalalignment='top', fontsize=14, color=text_color)\n",
        "  else:\n",
        "    fig.text(0.5, 1.08,f'Valid: {summary_valid_title} | NBM Lead Time (Days): {nbm_lead_time} | Points: NWS + RAWS',horizontalalignment='center', verticalalignment='top', fontsize=14, color=text_color)\n",
        "\n",
        "  ax1 = fig.add_subplot()\n",
        "\n",
        "\n",
        "  #bulk_df = pd.concat([bulk_stats[\"WR\"], bulk_stats[\"CR\"], bulk_stats[\"ER\"], bulk_stats[\"SR\"]])\n",
        "\n",
        "  ys = []\n",
        "  xs = [0.05,0.15,0.25,0.35,0.45,0.55,0.65,0.75,0.85,0.95]\n",
        "  for p in range(10,110,10):\n",
        "    if percentile_stat == \"nonexceedance\":\n",
        "      try:\n",
        "        ys.append(len(bulk_stats[dataframeid][(bulk_stats[dataframeid].ob < float(threshold)) & (bulk_stats[dataframeid].prob_exceedance <= p) & (bulk_stats[dataframeid].prob_exceedance > (p-10))]) / len(bulk_stats[dataframeid][(bulk_stats[dataframeid].prob_exceedance <= p) & (bulk_stats[dataframeid].prob_exceedance > (p-10))]))\n",
        "      except:\n",
        "        ys.append(np.nan)\n",
        "    else:\n",
        "      try:\n",
        "        ys.append(len(bulk_stats[dataframeid][(bulk_stats[dataframeid].ob > float(threshold)) & (bulk_stats[dataframeid].prob_exceedance <= p) & (bulk_stats[dataframeid].prob_exceedance > (p-10))]) / len(bulk_stats[dataframeid][(bulk_stats[dataframeid].prob_exceedance <= p) & (bulk_stats[dataframeid].prob_exceedance > (p-10))]))\n",
        "      except:\n",
        "        ys.append(np.nan)\n",
        "  if np.isnan(ys).any():\n",
        "    print(\"Uh-oh, you got some empty bins, might want to double-check some things\")\n",
        "    ys_tointerp = pd.Series(ys)\n",
        "    ys = ys_tointerp.interpolate()\n",
        "\n",
        "  ax1.set_aspect('equal')\n",
        "  ax1.plot(xs, xs, color='grey', linestyle='--')\n",
        "  ax1.plot(xs, ys, color='steelblue', marker='o')\n",
        "  for tick in ax1.get_xticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  for tick in ax1.get_yticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  ax1.tick_params(axis='y',labelsize=12, color=text_color)\n",
        "  ax1.set_title(dataframeid, color=text_color, fontsize=14, weight='bold')\n",
        "  ax1.set_xlabel(\"Forecast Probability\", color=text_color, fontsize=14)\n",
        "  ax1.set_ylabel(\"Observed Relative Frequency\", color=text_color, fontsize=14)\n",
        "  ax1.fill_between(xs, ys, xs, where=(np.subtract(ys, xs) > 0), interpolate=True, color=color1, alpha=0.25)\n",
        "  ax1.fill_between(xs, xs, ys, where=(np.subtract(ys, xs) < 0), interpolate=True, color=color0, alpha=0.25)\n",
        "\n",
        "\n",
        "  if percentile_stat == \"nonexceedance\":\n",
        "    ax1.text(0.98,0.01,f'ob hits = {len(bulk_stats[dataframeid][(bulk_stats[dataframeid].ob < float(threshold))])}', horizontalalignment='right', size=10, color='grey')\n",
        "\n",
        "  else:\n",
        "    ax1.text(0.98,0.01,f'ob hits = {len(bulk_stats[dataframeid][(bulk_stats[dataframeid].ob > float(threshold))])}', horizontalalignment='right', size=10, color='grey')\n",
        "\n",
        "figname=region_name+\"_\"+element+\"_\"+str(threshold)+\"_reliability.png\"\n",
        "plt.savefig(figname, facecolor=fig.get_facecolor(), bbox_inches='tight', pad_inches=0.2, dpi='figure')"
      ],
      "metadata": {
        "id": "bjnoAlSPwM2x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}