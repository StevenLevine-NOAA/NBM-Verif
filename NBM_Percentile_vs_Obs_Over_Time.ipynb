{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0S6wIBZGAVLgePyzimp0P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StevenLevine-NOAA/NBM-Verif/blob/notebooks/NBM_Percentile_vs_Obs_Over_Time.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Obs vs NBM Percentile Time Series**\n",
        "This python notebook grabs NBM percentile, core deterministic, and obs data for selected variables and sites over a particular county warning area.\n",
        "The idea here is to look at verification of a specific event (date/time) and to look at forecasts leading up to that event.  Hopefully, forecasts improve as forecast hour decreases.\n",
        "If desired, there is an option to look only at longer range forecasts.\n",
        "\n",
        "Three plotting options are available.  They are in separate cells so you can run or skip any or all.\n",
        "\n",
        "Option one takes the mean forecast for NBM determinsitic, NBM 10th, 50th and 90th percentile for all obs in the warning area leading up to verification time (forecast hour 00).\n",
        "\n",
        "Option two plots where the verifying ob lands for each site in NBM percentile space leading up to verification time (forecast hour 00).\n",
        "\n",
        "Option three plots where the NBM determinsitc forecast lands for each site in NBM percentile space leading up to verification time (forecast hour 00).\n",
        "\n",
        " 14-March 2023: Added Capability for maximum wind (no determinsitic data available)\n",
        "\n",
        " 30-June-2023: Fixed nan bug in loop of map plots.\n",
        "\n",
        " 10-July-2023: Changed interpolator so s=0, ensuring interpolation line goes through relevant points.  Also fixed bug where 0-precip obs were being replaced with NaNs (they are now 0's)\n",
        "\n",
        " 18-July-2023: Forecast lookback now goes every 12 hours rather than 24.  May add 6-hour intervals where possible in the future.\n",
        "\n",
        "*-Steve Levine NWS MDL/SMD 2-Mar-2023*"
      ],
      "metadata": {
        "id": "XG6WA3uiyawK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT:** These first two cells must be run in the proper before anything else when starting this notebook.  However the only need to be run once.\n",
        "\n",
        "After running cell 1, you will get a crash message.  You will need to wait a few seconds after it appears, but you can otherwise ignore it and run cell 2."
      ],
      "metadata": {
        "id": "QfOHkrw11Yss"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bj5RX9zRri35",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Initialize Notebook Part 1\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8jBHGABirWmp",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Initialize Notebook Part 2\n",
        "!mamba install -q -c conda-forge cartopy contextily pyproj pyepsg pygrib netCDF4\n",
        "\n",
        "import numpy as np\n",
        "from scipy.interpolate import CubicSpline as cs, UnivariateSpline as us\n",
        "import pandas as pd\n",
        "from urllib.request import urlretrieve, urlopen\n",
        "import requests\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "from netCDF4 import Dataset\n",
        "import pygrib\n",
        "import pyproj\n",
        "from pyproj import Proj, transform\n",
        "import os, re, sys, traceback\n",
        "#from google.colab import runtime\n",
        "\n",
        "import matplotlib\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "#from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.axes as maxes\n",
        "import matplotlib.patheffects as PathEffects\n",
        "from matplotlib.path import Path\n",
        "from matplotlib.textpath import TextToPath\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib.font_manager import FontProperties\n",
        "matplotlib.rcParams['font.sans-serif'] = 'Liberation Sans'\n",
        "matplotlib.rcParams['font.family'] = \"sans-serif\"\n",
        "from matplotlib.cm import get_cmap\n",
        "import seaborn as sns\n",
        "\n",
        "from cartopy import crs as ccrs, feature as cfeature\n",
        "from cartopy.io.shapereader import Reader\n",
        "from cartopy.feature import ShapelyFeature\n",
        "import contextily as cx\n",
        "import itertools\n",
        "\n",
        "import zipfile\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AFTER INITIALIZATION**\n",
        "The cells above only need to be run once.  Now, you can change the options below and run as many times as you want.  Each run will create new output files.\n",
        "\n",
        "**NOTE:** \"NWS\" In network_selection means METAR sites.\n",
        "use_stageiv only applies when looking at qpf.\n",
        "\n",
        "\"qpf\" means qpf24."
      ],
      "metadata": {
        "id": "dqeAxzG32fxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select Forecasts to Verify and Download Ob Data {display-mode: \"form\"}\n",
        "\n",
        "element = \"snow\" #@param [\"maxt\",\"mint\",\"qpf\",\"maxwind\",\"snow\"]\n",
        "use_stageiv = True #@param {type:\"boolean\"}\n",
        "valid_date = \"2023-07-15\" #@param{type:\"date\"}\n",
        "fcst_start = \"2023-07-11\" #@param{type:\"date\"}\n",
        "fcst_end = \"2023-07-14\" #@param{type:\"date\"}\n",
        "region_selection = \"CWA\" #@param [\"WR\",\"SR\",\"ER\",\"CR\",\"CONUS\",\"CWA\"]\n",
        "network_selection=\"NWS\" #@param [\"NWS\", \"RAWS\", \"NWS+RAWS\", \"NWS+RAWS+HADS\", \"ALL\", \"CUSTOM\", \"LIST\"]\n",
        "cwa_id=\"ILX\" #@param {type:\"string\"}\n",
        "#compare_to=\"NBM_D\" #@param [\"NBM_P50\", \"NBM_D\", \"MOS\"]\n",
        "use_stageiv = True #@param {type:\"boolean\"}\n",
        "use_nohrsc = True #@param {type:\"boolean\"}\n",
        "#nbm_init_hour=\"12\"\n",
        "export_ob_csv = True #@param {type:\"boolean\"}\n",
        "export_df_csv = True #@param {type:\"boolean\"}\n",
        "\n",
        "def K_to_F(kelvin):\n",
        "  fahrenheit = 1.8*(kelvin-273)+32.\n",
        "  return fahrenheit\n",
        "\n",
        "def mm_to_in(millimeters):\n",
        "  inches = millimeters * 0.0393701\n",
        "  return inches\n",
        "\n",
        "def mps_to_kts(mps):\n",
        "  kts = mps * 1.94384\n",
        "  return kts\n",
        "\n",
        "def meters_to_in(meters):\n",
        "  inches = meters*39.3701\n",
        "  return inches\n",
        "\n",
        "fcst_start_date=datetime.strptime(fcst_start,'%Y-%m-%d')\n",
        "fcst_end_date=datetime.strptime(fcst_end,'%Y-%m-%d')\n",
        "fcst_valid_date=datetime.strptime(valid_date,'%Y-%m-%d')\n",
        "\n",
        "if fcst_start_date > fcst_end_date:\n",
        "  raise Exception(\"FATAL ERROR: INVALID DATE RANGE (start date is later than end date)\")\n",
        "elif fcst_valid_date <= fcst_end_date:\n",
        "  raise Exception(\"FATAL ERROR: INVALID DATE RANGE (Valid date is before forecast end date)\")\n",
        "elif fcst_start_date >= fcst_valid_date:\n",
        "  raise Exception(\"FATAL ERROR: INVALID DATE RANGE (Forecast start date is later than valid date)\")\n",
        "\n",
        "#nbm_init = datetime.strptime(nbm_init_date,'%Y-%m-%d') + timedelta(hours=int(nbm_init_hour))\n",
        "\n",
        "if element == \"maxt\":\n",
        "  nbm_core_valid_hour=\"00\"\n",
        "  nbm_qmd_valid_hour=\"06\"\n",
        "  valid_date_start = datetime.strptime(valid_date,'%Y-%m-%d')\n",
        "  valid_date_end = datetime.strptime(valid_date,'%Y-%m-%d') + timedelta(days=1)\n",
        "  obs_start_hour = \"1200\"\n",
        "  obs_end_hour = \"0600\"\n",
        "  ob_stat = \"maximum\"\n",
        "  valid_end_datetime = valid_date_end + timedelta(hours=(int(obs_end_hour)/100))\n",
        "  rangeOfDays=fcst_valid_date-fcst_start_date\n",
        "  if rangeOfDays.days > 9:\n",
        "    print(\"     ‚ùå FATAL ERROR: Can only look back up to 9 days for maxT!\")\n",
        "    raise Exception(\"Based on your entries for fcst_start and valid_date, you are looking back \" + str(rangeOfDays.days) + \" days.  Aborting...\")\n",
        "\n",
        "elif element == \"mint\":\n",
        "  nbm_core_valid_hour=\"12\"\n",
        "  nbm_qmd_valid_hour=\"18\"\n",
        "  valid_date_start = datetime.strptime(valid_date,'%Y-%m-%d')\n",
        "  valid_date_end = datetime.strptime(valid_date,'%Y-%m-%d')\n",
        "  obs_start_hour = \"0000\"\n",
        "  obs_end_hour = \"1800\"\n",
        "  ob_stat = \"minimum\"\n",
        "  valid_end_datetime = valid_date_end + timedelta(hours=(int(obs_end_hour)/100))\n",
        "  rangeOfDays=valid_date_start-valid_date_start\n",
        "  if rangeOfDays.days > 9:\n",
        "    print(\"     ‚ùå FATAL ERROR: Can only look back up to 9 days for minT!\")\n",
        "    raise Exception(\"Based on your entries for fcst_start and valid_date, you are looking back \" + str(rangeOfDays.days) + \" days.  Aborting...\")\n",
        "\n",
        "elif element == \"qpf\" or element == \"snow\":\n",
        "  nbm_core_valid_hour = \"12\" #str(qpf_valid_time)\n",
        "  nbm_valid_hour = \"12\" #str(qpf_valid_time)\n",
        "  nbm_qmd_valid_hour=\"12\" #str(qpf_valid_time)\n",
        "  valid_date = datetime.strptime(valid_date,'%Y-%m-%d') + timedelta(hours=12) #timedelta(hours=int(qpf_valid_time))\n",
        "  valid_date_start = valid_date - timedelta(hours=24)\n",
        "  valid_date_end = valid_date\n",
        "  obs_start_hour = \"1200\" #str(qpf_valid_time)+\"00\"\n",
        "  obs_end_hour = \"1200\" #str(qpf_valid_time)+\"00\"\n",
        "  ob_stat = \"total\"\n",
        "  valid_end_datetime = valid_date_end\n",
        "  rangeOfDays=valid_date_end-valid_date_start\n",
        "  if rangeOfDays.days > 9:\n",
        "    print(\"     ‚ùå FATAL ERROR: Can only look back up to 9 days for QPF24!\")\n",
        "    raise Exception(\"Based on your entries for fcst_start and valid_date, you are looking back \" + str(rangeOfDays.days) + \" days.  Aborting...\")\n",
        "#elif element == \"snow\":\n",
        "#  nbm_core_valid_hour = \"12\"\n",
        "#  nbm_valid_hour = \"12\"\n",
        "#  nbm_qmd_valid_hour = \"12\"\n",
        "#  valid_date = datetime.strptime(valid_date,'%Y-%m-%d') + timedelta(hours=12) #timedelta(hours=int(qpf_valid_time))\n",
        "#  valid_date_start = valid_date - timedelta(hours=24)\n",
        "#  valid_date_end = valid_date\n",
        "#  obs_start_hour = \"1200\" #str(qpf_valid_time)+\"00\"\n",
        "#  obs_end_hour = \"1200\" #str(qpf_valid_time)+\"00\"\n",
        "#  ob_stat = \"total\"\n",
        "#  valid_end_datetime = valid_date_end\n",
        "#  rangeOfDays=valid_date_end-valid_date_start\n",
        "elif element == \"maxwind\":\n",
        "  nbm_qmd_valid_hour=\"06\"\n",
        "  obs_start_hour=\"0600\"\n",
        "  obs_end_hour=\"0600\"\n",
        "  ob_stat=\"maximum\"\n",
        "  valid_date_start = datetime.strptime(valid_date,'%Y-%m-%d')\n",
        "  valid_date_end = datetime.strptime(valid_date,'%Y-%m-%d') + timedelta(days=1)\n",
        "  valid_end_datetime=valid_date_end + timedelta(hours=(int(obs_end_hour)/100))\n",
        "  rangeOfDays=valid_date_end-valid_date_start\n",
        "  #nbm_qmd_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_qmd_valid_hour))\n",
        "  if rangeOfDays.days > 9:\n",
        "    print(\"     ‚ùå FATAL ERROR: Can only look back up to 9 days for Maxwind!\")\n",
        "    raise Exception(\"Based on your entries for fcst_start and valid_date, you are looking back \" + str(rangeOfDays.days) + \" days.  Aborting...\")\n",
        "\n",
        "#set default website strings\n",
        "synoptic_token = \"ea497aaa40464749b903ac1204fd8020\"\n",
        "statistics_api = \"https://api.synopticlabs.org/v2/stations/statistics?\"\n",
        "precipitation_api = \"https://api.synopticdata.com/v2/stations/precipitation?\"\n",
        "metadata_api = \"https://api.synopticdata.com/v2/stations/metadata?\"\n",
        "\n",
        "# Setup a dictionary for translating a form selection into a something we can pass to mesowest API\n",
        "network_dict = {\"NWS+RAWS+HADS\":\"&network=1,2,106\",\"NWS+RAWS\":\"&network=1,2\", \"NWS\":\"&network=1\", \"RAWS\": \"&network=2\", \"ALL\":\"\"} #, \"CUSTOM\": \"&network=\"+network_input, \"LIST\": \"&stid=\"+network_input}\n",
        "network_string = network_dict[network_selection]\n",
        "\n",
        "if region_selection == \"CONUS\":\n",
        "  region_list = [\"WR\", \"CR\", \"SR\", \"ER\"]\n",
        "elif region_selection == \"CWA\":\n",
        "  cwa_selection = [cwa_id]\n",
        "else:\n",
        "  cwa_selection = [region_selection]\n",
        "\n",
        "def cwa_list(input_region):\n",
        "  region_dict={\"WR\":\"BYZ,BOI,LKN,EKA,FGZ,GGW,TFX,VEF,LOX,MFR,MTR,MSO,PDT,PSR,PIH,PQR,REV,STO,SLC,SGX,HNX,SEW,OTX,TWC\",\n",
        "              \"CR\":\"ABR,BIS,CYS,LOT,DVN,BOU,DMX,DTX,DDC,DLH,FGF,GLD,GJT,GRR,GRB,GID,IND,JKL,EAX,ARX,ILX,LMK,MQT,MKX,MPX,LBF,APX,IWX,OAX,PAH,PUB,UNR,RIW,FSD,SGF,LSX,TOP,ICT\",\n",
        "              \"ER\":\"ALY,LWX,BGM,BOX,BUF,BTV,CAR,CTP,RLX,CHS,ILN,CLE,CAE,GSP,MHX,OKX,PHI,PBZ,GYX,RAH,RNK,AKQ,ILM\",\n",
        "              \"SR\":\"ABQ,AMA,FFC,EWX,BMX,BRO,CRP,EPZ,FWD,HGX,HUN,JAN,JAX,KEY,MRX,LCH,LZK,LUB,MLB,MEG,MFL,MOB,MAF,OHX,LIX,OUN,SJT,SHV,TAE,TBW,TSA\"}\n",
        "  if(input_region in [\"WR\",\"CR\",\"SR\",\"ER\"]):\n",
        "    cwas_list=region_dict[input_region]\n",
        "  else:\n",
        "    cwas_list=input_region\n",
        "  return cwas_list\n",
        "\n",
        "def ll_to_index(datalons, datalats, loclon, loclat):\n",
        "  abslat = np.abs(datalats-loclat)\n",
        "  abslon = np.abs(datalons-loclon)\n",
        "  c = np.maximum(abslon, abslat)\n",
        "  latlon_idx_flat = np.argmin(c)\n",
        "  latlon_idx = np.unravel_index(latlon_idx_flat, datalons.shape)\n",
        "  return(latlon_idx)\n",
        "\n",
        "def project_hrap(lon, lat, s4x, s4y):\n",
        "  lon = float(lon)\n",
        "  lat = float(lat)\n",
        "\n",
        "  globe = ccrs.Globe(semimajor_axis=6371200)\n",
        "  hrap_ccrs = proj = ccrs.Stereographic(central_latitude=90.0,\n",
        "                          central_longitude=255.0,\n",
        "                          true_scale_latitude=60.0, globe=globe)\n",
        "  latlon_ccrs = ccrs.PlateCarree()\n",
        "  hrap_coords = hrap_ccrs.transform_point(lon,lat,src_crs=latlon_ccrs)\n",
        "  hrap_idx = ll_to_index(s4x, s4y, hrap_coords[0], hrap_coords[1])\n",
        "  return hrap_idx\n",
        "\n",
        "def nohrsc_ll2ij(lon,lat,gridlons,gridlats):\n",
        "  #for a lat/lon grid\n",
        "  lon = float(lon)\n",
        "  lat = float(lat)\n",
        "  lonidx=(np.abs(lon-gridlons)).argmin()\n",
        "  latidx=(np.abs(lat-gridlats)).argmin()\n",
        "  return(latidx,lonidx)\n",
        "\n",
        "def get_stageiv():\n",
        "  siv_url = \"https://water.weather.gov/precip/downloads/\"+valid_date_end.strftime('%Y')+\"/\"+valid_date_end.strftime('%m')+\"/\"+valid_date_end.strftime('%d')+\"/nws_precip_1day_\"+valid_date_end.strftime('%Y%m%d')+\"_conus.nc\"\n",
        "  data = urlopen(siv_url).read()\n",
        "\n",
        "  nc = Dataset('data', memory=data)\n",
        "  #with Dataset(siv_file, 'r') as nc:\n",
        "  stageIV = nc.variables['observation']\n",
        "  s4x = nc.variables['x']\n",
        "  s4y = nc.variables['y']\n",
        "  return stageIV, s4x, s4y\n",
        "\n",
        "def get_nohrsc():\n",
        "  nohrsc_url = \"https://www.nohrsc.noaa.gov/snowfall_v2/data/\"+valid_date_end.strftime('%Y%m')+\"/sfav2_CONUS_24h_\"+valid_date_end.strftime('%Y%m%d%H')+\".nc\"\n",
        "  data = urlopen(nohrsc_url).read()\n",
        "\n",
        "  nc = Dataset('data',memory=data)\n",
        "  snow=np.asarray(nc.variables['Data']) #make lon by lat array (original lat by lon)\n",
        "  snowlat = np.asarray(nc.variables['lat'])\n",
        "  snowlon = np.asarray(nc.variables['lon'])\n",
        "  return snow,snowlon,snowlat\n",
        "\n",
        "\n",
        "#downlaod/process obs\n",
        "print('Getting obs...')\n",
        "obs={}\n",
        "if os.path.exists(\"obs\"):\n",
        "  pass\n",
        "else:\n",
        "  os.mkdir(\"obs\")\n",
        "for cwa in cwa_selection:\n",
        "  json_name=\"obs/Obs_\"+element+\"_\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour+\"_\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour+\"_\"+cwa+\".json\"\n",
        "  if os.path.exists(json_name):\n",
        "    print(\"Skipping download since JSON obs file already exists\")\n",
        "    pass\n",
        "  else:\n",
        "    if element == \"mint\" or element == \"maxt\":\n",
        "      api_token = \"&token=\"+synoptic_token\n",
        "      station_query = \"&cwa=\"+cwa #cwa_list(region)\n",
        "      vars_query = \"&vars=air_temp\"\n",
        "      start_query = \"&start=\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour\n",
        "      end_query = \"&end=\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour\n",
        "      stat_type = \"&type=\"+ob_stat\n",
        "      network_query = network_string\n",
        "      api_extras = \"&units=temp%7Cf&within=1440&status=active\"\n",
        "      obs_url = statistics_api + api_token + station_query + vars_query + start_query + end_query + stat_type + network_query + api_extras\n",
        "    elif element == \"maxwind\":\n",
        "      api_token = \"&token=\"+synoptic_token\n",
        "      station_query = \"&cwa=\"+cwa #cwa_list(region)\n",
        "      vars_query = \"&vars=wind_speed\"\n",
        "      start_query = \"&start=\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour\n",
        "      end_query = \"&end=\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour\n",
        "      stat_type = \"&type=\"+ob_stat\n",
        "      network_query = network_string\n",
        "      obs_url = statistics_api + api_token + station_query + vars_query + start_query + end_query + stat_type + network_query\n",
        "    elif element == \"qpf\":\n",
        "      if use_stageiv:\n",
        "        print(\"Downloading stage IV data...\")\n",
        "        api_token = \"&token=\"+synoptic_token\n",
        "        station_query = \"&cwa=\"+cwa #+cwa_list(region)\n",
        "        api_extras = \"&fields=status,latitude,longitude,name,elevation\"\n",
        "        network_query = network_string\n",
        "        obs_url = metadata_api + api_token + station_query + network_query + api_extras\n",
        "        stageIV, s4xs, s4ys = get_stageiv()\n",
        "        print(\"Meshing stage IV data...\")\n",
        "        s4xs, s4ys = np.meshgrid(s4xs, s4ys)\n",
        "      else:\n",
        "        api_token = \"&token=\"+synoptic_token\n",
        "        station_query = \"&cwa=\"+cwa #+cwa_list(region)\n",
        "        api_extras = \"&fields=status,latitude,longitude,name,elevation&obtimezone=utc\"\n",
        "        network_query = network_string\n",
        "        vars_query = \"&pmode=totals\"\n",
        "        units_query = \"&units=precip|in\"\n",
        "        start_query = \"&start=\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour\n",
        "        end_query = \"&end=\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour\n",
        "        obs_url = precipitation_api + api_token + station_query + network_query + vars_query + start_query + end_query + units_query + api_extras\n",
        "    elif element == \"snow\":\n",
        "      if use_nohrsc:\n",
        "        print(\"Using NOHRSC data\")\n",
        "        api_token = \"&token=\"+synoptic_token\n",
        "        station_query = \"&cwa=\"+cwa #+cwa_list(region)\n",
        "        api_extras = \"&fields=status,latitude,longitude,name,elevation\"\n",
        "        network_query = network_string\n",
        "        obs_url = metadata_api + api_token + station_query + network_query + api_extras\n",
        "        snow,snowlon,snowlat = get_nohrsc()\n",
        "        snowlons,snowlats = np.meshgrid(snowlon,snowlat)\n",
        "      else:\n",
        "        api_token = \"&token=\"+synoptic_token\n",
        "        station_query = \"&cwa=\"+cwa_list(region)\n",
        "        api_extras = \"&fields=status,latitude,longitude,name,elevation&obtimezone=utc\"\n",
        "        network_query = network_string\n",
        "        vars_query = \"&pmode=totals\"\n",
        "        units_query = \"&units=precip|in\"\n",
        "        start_query = \"&start=\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour\n",
        "        end_query = \"&end=\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour\n",
        "        obs_url = precipitation_api + api_token + station_query + network_query + vars_query + start_query + end_query + units_query + api_extras\n",
        "    print(\"Obs url: \" + obs_url)\n",
        "    urlretrieve(obs_url,json_name)\n",
        "\n",
        "if os.path.exists(json_name):\n",
        "  with open(json_name) as json_file:\n",
        "    obs_json = json.load(json_file)\n",
        "    print (\"Loaded Obs JSON file line 219: \" + json_name)\n",
        "    obs_lats = []\n",
        "    obs_lons = []\n",
        "    obs_value = []\n",
        "    obs_elev = []\n",
        "    obs_stid = []\n",
        "    obs_name = []\n",
        "    for stn in obs_json[\"STATION\"]:\n",
        "      if stn[\"STID\"] is None:\n",
        "        stid = \"N0N3\"\n",
        "      else:\n",
        "        stid = stn[\"STID\"]\n",
        "        name = stn[\"NAME\"]\n",
        "        if stn[\"ELEVATION\"] and stn[\"ELEVATION\"] is not None:\n",
        "          elev = stn[\"ELEVATION\"]\n",
        "        else:\n",
        "          elev = -999\n",
        "        lat = stn[\"LATITUDE\"]\n",
        "        lon = stn[\"LONGITUDE\"]\n",
        "        if element == \"mint\" or element == \"maxt\":\n",
        "          if 'air_temp_set_1' in stn['STATISTICS'] and stn['STATISTICS']['air_temp_set_1']:\n",
        "            if ob_stat in stn['STATISTICS']['air_temp_set_1'] and float(stn[\"LATITUDE\"]) != 0.:\n",
        "              stat = stn['STATISTICS']['air_temp_set_1'][ob_stat]\n",
        "              obs_stid.append(str(stid))\n",
        "              obs_name.append(str(name))\n",
        "              obs_elev.append(float(elev))\n",
        "              obs_lats.append(float(lat))\n",
        "              obs_lons.append(float(lon))\n",
        "              obs_value.append(float(stat))\n",
        "        elif element == \"maxwind\":\n",
        "          if 'wind_speed_set_1' in stn['STATISTICS'] and stn['STATISTICS']['wind_speed_set_1']:\n",
        "            if ob_stat in stn['STATISTICS']['wind_speed_set_1'] and float(stn[\"LATITUDE\"]) != 0.:\n",
        "              stat = stn['STATISTICS']['wind_speed_set_1'][ob_stat]\n",
        "              obs_stid.append(str(stid))\n",
        "              obs_name.append(str(name))\n",
        "              obs_elev.append(float(elev))\n",
        "              obs_lats.append(float(lat))\n",
        "              obs_lons.append(float(lon))\n",
        "              obs_value.append(mps_to_kts(float(stat)))\n",
        "        elif element == \"qpf\" or element == \"snow\":\n",
        "          if (stn[\"STATUS\"] == \"ACTIVE\") and float(stn[\"LATITUDE\"]) < 50.924 and float(stn[\"LONGITUDE\"]) > -125.650 and float(stn[\"LONGITUDE\"]) < -66.008:\n",
        "            obs_stid.append(str(stid))\n",
        "            obs_name.append(str(name))\n",
        "            obs_elev.append(float(elev))\n",
        "            obs_lats.append(float(lat))\n",
        "            obs_lons.append(float(lon))\n",
        "            if element == \"qpf\" and use_stageiv is True:\n",
        "              coords = project_hrap(lon,lat,s4xs,s4ys)\n",
        "              siv_value = float(stageIV[coords])\n",
        "              if (siv_value >= 0.01):\n",
        "                obs_value.append(siv_value)\n",
        "              else:\n",
        "                obs_value.append(0.0)\n",
        "            elif  element == \"qpf\" and use_stageiv is False:\n",
        "              if \"precipitation\" in stn[\"OBSERVATIONS\"]:\n",
        "                if \"total\" in stn[\"OBSERVATIONS\"][\"precipitation\"][0]:\n",
        "                  ptotal = stn[\"OBSERVATIONS\"][\"precipitation\"][0][\"total\"]\n",
        "                  if ptotal >= 0.005:\n",
        "                    obs_value.append(ptotal)\n",
        "                  else:\n",
        "                    obs_value.append(0.0)\n",
        "                else:\n",
        "                  obs_value.append(np.nan)\n",
        "              else:\n",
        "                obs_value.append(np.nan)\n",
        "            elif use_nohrsc and element == \"snow\":\n",
        "              coords = nohrsc_ll2ij(lon,lat,snowlon,snowlat)\n",
        "              nohrsc_value = meters_to_in(float(snow[coords]))\n",
        "              if nohrsc_value >= 0.005:\n",
        "                obs_value.append(nohrsc_value)\n",
        "              elif nohrsc_value < 0.0:\n",
        "                obs_value.append(np.nan)\n",
        "              else:\n",
        "                obs_value.append(0.0)\n",
        "            else:\n",
        "              if \"precipitation\" in stn[\"OBSERVATIONS\"]:\n",
        "                if \"total\" in stn[\"OBSERVATIONS\"][\"precipitation\"][0]:\n",
        "                  ptotal = stn[\"OBSERVATIONS\"][\"precipitation\"][0][\"total\"]\n",
        "                  if ptotal >= 0.01:\n",
        "                    obs_value.append(ptotal)\n",
        "                  else:\n",
        "                    obs_value.append(np.nan)\n",
        "                else:\n",
        "                    obs_value.append(np.nan)\n",
        "              else:\n",
        "                obs_value.append(np.nan)\n",
        "    print(\"Finished loading JSON obs into dictionary!\")\n",
        "    csv_name=\"obs_\"+element+\"_\"+cwa+\"_\"+fcst_valid_date.strftime(\"%Y%m%d\")+\".csv\"\n",
        "    obs[cwa] = pd.DataFrame()\n",
        "    obs[cwa][\"stid\"] = obs_stid\n",
        "    obs[cwa][\"name\"] = obs_name\n",
        "    obs[cwa][\"elevation\"] = obs_elev\n",
        "    obs[cwa][\"lat\"] = obs_lats\n",
        "    obs[cwa][\"lon\"] = obs_lons\n",
        "    #if element == \"maxwind\":\n",
        "    #  obs[cwa][\"ob_\"+element] = mps_to_kts(obs_value)\n",
        "    #else:\n",
        "  obs[cwa][\"ob_\"+element] = obs_value\n",
        "  if export_ob_csv is True:\n",
        "    obs[cwa].to_csv(csv_name)\n",
        "    print(\"Written Obs CSV file: \" + csv_name)\n",
        "\n"
      ],
      "metadata": {
        "id": "rZDWCZ5WD390"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we download the relevant NBM data.  Files come from the AWS 'big data' NOAA server.\n",
        "\n",
        "Note that this section will have to be re-run whenever the above cell is re-run."
      ],
      "metadata": {
        "id": "o1WsB0_Dp5dY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Forecast Data and Match with Obs\n",
        "\n",
        "def ll_to_index(datalons, datalats, loclon, loclat):\n",
        "  abslat = np.abs(datalats-loclat)\n",
        "  abslon = np.abs(datalons-loclon)\n",
        "  c = np.maximum(abslon, abslat)\n",
        "  latlon_idx_flat = np.argmin(c)\n",
        "  latlon_idx = np.unravel_index(latlon_idx_flat, datalons.shape)\n",
        "  return(latlon_idx)\n",
        "\n",
        "def find_roots(x,y):\n",
        "  s = np.abs(np.diff(np.sign(y))).astype(bool)\n",
        "  return x[:-1][s] + np.diff(x)[s]/(np.abs(y[1:][s]/y[:-1][s])+1)\n",
        "\n",
        "def download_subset(remote_url, remote_file, local_filename):\n",
        "  print(\"   > Downloading a subset of NBM gribs\")\n",
        "  if os.path.exists(\"nbm\"):\n",
        "    pass\n",
        "  else:\n",
        "    os.mkdir(\"nbm\")\n",
        "  local_file = \"nbm/\"+local_filename\n",
        "  if \"qmd\" in remote_file:\n",
        "    if element == \"maxt\":\n",
        "      if (int(nbm_qmd_forecasthour_start) % 24 == 0) and (int(nbm_qmd_forecasthour) % 24 ==0):\n",
        "        search_string = f':TMP:2 m above ground:{str(int(int(nbm_qmd_forecasthour_start)/24))}-{str(int(int(nbm_qmd_forecasthour)/24))} day max fcst:'\n",
        "      else:\n",
        "        search_string = f':TMP:2 m above ground:{str(int(nbm_qmd_forecasthour_start))}-{str(int(nbm_qmd_forecasthour))} hour max fcst:'\n",
        "    elif element == \"mint\":\n",
        "      if (int(nbm_qmd_forecasthour_start) % 24 == 0) and (int(nbm_qmd_forecasthour) % 24 ==0):\n",
        "        search_string = f':TMP:2 m above ground:{str(int(int(nbm_qmd_forecasthour_start)/24))}-{str(int(int(nbm_qmd_forecasthour)/24))} day min fcst:'\n",
        "      else:\n",
        "        search_string = f':TMP:2 m above ground:{str(int(nbm_qmd_forecasthour_start))}-{str(int(nbm_qmd_forecasthour))} hour min fcst:'\n",
        "    elif element == \"maxwind\":\n",
        "      if (int(nbm_qmd_forecasthour_start) % 24 == 0) and (int(nbm_qmd_forecasthour) % 24 == 0):\n",
        "        search_string = f':WIND:10 m above ground:{str(int(nbm_qmd_forecasthour_start/24))}-{str(int(nbm_qmd_forecasthour/24))} hour max fcst:'\n",
        "      else:\n",
        "        search_string = f':WIND:10 m above ground:{str(int(nbm_qmd_forecasthour_start))}-{str(int(nbm_qmd_forecasthour))} hour max fcst:'\n",
        "    elif element == \"qpf\":\n",
        "      if (int(nbm_qmd_forecasthour_start) % 24 == 0) and (int(nbm_qmd_forecasthour) % 24 ==0):\n",
        "        search_string = f':APCP:surface:{str(int(int(nbm_qmd_forecasthour_start)/24))}-{str(int(int(nbm_qmd_forecasthour)/24))} day acc fcst:'\n",
        "      else:\n",
        "        search_string = f':APCP:surface:{str(int(nbm_qmd_forecasthour_start))}-{str(int(nbm_qmd_forecasthour))} hour acc fcst:'\n",
        "  elif \"core\" in remote_file:\n",
        "    if element == \"maxt\":\n",
        "      search_string = f':TMAX:2 m above ground:{str(int(nbm_core_forecasthour_start))}-{str(int(nbm_core_forecasthour))} hour max fcst:'\n",
        "    elif element == \"mint\":\n",
        "      search_string = f':TMIN:2 m above ground:{str(int(nbm_core_forecasthour_start))}-{str(int(nbm_core_forecasthour))} hour min fcst:'\n",
        "    elif element == \"snow\":\n",
        "      search_string = f':ASNOW:surface:{str(int(nbm_core_forecasthour_start))}-{str(int(nbm_core_forecasthour))} hour acc'\n",
        "  #print(search_string)\n",
        "  idx = remote_url+\".idx\"\n",
        "  r = requests.get(idx)\n",
        "  if not r.ok:\n",
        "    print('     ‚ùå SORRY! Status Code:', r.status_code, r.reason)\n",
        "    print(f'      ‚ùå It does not look like the index file exists: {idx}')\n",
        "\n",
        "  lines = r.text.split('\\n')\n",
        "  expr = re.compile(search_string)\n",
        "  byte_ranges = {}\n",
        "  for n, line in enumerate(lines, start=1):\n",
        "      # n is the line number (starting from 1) so that when we call for\n",
        "      # `lines[n]` it will give us the next line. (Clear as mud??)\n",
        "\n",
        "      # Use the compiled regular expression to search the line\n",
        "      if expr.search(line):\n",
        "          # aka, if the line contains the string we are looking for...\n",
        "\n",
        "          # Get the beginning byte in the line we found\n",
        "          parts = line.split(':')\n",
        "          rangestart = int(parts[1])\n",
        "\n",
        "          # Get the beginning byte in the next line...\n",
        "          if n+1 < len(lines):\n",
        "              # ...if there is a next line\n",
        "              parts = lines[n].split(':')\n",
        "              rangeend = int(parts[1])\n",
        "          else:\n",
        "              # ...if there isn't a next line, then go to the end of the file.\n",
        "              rangeend = ''\n",
        "\n",
        "          # Store the byte-range string in our dictionary,\n",
        "          # and keep the line information too so we can refer back to it.\n",
        "          byte_ranges[f'{rangestart}-{rangeend}'] = line\n",
        "          #print(line)\n",
        "  for i, (byteRange, line) in enumerate(byte_ranges.items()):\n",
        "\n",
        "        if i == 0:\n",
        "            # If we are working on the first item, overwrite the existing file.\n",
        "            curl = f'curl -s --range {byteRange} {remote_url} > {local_file}'\n",
        "        else:\n",
        "            # If we are working on not the first item, append the existing file.\n",
        "            curl = f'curl -s --range {byteRange} {remote_url} >> {local_file}'\n",
        "        try:\n",
        "          num, byte, date, var, level, forecast, _ = line.split(':')\n",
        "        except:\n",
        "          pass\n",
        "\n",
        "        #print(f'  Downloading GRIB line [{num:>3}]: variable={var}, level={level}, forecast={forecast}')\n",
        "        os.system(curl)\n",
        "\n",
        "  if os.path.exists(local_file):\n",
        "      print(f'      ‚úÖ Success! Searched for [{search_string}] and got [{len(byte_ranges)}] GRIB fields and saved as {local_file}')\n",
        "      return local_file\n",
        "  else:\n",
        "      print(print(f'      ‚ùå Unsuccessful! Searched for [{search_string}] and did not find anything!'))\n",
        "\n",
        "\n",
        "def daterange(element,start_date,end_date):\n",
        "  end_date=end_date+timedelta(hours=12) #ending at 12Z\n",
        "  if element == \"snow\":\n",
        "    return pd.date_range(start=start_date,end=end_date,freq='12H',inclusive='left')\n",
        "  else:\n",
        "    return pd.date_range(start=start_date,end=end_date,freq='12H',inclusive='both')\n",
        "\n",
        "fcst={} #fcst is dictionary of dataframes by cwa, much like obs\n",
        "masterfcst=pd.DataFrame() #to concatenate to\n",
        "\n",
        "fcst_stop_date=fcst_end_date\n",
        "\n",
        "for nbmdate in daterange(element,fcst_start_date,fcst_stop_date):\n",
        "    nbmdatestr=nbmdate.strftime(\"%Y-%m-%d-%H\")\n",
        "    nbm_init_hour=nbmdate.strftime('%H')\n",
        "    print(\"Grabbing file from: \" + nbmdatestr)\n",
        "    if element == \"snow\":\n",
        "      nbm_init=nbmdate + timedelta(hours=1) #3)\n",
        "    else:\n",
        "      nbm_init=nbmdate #+ timedelta(hours=12)\n",
        "    #print(\"Pulling determinisitc data for: \" + nbm_init)\n",
        "    if element == \"mint\" or element == \"maxt\":\n",
        "      nbm_core_valid_end_datetime = fcst_valid_date + timedelta(hours=int(nbm_core_valid_hour)+24)\n",
        "      nbm_qmd_valid_end_datetime = fcst_valid_date + timedelta(hours=int(nbm_qmd_valid_hour)+24)\n",
        "      core_init = nbm_init + timedelta(hours = 7)\n",
        "      nbm_core_fhdelta = nbm_core_valid_end_datetime - core_init\n",
        "      nbm_core_forecasthour = nbm_core_fhdelta.total_seconds() / 3600.\n",
        "      nbm_core_forecasthour_start = nbm_core_forecasthour - 12\n",
        "    elif element == \"qpf\" or element == \"snow\":\n",
        "      core_init = nbm_init\n",
        "      nbm_core_valid_end_datetime = fcst_valid_date + timedelta(hours=int(nbm_core_valid_hour))\n",
        "      nbm_qmd_valid_end_datetime = fcst_valid_date + timedelta(hours=int(nbm_qmd_valid_hour))\n",
        "      nbm_core_fhdelta = nbm_core_valid_end_datetime - nbm_init\n",
        "      nbm_core_forecasthour = nbm_core_fhdelta.total_seconds() / 3600.\n",
        "      nbm_core_forecasthour_start = nbm_core_forecasthour - 24\n",
        "    elif element == \"maxwind\":\n",
        "      #adding core valid times here to help out with plotting later\n",
        "      #They are dummy values as there is no determinsitic data for max wind\n",
        "      nbm_qmd_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_qmd_valid_hour))\n",
        "      nbm_core_valid_end_datetime = nbm_qmd_valid_end_datetime - timedelta(hours=12)\n",
        "      nbm_qmd_fhdelta = nbm_qmd_valid_end_datetime - nbm_init\n",
        "      nbm_core_fhdelta = nbm_core_valid_end_datetime - nbm_init\n",
        "      nbm_qmd_forecasthour = nbm_qmd_fhdelta.total_seconds() / 3600.\n",
        "      nbm_core_forecasthour = nbm_core_fhdelta.total_seconds() / 3600.\n",
        "    nbm_qmd_fhdelta = nbm_qmd_valid_end_datetime - nbm_init\n",
        "    nbm_qmd_forecasthour = nbm_qmd_fhdelta.total_seconds() / 3600.\n",
        "    if element == \"qpf\" or element == \"maxwind\" or element == \"snow\":\n",
        "      nbm_qmd_forecasthour_start = nbm_qmd_forecasthour - 24\n",
        "    else:\n",
        "      nbm_qmd_forecasthour_start = nbm_qmd_forecasthour - 18\n",
        "    #get file names\n",
        "    nbm_init_filen = nbm_init.strftime('%Y%m%d') + \"_\" + nbm_init.strftime('%H')\n",
        "    nbm_url_base = \"https://noaa-nbm-grib2-pds.s3.amazonaws.com/blend.\"+nbm_init.strftime('%Y%m%d') \\\n",
        "                +\"/\"+nbm_init.strftime('%H')+\"/\"\n",
        "    if element != \"maxwind\":\n",
        "      print(\"Pulling determinstic data for hour: \" + str(nbm_core_forecasthour))\n",
        "      nbm_init_filen_core = core_init.strftime('%Y%m%d') + \"_\" + core_init.strftime('%H')\n",
        "      nbm_url_base_core = \"https://noaa-nbm-grib2-pds.s3.amazonaws.com/blend.\"+core_init.strftime('%Y%m%d') \\\n",
        "                  +\"/\"+core_init.strftime('%H')+\"/\"\n",
        "      temp_vars = [\"maxt\",\"mint\"]\n",
        "      if (element == \"qpf\"):\n",
        "        detr_file = f'blend.t{int(nbm_init_hour):02}z.qmd.f{int(nbm_qmd_forecasthour):03}.co.grib2'\n",
        "        detr_file_subset = f'blend.t{int(nbm_init_hour):02}z.qmd.{nbm_init_filen}{nbm_init_filen}f{int(nbm_qmd_forecasthour):03}.co.{element}_subset.grib2'\n",
        "        if (int(nbm_qmd_forecasthour) < 12):\n",
        "          break\n",
        "        detr_url = nbm_url_base+\"qmd/\"+detr_file\n",
        "\n",
        "      else: #temperature or snow\n",
        "      #elif any(te in element for te in temp_vars):\n",
        "        detr_file = f\"blend.t{int(core_init.strftime('%H')):02}z.core.f{int(nbm_core_forecasthour):03}.co.grib2\"\n",
        "        detr_file_subset = f\"blend.t{int(core_init.strftime('%H')):02}z.core.{nbm_init_filen_core}f{int(nbm_core_forecasthour):03}.co.{element}_subset.grib2\"\n",
        "        if (int(nbm_core_forecasthour) < 12):\n",
        "          break\n",
        "        detr_url = nbm_url_base_core+\"core/\"+detr_file\n",
        "\n",
        "      #download file if we don't have it yet\n",
        "      if os.path.exists(\"nbm/\"+detr_file_subset):\n",
        "        print(\"   > NBM deterministic already exists\")\n",
        "      else:\n",
        "        print(\"   > Getting NBM deterministic\")\n",
        "        if os.path.exists(\"nbm\"):\n",
        "          pass\n",
        "        else:\n",
        "          os.mkdir(\"nbm\")\n",
        "      #urlretrieve(detr_url, \"nbm/\"+detr_file)\n",
        "      download_subset(detr_url, detr_file, detr_file_subset)\n",
        "      #extract deterministic values\n",
        "      nbmd = pygrib.open(\"nbm/\"+detr_file_subset)\n",
        "      if element == \"maxt\":\n",
        "        deterministic = nbmd.select(name=\"Maximum temperature\",lengthOfTimeRange=12, stepTypeInternal=\"max\")[0]\n",
        "        deterministic_array = K_to_F(deterministic.values)\n",
        "      elif element == \"mint\":\n",
        "        deterministic = nbmd.select(name=\"Minimum temperature\",lengthOfTimeRange=12, stepTypeInternal=\"min\")[0]\n",
        "        deterministic_array = K_to_F(deterministic.values)\n",
        "      elif element == \"qpf\":\n",
        "        deterministic = nbmd.select(name=\"Total Precipitation\",lengthOfTimeRange=24)[-1]\n",
        "        deterministic_array = mm_to_in(deterministic.values)\n",
        "      elif element == \"snow\":\n",
        "        #deterministic = nbmd.select(name=\"Total snowfall\",lengthOfTimeRange=24)[7] #look at 8th record\n",
        "        deterministic = nbmd.select(name=\"unknown\",lengthOfTimeRange=24)[7]\n",
        "        deterministic_array = mm_to_in(deterministic.values)\n",
        "      nbmlats, nbmlons = deterministic.latlons()\n",
        "      nbmd.close()\n",
        "\n",
        "      for cwa in cwa_selection:\n",
        "        fcst[cwa] = pd.DataFrame()\n",
        "        point_lats = obs[cwa][\"lat\"].values\n",
        "        point_lons = obs[cwa][\"lon\"].values\n",
        "        stations = obs[cwa][\"stid\"].values\n",
        "        obvals = obs[cwa][\"ob_\"+element].values\n",
        "        fcsthrs=np.full(len(stations),nbm_core_forecasthour)\n",
        "        detr_values = []\n",
        "        nbm_fidx = []\n",
        "      #if element == \"maxwind\":\n",
        "        #detr_values=np.empty(np.shape(stations))\n",
        "        #detr_values.fill(np.nan)\n",
        "        #for i in range(0,len(point_lats)):\n",
        "        #  coords = ll_to_index(nbmlons,nbmlats,point_lons[i],point_lats[i])\n",
        "        #  nbm_fidx.append(coords)\n",
        "      #else:\n",
        "        for i in range(0, len(point_lats)):\n",
        "          coords = ll_to_index(nbmlons,nbmlats,point_lons[i],point_lats[i])\n",
        "          nbm_fidx.append(coords)\n",
        "          detr_value = deterministic_array[coords]\n",
        "          detr_values.append(detr_value)\n",
        "        fcst[cwa][\"stid\"] = stations\n",
        "        fcst[cwa][\"NBM_fidx\"] = nbm_fidx\n",
        "        fcst[cwa][\"NBM_D\"] = detr_values\n",
        "        fcst[cwa][\"forecast_hour\"] = fcsthrs\n",
        "        fcst[cwa][\"OB\"] = obvals\n",
        "    else: #for maxwind (no deterministic available)\n",
        "      print(\"MAXWIND: Putting stations locations into dataframe\")\n",
        "      for cwa in cwa_selection:\n",
        "        fcst[cwa] = pd.DataFrame()\n",
        "        point_lats = obs[cwa][\"lat\"].values\n",
        "        point_lons = obs[cwa][\"lon\"].values\n",
        "        stations = obs[cwa][\"stid\"]\n",
        "        obvals = obs[cwa][\"ob_\"+element].values\n",
        "        nbm_fidx = []\n",
        "        nbmlats = None\n",
        "        nbmlons = None\n",
        "        fcsthrs=np.full(len(stations),nbm_core_forecasthour)\n",
        "        detr_values=np.empty(np.shape(stations))\n",
        "        detr_values.fill(np.nan)\n",
        "        fcst[cwa][\"stid\"] = stations\n",
        "        fcst[cwa][\"NBM_D\"] = detr_values\n",
        "        fcst[cwa][\"forecast_hour\"] = fcsthrs\n",
        "        fcst[cwa][\"OB\"] = obvals\n",
        "\n",
        "    if element == \"snow\":\n",
        "      perc_list = [5,10,25,50,75,90,95]\n",
        "    else:\n",
        "      perc_list = [1,5,10,20,30,40,50,60,70,80,90,95,99]\n",
        "    perc_dict = {\"maxt\":\"maxt18p\", \"mint\":\"mint18p\", \"qpf\":\"qpf24p\",\"snow\":\"snow24p\"}\n",
        "    if element == \"snow\":\n",
        "      perc_file=f\"blend.t{int(core_init.strftime('%H')):02}z.core.f{int(nbm_core_forecasthour):03}.co.grib2\"\n",
        "      perc_url=nbm_url_base_core+\"core/\"+perc_file\n",
        "    else:\n",
        "      perc_file = f'blend.t{int(nbm_init_hour):02}z.qmd.f{int(nbm_qmd_forecasthour):03}.co.grib2'\n",
        "      perc_url = nbm_url_base+\"qmd/\"+perc_file\n",
        "    #perc_file = f'blend.t{int(nbm_init_hour):02}z.qmd.f{int(nbm_qmd_forecasthour):03}.co.grib2'\n",
        "    perc_file_subset = f'blend.t{int(nbm_init_hour):02}z.qmd.{nbm_init_filen}{nbm_init_filen}f{int(nbm_qmd_forecasthour):03}.co.{element}_subset.grib2'\n",
        "    #perc_url = nbm_url_base+\"qmd/\"+perc_file\n",
        "    print(\"  >> perc_url = \" + perc_url)\n",
        "    print(\"  >> perc_file_subset = \" + perc_file_subset)\n",
        "    if os.path.exists(\"nbm/\"+perc_file_subset):\n",
        "      print(\"   > NBM probabilistic already exists\")\n",
        "    else:\n",
        "      #urlretrieve(perc_url, \"nbm/\"+perc_file)\n",
        "      print(\"   > Getting NBM probabilistic\")\n",
        "      download_subset(perc_url, perc_file, perc_file_subset)\n",
        "\n",
        "    nbmperc = pygrib.open(\"nbm/\"+perc_file_subset)\n",
        "    print('   > Extracting NBM Probabilistic')\n",
        "    #load percentile data\n",
        "    for perc in perc_list:\n",
        "      print(f'     >> Extracting NBM P{int(perc):01}')\n",
        "      perc_name = \"NBM_P\"+str(perc)\n",
        "      if element == \"maxt\":\n",
        "        percdata = K_to_F(nbmperc.select(name=\"2 metre temperature\", stepTypeInternal=\"max\", percentileValue=perc)[0].values)\n",
        "      elif element == \"mint\":\n",
        "        percdata = K_to_F(nbmperc.select(name=\"2 metre temperature\", stepTypeInternal=\"min\", percentileValue=perc)[0].values)\n",
        "      elif element == \"qpf\":\n",
        "        percdata = mm_to_in(nbmperc.select(name=\"Total Precipitation\",lengthOfTimeRange=24, percentileValue=perc)[0].values)\n",
        "      elif element == \"maxwind\":\n",
        "        percinv = nbmperc.select(name=\"10 metre wind speed\",lengthOfTimeRange=24,stepTypeInternal=\"max\",percentileValue=perc)[0]\n",
        "        percdata=mps_to_kts(percinv.values) #knots\n",
        "      elif element == \"snow\":\n",
        "        #percinv = nbmperc.select(name=\"Total snowfall\",lengthOfTimeRange=24,percentileValue=perc)[0]\n",
        "        percinv = nbmperc.select(name=\"unknown\",lengthOfTimeRange=24,percentileValue=perc)[0]\n",
        "        percdata=meters_to_in(percinv.values)\n",
        "      #get and load ob i/j points for maxwind obs (if not already defined from core/deterministic or previous file)\n",
        "      if nbmlats is None:\n",
        "        nbmlats,nbmlons=percinv.latlons()\n",
        "        for i in range(0,len(point_lats)):\n",
        "          coords = ll_to_index(nbmlons,nbmlats,point_lons[i],point_lats[i])\n",
        "          nbm_fidx.append(coords)\n",
        "        fcst[cwa][\"NBM_fidx\"] = nbm_fidx\n",
        "\n",
        "      #now run the percentile calculations\n",
        "      for cwa in cwa_selection:\n",
        "        nbm_coords = fcst[cwa][\"NBM_fidx\"].values\n",
        "        perc_values = []\n",
        "        for i in range(0, len(nbm_coords)):\n",
        "          perc_value = percdata[nbm_coords[i]]\n",
        "          perc_values.append(perc_value)\n",
        "        fcst[cwa][perc_name] = perc_values\n",
        "    nbmperc.close()\n",
        "    print('Creating point distribution curves and interpolating...')\n",
        "    for cwa in cwa_selection:\n",
        "      if element == \"snow\":\n",
        "        perc_start = fcst[cwa].columns.get_loc(\"NBM_P5\")\n",
        "        perc_end = fcst[cwa].columns.get_loc(\"NBM_P95\")\n",
        "      else:\n",
        "        perc_start = fcst[cwa].columns.get_loc(\"NBM_P1\")\n",
        "        perc_end = fcst[cwa].columns.get_loc(\"NBM_P99\")\n",
        "      all_percs = fcst[cwa].iloc[:, perc_start:perc_end+1].values\n",
        "      var_string = \"ob_\"+element\n",
        "      all_obs = obs[cwa][[var_string]].values\n",
        "      all_nbmd = fcst[cwa][['NBM_D']].values\n",
        "      all_lats = obs[cwa][['lat']].values\n",
        "      all_lons = obs[cwa][['lon']].values\n",
        "      obs_percs = []\n",
        "      nbmd_percs = []\n",
        "      mos_percs = []\n",
        "      for i in range(0,len(all_obs)):\n",
        "        udf = us(perc_list, all_percs[i,:], bbox=[0,100], ext=0, s=0)\n",
        "        if np.isnan(all_obs[i]):\n",
        "          ob_perc = np.nan\n",
        "        elif all_obs[i] <= udf(0):\n",
        "          ob_perc = -10\n",
        "        elif all_obs[i] >= udf(100):\n",
        "          ob_perc = 110\n",
        "        else:\n",
        "          ob_perc = find_roots(np.arange(0,101,1), udf(np.arange(0,101,1)) - all_obs[i])\n",
        "          ob_perc = ob_perc[0].round(1)\n",
        "\n",
        "        if np.isnan(all_nbmd[i]):\n",
        "          nbm_perc = np.nan\n",
        "        elif all_nbmd[i] <= udf(0):\n",
        "          nbm_perc = -10\n",
        "        elif all_nbmd[i] >= udf(100):\n",
        "          nbm_perc = 110\n",
        "        else:\n",
        "          nbm_perc = find_roots(np.arange(0,101,1), udf(np.arange(0,101,1)) - all_nbmd[i])\n",
        "          nbm_perc = nbm_perc[0].round(1)\n",
        "\n",
        "        if np.isnan(ob_perc):\n",
        "          obs_percs.append(ob_perc)\n",
        "        else:\n",
        "          obs_percs.append(int(ob_perc))\n",
        "        if np.isnan(nbm_perc):\n",
        "          nbmd_percs.append(nbm_perc)\n",
        "        else:\n",
        "          nbmd_percs.append(int(nbm_perc))\n",
        "      fcsthrs=np.full(len(nbmd_percs),nbm_core_forecasthour)\n",
        "      fcst[cwa][\"ob_perc\"] = obs_percs\n",
        "      fcst[cwa][\"NBMd_perc\"] = nbmd_percs\n",
        "      fcst[cwa][\"forecast_hour\"] = fcsthrs\n",
        "      fcst[cwa]['lat'] = all_lats\n",
        "      fcst[cwa]['lon'] = all_lons\n",
        "      tmpframe=pd.concat([masterfcst,fcst[cwa]])\n",
        "      masterfcst=tmpframe\n",
        "masterfcst.replace(-9999.0,np.nan)\n",
        "\n",
        "if export_df_csv:\n",
        "  csv_name=\"obs_\"+element+\"_series_valid_\"+valid_end_datetime.strftime('%Y%m%d')+\"_\"+cwa+\".csv\"\n",
        "  masterfcst.to_csv(csv_name,na_rep=\"MISSING\")\n",
        "  print(\"  >  Created and saved:\" + csv_name)"
      ],
      "metadata": {
        "id": "fr_qYK46B6Xw",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plotting Option One - Averaged Forecasts Over Domain\n",
        "masterfcst=masterfcst.replace(-9999.0,np.NaN)\n",
        "units={\"maxt\":\"deg F\", \"mint\":\"deg F\", \"qpf\":\"in\", \"maxwind\":\"kts\", \"snow\":\"in\"}\n",
        "fig=plt.figure(constrained_layout=False,figsize=(22,16),dpi=80)\n",
        "grid=fig.add_gridspec(1,1)\n",
        "ax=fig.add_subplot(1,1,1)\n",
        "mosframe=masterfcst.groupby(['forecast_hour'])[['forecast_hour','OB','NBM_D','NBM_P10','NBM_P50','NBM_P90']].mean()\n",
        "plt.axhline(mosframe.loc[mosframe.index[0],'OB'],color='black',linewidth=4,linestyle='dashed',label=\"OB\")\n",
        "for var in ['NBM_P10','NBM_P50','NBM_P90']:\n",
        "  mosframe.plot(ax=ax,kind=\"line\",x=\"forecast_hour\",y=var,grid=True,marker='o',markersize=3,fontsize=18,linewidth=2)\n",
        "if element != \"maxwind\":\n",
        "  mosframe.plot(ax=ax,kind=\"line\",x=\"forecast_hour\",y='NBM_D',grid=True,marker='o',markersize=3,fontsize=18,linewidth=2)\n",
        "ax.legend(loc='lower center',bbox_to_anchor=[0.5,1.00],fancybox=True,shadow=True,ncol=6,prop={'size': 18})\n",
        "ax.set_xlim(ax.get_xlim()[::-1])\n",
        "plt.xlabel(\"Forecast Hour\",fontsize=26)\n",
        "plt.ylabel(\"Forecast Value (\" + units[element] + \")\",fontsize=26)\n",
        "title_string=\"Time Series of Mean Forecast and Obs of \" + element + \" valid at \" + valid_date_start.strftime('%m/%d/%y')  + \" over WFO \" + cwa\n",
        "fig.text(0.5,0.94,title_string,horizontalalignment='center',verticalalignment='bottom',weight='bold',fontsize=26)\n",
        "fig.savefig(cwa+\"_\"+element+\"_mean_valid_\" + valid_date_start.strftime('%m%d%y')+\".png\")"
      ],
      "metadata": {
        "id": "LpOmUnqmXlai",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plotting Option Two - Obs in Percentile Space Over Time\n",
        "#generate time series plot\n",
        "fig=plt.figure(constrained_layout=False,figsize=(22,16),dpi=80)\n",
        "grid=fig.add_gridspec(1,1)\n",
        "ax=fig.add_subplot(1,1,1)\n",
        "#combine obs and forecast dataframes into one\n",
        "for cwa in cwa_selection:\n",
        "  allvals=pd.merge(masterfcst,obs[cwa],on='stid')\n",
        "allvals.to_csv(cwa+'_'+element+'ob_percentiles_valid_'+valid_date_start.strftime('%m%d%y')+'.csv')\n",
        "fcvals = pd.unique(allvals['forecast_hour'])\n",
        "for stn in pd.unique(allvals['stid']):\n",
        "  stnframe=allvals.loc[allvals['stid'] == stn]\n",
        "  stnframe.plot(ax=ax,x=\"forecast_hour\",y=\"ob_perc\",label=stn,xlabel=\"Forecast Hour\",ylabel=\"Percentile\",ylim=(-15,115),grid=True,marker='o',markersize=3,linewidth=2,fontsize=18)\n",
        "ax.legend(loc='lower center',bbox_to_anchor=[0.5,1.00],fancybox=True,shadow=True,ncol=11,prop={'size': 18})\n",
        "ax.set_xlim(ax.get_xlim()[::-1])\n",
        "title_string=\"Time Series of obs from \" + fcst_start_date.strftime('%m/%d/%y')+ \"-\" + fcst_end_date.strftime('%m/%d/%y') + \" in NBM Percentile Space\"\n",
        "sub_string=element + \" Forecasts valid \" + valid_date_start.strftime('%m/%d/%y')\n",
        "#ax.set_title(title_string+'\\n'+sub_string)\n",
        "plt.axhline(0,color='black',linewidth=3,linestyle='dashed')\n",
        "plt.axhline(100,color='black',linewidth=3,linestyle='dashed')\n",
        "plt.xlabel(\"Forecast Hour\",fontsize=26)\n",
        "plt.ylabel(\"Blend Percentile\",fontsize=26)\n",
        "fig.text(0.5,1.02,title_string,horizontalalignment='center',verticalalignment='bottom',weight='bold',fontsize=26)\n",
        "fig.text(0.5,1.02,sub_string,horizontalalignment='center',verticalalignment='top',fontsize=20)\n",
        "fig.savefig(cwa+'_'+element+'_ob_pctplot_bystn_valid_'+valid_date_start.strftime('%m%d%y')+\".png\") #,bbox_inches='tight')"
      ],
      "metadata": {
        "id": "jVzxVVLbe7Wi",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plotting Option Three - NBM-D in Percentile Space\n",
        "#generate time series plot\n",
        "if element == \"maxwind\":\n",
        "  raise Exception(\"FATAL ERROR: No deterministic data available for maxwind\")\n",
        "fig=plt.figure(constrained_layout=False,figsize=(22,16),dpi=80)\n",
        "grid=fig.add_gridspec(1,1)\n",
        "ax=fig.add_subplot(1,1,1)\n",
        "#combine obs and forecast dataframes into one\n",
        "for cwa in cwa_selection:\n",
        "  allvals=pd.merge(masterfcst,obs[cwa],on='stid')\n",
        "allvals.to_csv(cwa+'_'+element+'nbmd_percentiles_valid_'+valid_date_start.strftime('%m%d%y')+'.csv')\n",
        "fcvals = pd.unique(allvals['forecast_hour'])\n",
        "for stn in pd.unique(allvals['stid']):\n",
        "  stnframe=allvals.loc[allvals['stid'] == stn]\n",
        "  stnframe.plot(ax=ax,x=\"forecast_hour\",y=\"NBMd_perc\",label=stn,xlabel=\"Forecast Hour\",ylabel=\"Percentile\",ylim=(-15,115),grid=True,marker='o',markersize=3,linewidth=2,fontsize=18)\n",
        "ax.legend(loc='lower center',bbox_to_anchor=[0.5,1.00],fancybox=True,shadow=True,ncol=11,prop={'size': 18})\n",
        "ax.set_xlim(ax.get_xlim()[::-1])\n",
        "title_string=\"Time Series of Deterministic NBM from \" + fcst_start_date.strftime('%m/%d/%y')+ \"-\" + fcst_end_date.strftime('%m/%d/%y') + \" in NBM Percentile Space\"\n",
        "sub_string=element + \" Forecasts valid \" + valid_date_start.strftime('%m/%d/%y')\n",
        "#ax.set_title(title_string+'\\n'+sub_string)\n",
        "plt.axhline(0,color='black',linewidth=3,linestyle='dashed')\n",
        "plt.axhline(100,color='black',linewidth=3,linestyle='dashed')\n",
        "plt.xlabel(\"Forecast Hour\",fontsize=26)\n",
        "plt.ylabel(\"Blend Percentile\",fontsize=26)\n",
        "fig.text(0.5,1.02,title_string,horizontalalignment='center',verticalalignment='bottom',weight='bold',fontsize=26)\n",
        "fig.text(0.5,1.02,sub_string,horizontalalignment='center',verticalalignment='top',fontsize=20)\n",
        "fig.savefig(cwa+'_'+element+'_nbmd_pctplot_bystn_valid_'+valid_date_start.strftime('%m%d%y')+\".png\") #,bbox_inches='tight')"
      ],
      "metadata": {
        "id": "tG_MvPj1a7d6",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plotting Option Four - Individual Obs or Deterministic in Percentile Space\n",
        "compare_to = \"obs\" #@param [\"obs\", \"deterministic\"]\n",
        "cwa_outline = True #@param {type:\"boolean\"}\n",
        "county_outline = True #@param {type:\"boolean\"}\n",
        "\n",
        "if compare_to == \"obs\":\n",
        "  compare_element = \"Obs\"\n",
        "  compare_var = \"ob_perc\"\n",
        "else:\n",
        "  compare_element = \"Detr\"\n",
        "  compare_var = \"NBMd_perc\"\n",
        "\n",
        "title_dict = {\"maxt\":[\"Max T\",\"PMaxT\"],\"mint\":[\"Min T\",\"PMinT\"], \"qpf\":[\"QPF\",\"PQPF\"], \"maxwind\":[\"Max Wind\",\"Prob Max Wind\"], \"snow\":[\"Snow Acc\", \"Prob Snow Acc\"]}\n",
        "background_color = '#272727'\n",
        "text_color = 'white'\n",
        "map_land_color = '#414143'\n",
        "map_water_color = '#272727'\n",
        "map_border_color = 'white'\n",
        "matplotlib.rc('axes',facecolor=background_color, edgecolor=text_color)\n",
        "\n",
        "if element == \"qpf\":\n",
        "  cmap = get_cmap('BrBG')\n",
        "  cmap.set_under(color='black')\n",
        "  cmap.set_over(color='yellow')\n",
        "elif element == \"snow\":\n",
        "  cmap = get_cmap('cool')\n",
        "  cmap.set_under(color='black')\n",
        "  cmap.set_over(color='yellow')\n",
        "else:\n",
        "  #cmap = 'Spectral'\n",
        "  cmap = get_cmap('bwr')\n",
        "  cmap.set_under(color='black')\n",
        "  cmap.set_over(color='yellow')\n",
        "\n",
        "if use_stageiv and element==\"qpf\":\n",
        "  points_str = f'Stage IV @ {network_selection}'\n",
        "else:\n",
        "  points_str = network_selection\n",
        "\n",
        "if (element == \"qpf\" or element == \"snow\"):\n",
        "  valid_datetime = valid_date\n",
        "  fig_valid_date = nbm_qmd_valid_end_datetime.strftime('%Y%m%d_%HZ')\n",
        "  valid_title = nbm_qmd_valid_end_datetime.strftime('%HZ %a %m-%d-%Y')\n",
        "else:\n",
        "  valid_datetime = datetime.strptime(valid_date,'%Y-%m-%d')\n",
        "  fig_valid_date = valid_datetime.strftime('%Y%m%d')\n",
        "  valid_title = valid_datetime.strftime('%a %m-%d-%Y')\n",
        "\n",
        "if (element == \"snow\"):\n",
        "  nbm_init_title = core_init.strftime('%HZ %m-%d-%Y')\n",
        "else:\n",
        "  nbm_init_title = nbm_init.strftime('%HZ %m-%d-%Y')\n",
        "\n",
        "def flip(items, ncol):\n",
        "    return itertools.chain(*[items[i::ncol] for i in range(ncol)])\n",
        "\n",
        "for fhr in pd.unique(masterfcst['forecast_hour']):\n",
        "\n",
        "  tmpfcst=masterfcst.loc[masterfcst['forecast_hour'] == fhr]\n",
        "\n",
        "  if region_selection == \"CONUS\":\n",
        "    dataframeid = \"CONUS\"\n",
        "    #set up multipanel plot\n",
        "    west =-125.650\n",
        "    south = 23.377\n",
        "    east = -66.008\n",
        "    north = 50.924\n",
        "    width_ratios = [7,3,3,3]\n",
        "    lloc = \"lower right\"\n",
        "    fig = plt.figure(constrained_layout=True, figsize=(16,9), facecolor=background_color, frameon=True, dpi=150)\n",
        "    grid = fig.add_gridspec(4,4, width_ratios=width_ratios, hspace=0.2, wspace=0.2, left=0.1, right=0.9)\n",
        "    fig.text(0.30, 0.885,f'{region_selection} {title_dict[element][0]} {compare_element} in NBM {title_dict[element][1]} Percentile Space',horizontalalignment='center',weight='bold',fontsize=25,color=text_color)\n",
        "    fig.text(0.30, 0.855,f'Valid: {valid_title}  |  NBM Init: {nbm_init_title}  |  Points: {points_str}',horizontalalignment='center',fontsize=16,color=text_color)\n",
        "    ax1 = fig.add_subplot(grid[:,:-2], projection=ccrs.Mercator(globe=None))\n",
        "    ax2 = fig.add_subplot(grid[0,2])\n",
        "    ax3 = fig.add_subplot(grid[0,3])\n",
        "    ax4 = fig.add_subplot(grid[1,2])\n",
        "    ax5 = fig.add_subplot(grid[1,3])\n",
        "    ax6 = fig.add_subplot(grid[2:,2:])\n",
        "\n",
        "    conus_df = pd.concat([pltfcst[\"WR\"], pltfcst[\"CR\"], pltfcst[\"ER\"],pltfcst[\"SR\"]])\n",
        "    lats = conus_df[\"lat\"].values\n",
        "    lons = conus_df[\"lon\"].values\n",
        "    point_data = conus_df[compare_var].values\n",
        "    mean = conus_df[compare_var].mean()\n",
        "    median = conus_df[compare_var].median()\n",
        "    mode = conus_df[compare_var].mode().values[0]\n",
        "\n",
        "    proj = ccrs.PlateCarree()\n",
        "\n",
        "    ax1.set_anchor('S')\n",
        "    ax1.set_extent([west, east, south, north], crs=proj)\n",
        "    #ax1.add_feature(cfeature.LAND, edgecolor='none', facecolor='#414143', zorder=-1)\n",
        "    ax1.add_feature(cfeature.OCEAN, edgecolor='none', facecolor=map_water_color, zorder=-2)\n",
        "    ax1.add_feature(cfeature.NaturalEarthFeature('physical', 'land', '50m', edgecolor='none', facecolor=map_land_color, zorder=-1))\n",
        "    #ax1.add_feature(cfeature.LAKES, edgecolor='none', facecolor='#272727', zorder=0)\n",
        "    ax1.add_feature(cfeature.NaturalEarthFeature('physical', 'lakes', '10m', edgecolor='none', facecolor=map_water_color, zorder=0))\n",
        "    ax1.add_feature(cfeature.BORDERS, edgecolor=map_border_color, facecolor='none', linewidth=2, zorder=1)\n",
        "    #ax1.add_feature(cfeature.NaturalEarthFeature('cultural', 'countries', '50m', edgecolor=map_border_color, facecolor='none', linewidth=2, zorder=2))\n",
        "    ax1.add_feature(cfeature.NaturalEarthFeature('cultural', 'admin_1_states_provinces_lines', '50m', edgecolor=map_border_color, facecolor='none', linewidth=1, zorder=2))\n",
        "    #cx.add_basemap(ax1, source='https://server.arcgisonline.com/ArcGIS/rest/services/Canvas/World_Dark_Gray_Base/MapServer/tile/{z}/{y}/{x}', attribution=False)\n",
        "    scatter = ax1.scatter(lons, lats, c= point_data, cmap=cmap, s=45, transform=proj, vmin=0.0, vmax=100.0)\n",
        "    #handles, labels = scatter.legend_elements(num=10)\n",
        "    #legend1 = ax1.legend(flip(handles, 6), flip(labels, 6), ncol=6,loc=lloc, title=f'{compare_element} in NBM Percentile Space', fancybox=True)\n",
        "    numcols=abs(np.amax(point_data) - np.amin(point_data))//10\n",
        "    legend1 = ax1.legend(*scatter.legend_elements(num=numcols), loc=lloc, title=f'{compare_element} \\n Rank', fancybox=True)\n",
        "    plt.setp(legend1.get_title(), multialignment='center', color=text_color)\n",
        "    for text in legend1.get_texts():\n",
        "      text.set_color(text_color)\n",
        "    ax1.add_artist(legend1)\n",
        "    ax1.add_feature(cfeature.NaturalEarthFeature(\n",
        "      'cultural', 'admin_1_states_provinces_lines', '110m',\n",
        "      edgecolor='gray', facecolor='none'))\n",
        "    if cwa_outline:\n",
        "      try:\n",
        "        if os.path.exists(\"shp/w_22mr22.shp\"):\n",
        "          pass\n",
        "        else:\n",
        "          cwa_url = \"https://www.weather.gov/source/gis/Shapefiles/WSOM/w_22mr22.zip\"\n",
        "          os.mkdir(\"shp\")\n",
        "          urlretrieve(cwa_url, \"shp/nws_cwa_outlines.zip\")\n",
        "          #!unzip shp/nws_cwa_outlines.zip -d shp\n",
        "          with zipfile.ZipFile(\"shp/nws_cwa_outlines.zip\", 'r') as zip_ref:\n",
        "            zip_ref.extractall(\"shp\")\n",
        "        cwa_feature = ShapelyFeature(Reader(\"shp/w_22mr22.shp\").geometries(),ccrs.PlateCarree(), edgecolor='grey', facecolor='none', linewidth=0.5, linestyle=':', zorder=3)\n",
        "        ax1.add_feature(cwa_feature)\n",
        "      except:\n",
        "        print(\"   > Aw shucks, no CWA boundaries for you. Sorry bout that.\")\n",
        "    if county_outline:\n",
        "      try:\n",
        "        if os.path.exists(\"shp/c_08mr23.zip\"):\n",
        "          pass\n",
        "        else:\n",
        "          county_url = \"https://www.weather.gov/source/gis/Shapefiles/County/c_08mr23.zip\"\n",
        "          os.mkdir(\"shp\")\n",
        "          urlretrieve(county_url, \"shp/counties.zip\")\n",
        "          with zipfile.ZipFile(\"shp/counties.zip\",'r') as zip_ref:\n",
        "            zip.ref.extractall(\"shp\")\n",
        "        cty_feature = ShapelyFeature(Reader(\"shp/c_08mr23.shp\").geometries().ccrs.PlateCarree(), edgecolor='white',facecolor='none',linewidth=1.0,linestyle=\"--\",zorder=3)\n",
        "        ax1.add_feature(cty_feature)\n",
        "      except:\n",
        "        print(\"   > Cannot plot county boundaries.\")\n",
        "\n",
        "    mean_wr = tmpfcst[\"WR\"][compare_var].mean()\n",
        "    median_wr = tmpfcst[\"WR\"][compare_var].median()\n",
        "    #mode_wr = masterfcst[\"WR\"][compare_var].mode().values[0]\n",
        "    ax2.set_anchor('N')\n",
        "    sns.histplot(data=tmpfcst[\"WR\"], x=compare_var, ax=ax2, kde=True, bins=range(0,110,10),color='steelblue',edgecolor='lightgrey')\n",
        "    ax2.set_xlabel(\"Western Region\", color=text_color, fontsize=12)\n",
        "    ax2.axvline(mean_wr, color='salmon', linestyle='--', label=\"Mean\")\n",
        "    ax2.axvline(median_wr, color='mediumaquamarine', linestyle='-', label=\"Median\")\n",
        "    #ax2.axvline(mode_wr, color='lightskyblue', linestyle='-', label=\"Mode\")\n",
        "    ax2.grid(False)\n",
        "    for tick in ax2.get_xticklabels():\n",
        "      tick.set_color(text_color)\n",
        "    for tick in ax2.get_yticklabels():\n",
        "      tick.set_color(text_color)\n",
        "    ax2.tick_params(axis='y',labelsize=8, color=text_color)\n",
        "    legend2 = ax2.legend()\n",
        "    for text in legend2.get_texts():\n",
        "      text.set_color(text_color)\n",
        "    ax2.set(ylabel=None)\n",
        "\n",
        "    mean_cr = tmpfcst[\"CR\"][compare_var].mean()\n",
        "    median_cr = tmpfcst[\"CR\"][compare_var].median()\n",
        "    #mode_cr = tmpfcst[\"CR\"][compare_var].mode().values[0]\n",
        "    ax3.set_anchor('N')\n",
        "    sns.histplot(data=tmpfcst[\"CR\"], x=compare_var, ax=ax3, kde=True, bins=range(0,110,10),color='steelblue',edgecolor='lightgrey')\n",
        "    ax3.set_xlabel(\"Central Region\", color=text_color, fontsize=12)\n",
        "    ax3.axvline(mean_cr, color='salmon', linestyle='--', label=\"Mean\")\n",
        "    ax3.axvline(median_cr, color='mediumaquamarine', linestyle='-', label=\"Median\")\n",
        "    #ax3.axvline(mode_cr, color='lightskyblue', linestyle='-', label=\"Mode\")\n",
        "    ax3.grid(False)\n",
        "    for tick in ax3.get_xticklabels():\n",
        "      tick.set_color(text_color)\n",
        "    for tick in ax3.get_yticklabels():\n",
        "      tick.set_color(text_color)\n",
        "    ax3.tick_params(axis='y',labelsize=8, color=text_color)\n",
        "    legend3 = ax3.legend()\n",
        "    for text in legend3.get_texts():\n",
        "      text.set_color(text_color)\n",
        "    ax3.set(ylabel=None)\n",
        "\n",
        "\n",
        "    mean_er = tmpfcst[\"ER\"][compare_var].mean()\n",
        "    median_er = tmpfcst[\"ER\"][compare_var].median()\n",
        "    #mode_er = tmpfcst[\"ER\"][compare_var].mode().values[0]\n",
        "    ax4.set_anchor('N')\n",
        "    sns.histplot(data=tmpfcst[\"ER\"], x=compare_var, ax=ax4, kde=True, bins=range(0,110,10),color='steelblue',edgecolor='lightgrey')\n",
        "    ax4.set_xlabel(\"Eastern Region\", color=text_color, fontsize=12)\n",
        "    ax4.axvline(mean_er, color='salmon', linestyle='--', label=\"Mean\")\n",
        "    ax4.axvline(median_er, color='mediumaquamarine', linestyle='-', label=\"Median\")\n",
        "    #ax4.axvline(mode_er, color='lightskyblue', linestyle='-', label=\"Mode\")\n",
        "    ax4.grid(False)\n",
        "    for tick in ax4.get_xticklabels():\n",
        "      tick.set_color(text_color)\n",
        "    for tick in ax4.get_yticklabels():\n",
        "      tick.set_color(text_color)\n",
        "    ax4.tick_params(axis='y',labelsize=8, color=text_color)\n",
        "    legend4 = ax4.legend()\n",
        "    for text in legend4.get_texts():\n",
        "      text.set_color(text_color)\n",
        "    ax4.set(ylabel=None)\n",
        "\n",
        "\n",
        "    mean_sr = tmpfcst[\"SR\"][compare_var].mean()\n",
        "    median_sr = tmpfcst[\"SR\"][compare_var].median()\n",
        "    #mode_sr = masterfcst[\"SR\"][compare_var].mode().values[0]\n",
        "    ax5.set_anchor('N')\n",
        "    sns.histplot(data=tmpfcst[\"SR\"], x=compare_var, ax=ax5, kde=True, bins=range(0,110,10),color='steelblue',edgecolor='lightgrey')\n",
        "    ax5.set_xlabel(\"Southern Region\", color=text_color, fontsize=12)\n",
        "    ax5.axvline(mean_sr, color='salmon', linestyle='--', label=\"Mean\")\n",
        "    ax5.axvline(median_sr, color='mediumaquamarine', linestyle='-', label=\"Median\")\n",
        "    #ax5.axvline(mode_sr, color='lightskyblue', linestyle='-', label=\"Mode\")\n",
        "    ax5.grid(False)\n",
        "    for tick in ax5.get_xticklabels():\n",
        "      tick.set_color(text_color)\n",
        "    for tick in ax5.get_yticklabels():\n",
        "      tick.set_color(text_color)\n",
        "    ax5.tick_params(axis='y',labelsize=8, color=text_color)\n",
        "    legend5 = ax5.legend()\n",
        "    for text in legend5.get_texts():\n",
        "      text.set_color(text_color)\n",
        "    ax5.set(ylabel=None)\n",
        "\n",
        "    ax6.set_anchor('NC')\n",
        "    sns.histplot(data=point_data, ax=ax6, kde=True, bins=range(0,110,10),color='steelblue',edgecolor='lightgrey')\n",
        "    ax6.set_xlabel(f'{compare_element} in NBM {title_dict[element][1]} Percentile Bins', color=text_color, fontsize=12)\n",
        "    ax6.axvline(mean, color='salmon', linestyle='--', label=\"Mean\")\n",
        "    ax6.axvline(median, color='mediumaquamarine', linestyle='-', label=\"Median\")\n",
        "    #ax6.axvline(mode, color='lightskyblue', linestyle='-', label=\"Mode\")\n",
        "    ax6.grid(False)\n",
        "    for tick in ax6.get_xticklabels():\n",
        "      tick.set_color(text_color)\n",
        "    for tick in ax6.get_yticklabels():\n",
        "      tick.set_color(text_color)\n",
        "    ax6.tick_params(axis='y',labelsize=8, color=text_color)\n",
        "    legend6 = ax6.legend()\n",
        "    for text in legend6.get_texts():\n",
        "      text.set_color(text_color)\n",
        "    ax6.set(ylabel=None)\n",
        "\n",
        "    #figname=region_selection+\"_\"+element+\"_\"+valid_date+\".png\"\n",
        "    #plt.savefig(figname, facecolor=fig.get_facecolor(), bbox_inches=None, pad_inches=0.2, dpi='figure')\n",
        "\n",
        "  else:\n",
        "    #set up two panel plot\n",
        "    if (region_selection == \"WR\"):\n",
        "        west = -126.917\n",
        "        south = 30.586\n",
        "        east = -102.740\n",
        "        north = 49.755\n",
        "        width, height = (16,9)\n",
        "        width_ratios = [9,8]\n",
        "        lloc = \"lower right\"\n",
        "    if (region_selection == \"CR\"):\n",
        "        west = -111.534\n",
        "        south = 33.295\n",
        "        east = -81.723\n",
        "        north = 49.755\n",
        "        width, height = (16,7)\n",
        "        width_ratios = [9,7]\n",
        "        lloc = \"lower center\"\n",
        "    if (region_selection == \"ER\"):\n",
        "        west = -86.129\n",
        "        south = 31.223\n",
        "        east = -66.465\n",
        "        north = 47.676\n",
        "        width, height = (16,7.25)\n",
        "        width_ratios = [6.9,9.5]\n",
        "        lloc = \"lower right\"\n",
        "    if (region_selection == \"SR\"):\n",
        "        west = -109.758\n",
        "        south = 23.313\n",
        "        east = -79.247\n",
        "        north = 36.899\n",
        "        width, height = (16,5.6)\n",
        "        width_ratios = [10,6]\n",
        "        lloc = \"lower center\"\n",
        "    if (region_selection == \"CWA\"):\n",
        "        west = np.min(tmpfcst[\"lon\"]) - 0.5\n",
        "        south = np.min(tmpfcst[\"lat\"]) - 0.5\n",
        "        east = np.max(tmpfcst[\"lon\"]) + 1.0\n",
        "        north = np.max(tmpfcst[\"lat\"]) + 0.5\n",
        "        width, height = (16,9)\n",
        "        ratioxy = 16./9.\n",
        "        width_ratios = [ratioxy, 1]\n",
        "        lloc = \"center right\"\n",
        "\n",
        "    #width, height = (16,9)\n",
        "    fig = plt.figure(constrained_layout=True, figsize=(width,height), facecolor=background_color, frameon=True, dpi=150)\n",
        "    if (region_selection == \"CWA\"):\n",
        "      dataframeid = cwa_id\n",
        "    else:\n",
        "      dataframeid = region_selection\n",
        "    #ratioxy = 16./9.\n",
        "    #width_ratios = [ratioxy, 1]\n",
        "    grid = fig.add_gridspec(1,2, hspace=0.2, width_ratios=width_ratios, height_ratios = [1], wspace=0.2)\n",
        "    ax1 = fig.add_subplot(grid[0,0], projection=ccrs.Mercator())\n",
        "    #ax1 = fig.add_subplot(grid[0,0], projection=ccrs.LambertConformal(central_latitude=25, central_longitude=265, standard_parallels=(25,25)))\n",
        "    ax2 = fig.add_subplot(grid[0,1], )\n",
        "    fig.text(0.5, 1.05,f'{dataframeid} {title_dict[element][0]} {compare_element} in NBM v4.1 {title_dict[element][1]} Percentile Space',\\\n",
        "            horizontalalignment='center', verticalalignment='bottom', weight='bold',fontsize=20,color=text_color)\n",
        "    fig.text(0.5, 1.05,f'Valid: {valid_title} | NBM Init: {nbm_init_title} | Points: {points_str}', \\\n",
        "            horizontalalignment='center',verticalalignment='top', fontsize=16,color=text_color)\n",
        "\n",
        "    lats = tmpfcst[\"lat\"].values\n",
        "    lons = tmpfcst[\"lon\"].values\n",
        "    point_data = tmpfcst[compare_var].values\n",
        "    mean = tmpfcst[compare_var].mean()\n",
        "    median = tmpfcst[compare_var].median()\n",
        "    #mode = tmpfcst.mode().values[0]\n",
        "    proj = ccrs.PlateCarree()\n",
        "    numcols=(abs(np.nanmax(point_data) - np.nanmin(point_data))//10) + 1\n",
        "\n",
        "    ax1.set_anchor('N')\n",
        "    ax1.set_facecolor(background_color)\n",
        "    ax1.set_extent([west, east, south, north], crs=proj)\n",
        "    #ax1.add_feature(cfeature.LAND, edgecolor='none', facecolor='#414143', zorder=-1)\n",
        "    ax1.add_feature(cfeature.OCEAN, edgecolor='none', facecolor=map_water_color, zorder=-2)\n",
        "    ax1.add_feature(cfeature.NaturalEarthFeature('physical', 'land', '50m', edgecolor='none', facecolor=map_land_color, zorder=-1))\n",
        "    #ax1.add_feature(cfeature.LAKES, edgecolor='none', facecolor='#272727', zorder=0)\n",
        "    ax1.add_feature(cfeature.NaturalEarthFeature('physical', 'lakes', '10m', edgecolor='none', facecolor=map_water_color, zorder=0))\n",
        "    ax1.add_feature(cfeature.BORDERS, edgecolor=map_border_color, facecolor='none', linewidth=2, zorder=2)\n",
        "    #ax1.add_feature(cfeature.NaturalEarthFeature('cultural', 'countries', '50m', edgecolor=map_border_color, facecolor='none', linewidth=2, zorder=2))\n",
        "    ax1.add_feature(cfeature.NaturalEarthFeature('cultural', 'admin_1_states_provinces_lines', '50m', edgecolor=map_border_color, facecolor='none', linewidth=1, zorder=5))\n",
        "    #ax1.add_feature(cfeature.NaturalEarthFeature('cultural', 'admin_1_states_provinces_lines', '50m', edgecolor=map_border, facecolor='none', linewidth=1, zorder=5))\n",
        "    #cx.add_basemap(ax1, source='https://server.arcgisonline.com/ArcGIS/rest/services/Canvas/World_Dark_Gray_Base/MapServer/tile/{z}/{y}/{x}', attribution=False)\n",
        "    scatter = ax1.scatter(lons, lats, c= point_data, cmap=cmap, s=45, transform=proj, zorder=2, vmin=0.0, vmax=100.0)\n",
        "\n",
        "    if region_selection in (\"CR\",\"SR\"):\n",
        "      handles, labels = scatter.legend_elements(num=numcols)\n",
        "      legend1 = ax1.legend(flip(handles, 6), flip(labels, 6), ncol=6,loc=lloc, title=f'{compare_element} in NBM Percentile Space', fancybox=True)\n",
        "    else:\n",
        "      legend1 = ax1.legend(*scatter.legend_elements(num=numcols),\n",
        "                        loc=lloc, title=f'{compare_element} \\n Rank', fancybox=True)\n",
        "    plt.setp(legend1.get_title(), multialignment='center', color=text_color)\n",
        "    for text in legend1.get_texts():\n",
        "      text.set_color(text_color)\n",
        "    ax1.add_artist(legend1)\n",
        "    #ax1.set(aspect='equal', adjustable='box')\n",
        "    if cwa_outline:\n",
        "      try:\n",
        "        if os.path.exists(\"shp/w_22mr22.shp\"):\n",
        "          pass\n",
        "        else:\n",
        "          cwa_url = \"https://www.weather.gov/source/gis/Shapefiles/WSOM/w_22mr22.zip\"\n",
        "          os.mkdir(\"shp\")\n",
        "          urlretrieve(cwa_url, \"shp/nws_cwa_outlines.zip\")\n",
        "          #!unzip shp/nws_cwa_outlines.zip -d shp\n",
        "        with zipfile.ZipFile(\"shp/nws_cwa_outlines.zip\", 'r') as zip_ref:\n",
        "          zip_ref.extractall(\"shp\")\n",
        "          cwa_feature = ShapelyFeature(Reader(\"shp/w_22mr22.shp\").geometries(),ccrs.PlateCarree(), edgecolor='black', facecolor='none', linewidth=1, linestyle='-', zorder=4)\n",
        "          ax1.add_feature(cwa_feature)\n",
        "      except:\n",
        "        print(\"Aw shucks, no CWA boundaries for you. Sorry bout that.\")\n",
        "    if county_outline:\n",
        "      try:\n",
        "        if os.path.exists(\"shp/c_08mr23.zip\"):\n",
        "          pass\n",
        "        else:\n",
        "          county_url = \"https://www.weather.gov/source/gis/Shapefiles/County/c_08mr23.zip\"\n",
        "          if os.path.exists(\"shp\"):\n",
        "            pass\n",
        "          else:\n",
        "            os.mkdir(\"shp\")\n",
        "            urlretrieve(county_url, \"shp/counties.zip\")\n",
        "            print(\"   >> Downloaded county zip file\")\n",
        "        with zipfile.ZipFile(\"shp/counties.zip\",'r') as cty_ref:\n",
        "          cty_ref.extractall(\"shp\")\n",
        "          print(\"   >> Extracted county shape files\")\n",
        "          cty_feature = ShapelyFeature(Reader(\"shp/c_08mr23.shp\").geometries(),ccrs.PlateCarree(), edgecolor='grey', facecolor='none', linewidth=0.5, linestyle=':', zorder=3)\n",
        "          ax1.add_feature(cty_feature)\n",
        "      except:\n",
        "        print(\"   >> Cannot plot county boundaries.\")\n",
        "\n",
        "    #if region_selection == \"SR\":\n",
        "    #ax2.set(aspect=1)\n",
        "    ax2.set_anchor('C')\n",
        "    sns.histplot(data=tmpfcst, x=compare_var, ax=ax2, kde=True, bins=range(-10,115,10),color='steelblue',edgecolor='lightgrey')\n",
        "    ax2.set_xlabel(f'{compare_element} in NBM {title_dict[element][1]} Percentile Bins', color=text_color, fontsize=12)\n",
        "    ax2.axvline(mean, color='salmon', linestyle='--', label=\"Mean\")\n",
        "    ax2.axvline(median, color='mediumaquamarine', linestyle='-', label=\"Median\")\n",
        "\n",
        "    ax2.grid(False)\n",
        "    for tick in ax2.get_xticklabels():\n",
        "      tick.set_color(text_color)\n",
        "    for tick in ax2.get_yticklabels():\n",
        "      tick.set_color(text_color)\n",
        "    ax2.tick_params(axis='y',labelsize=8, color=text_color)\n",
        "    legend2 = ax2.legend()\n",
        "    for text in legend2.get_texts():\n",
        "      text.set_color(text_color)\n",
        "    ax2.set(ylabel=None)\n",
        "\n",
        "\n",
        "  figname=dataframeid+\"_\"+compare_element+\"_\"+element+\"_\"+valid_end_datetime.strftime('%Y%m%d')+\"_fh\"+str(int(fhr))+\".png\"\n",
        "  plt.savefig(figname, facecolor=fig.get_facecolor(), bbox_inches='tight', pad_inches=0.2, dpi='figure')\n",
        "  print(f'   > Done! Saved plot as {figname}')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "N58bnHEIreQn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}