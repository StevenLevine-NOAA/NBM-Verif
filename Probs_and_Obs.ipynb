{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8kmSxsLaGdbeyiYkWlCNf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StevenLevine-NOAA/NBM-Verif/blob/notebooks/Probs_and_Obs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5p8gLFDP7eUY",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Initialize Notebook Part 1\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initialize Notebook Part 2\n",
        "#!mamba install -q -c conda-forge cartopy contextily pyproj pyepsg pygrib netCDF4\n",
        "!pip install cartopy contextily pyproj pyepsg pygrib netCDF4\n",
        "import numpy as np\n",
        "from scipy.interpolate import CubicSpline as cs, UnivariateSpline as us\n",
        "import pandas as pd\n",
        "from urllib.request import urlretrieve, urlopen\n",
        "import requests\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "from netCDF4 import Dataset\n",
        "import pygrib\n",
        "import pyproj\n",
        "from pyproj import Proj, transform\n",
        "import os, re, traceback\n",
        "\n",
        "import matplotlib\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "#from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.axes as maxes\n",
        "import matplotlib.patheffects as PathEffects\n",
        "from matplotlib.path import Path\n",
        "from matplotlib.textpath import TextToPath\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib.font_manager import FontProperties\n",
        "matplotlib.rcParams['font.sans-serif'] = 'Liberation Sans'\n",
        "matplotlib.rcParams['font.family'] = \"sans-serif\"\n",
        "from matplotlib.cm import get_cmap\n",
        "import seaborn as sns\n",
        "\n",
        "from cartopy import crs as ccrs, feature as cfeature\n",
        "from cartopy.io.shapereader import Reader\n",
        "from cartopy.feature import ShapelyFeature\n",
        "import contextily as cx\n",
        "import itertools\n",
        "\n",
        "import zipfile\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "2U6kbnHr7xYp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select Options and Set Defaults\n",
        "\n",
        "element = \"maxt\" #@param [\"maxt\", \"mint\",\"qpf\",\"maxwind\",\"snow\"]\n",
        "valid_date = \"2024-01-18\" #@param {type:\"date\"}\n",
        "qpf_valid_time = 12 #@param {type:\"slider\", min:0, max:18, step:6}\n",
        "use_stageiv = True #@param {type:\"boolean\"}\n",
        "use_nohrsc = True #@param {type:\"boolean\"}\n",
        "nbm_init_date = \"2024-01-13\" #@param {type:\"date\"}\n",
        "nbm_init_hour = 0 #@param {type:\"slider\", min:0, max:18, step:6}\n",
        "region_selection = \"CR\" #@param [\"WR\", \"SR\", \"CR\", \"ER\", \"AR\", \"CONUS\", \"CWA\"]\n",
        "cwa_id = \"\" #@param {type:\"string\"}\n",
        "network_selection = \"NWS\" #@param [\"NWS\", \"RAWS\", \"NWS+RAWS\", \"NWS+RAWS+HADS\", \"ALL\", \"CUSTOM\", \"LIST\"]\n",
        "cwa_outline = True #@param {type:\"boolean\"}\n",
        "county_outline = False #@param {type:\"boolean\"}\n",
        "export_csv = True #@param {type:\"boolean\"}\n",
        "#@markdown Light or dark theme plots?\n",
        "plot_style = \"dark\" #@param [\"light\", \"dark\"]\n",
        "\n",
        "if region_selection == \"CONUS\":\n",
        "  region_list = [\"WR\", \"CR\", \"SR\", \"ER\"]\n",
        "elif region_selection == \"CWA\":\n",
        "  region_list = [cwa_id]\n",
        "#elif region_selection == \"AR\":\n",
        "  #region_list=[\"AJK\",\"ARH\",\"AFC\"]\n",
        "else:\n",
        "  region_list = [region_selection]\n",
        "\n",
        "def cwa_list(input_region):\n",
        "  region_dict ={\"WR\":\"BYZ,BOI,LKN,EKA,FGZ,GGW,TFX,VEF,LOX,MFR,MTR,MSO,PDT,PSR,PIH,PQR,REV,STO,SLC,SGX,HNX,SEW,OTX,TWC\",\n",
        "              \"CR\":\"ABR,BIS,CYS,LOT,DVN,BOU,DMX,DTX,DDC,DLH,FGF,GLD,GJT,GRR,GRB,GID,IND,JKL,EAX,ARX,ILX,LMK,MQT,MKX,MPX,LBF,APX,IWX,OAX,PAH,PUB,UNR,RIW,FSD,SGF,LSX,TOP,ICT\",\n",
        "              \"ER\":\"ALY,LWX,BGM,BOX,BUF,BTV,CAR,CTP,RLX,CHS,ILN,CLE,CAE,GSP,MHX,OKX,PHI,PBZ,GYX,RAH,RNK,AKQ,ILM\",\n",
        "              \"SR\":\"ABQ,AMA,FFC,EWX,BMX,BRO,CRP,EPZ,FWD,HGX,HUN,JAN,JAX,KEY,MRX,LCH,LZK,LUB,MLB,MEG,MFL,MOB,MAF,OHX,LIX,OUN,SJT,SHV,TAE,TBW,TSA\",\n",
        "              \"AR\":\"AJK,ARH,AFC\"}\n",
        "  if (input_region in [\"WR\", \"CR\", \"SR\", \"ER\",\"AR\"]):\n",
        "    cwas_list = region_dict[input_region]\n",
        "  else:\n",
        "    cwas_list = input_region\n",
        "  return cwas_list\n",
        "\n",
        "nbm_init = datetime.strptime(nbm_init_date,'%Y-%m-%d') + timedelta(hours=int(nbm_init_hour))\n",
        "\n",
        "if element == \"maxt\":\n",
        "    nbm_core_valid_hour=\"00\"\n",
        "    nbm_qmd_valid_hour=\"06\"\n",
        "    valid_date_start = datetime.strptime(valid_date,'%Y-%m-%d')\n",
        "    valid_date_end = datetime.strptime(valid_date,'%Y-%m-%d') + timedelta(days=1)\n",
        "    obs_start_hour = \"1200\"\n",
        "    obs_end_hour = \"0600\"\n",
        "    ob_stat = \"maximum\"\n",
        "    valid_end_datetime = valid_date_end + timedelta(hours=(int(obs_end_hour)/100))\n",
        "    nbm_core_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_core_valid_hour))\n",
        "    nbm_qmd_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_qmd_valid_hour))\n",
        "    core_init = nbm_init + timedelta(hours = 7)\n",
        "    nbm_core_fhdelta = nbm_core_valid_end_datetime - core_init\n",
        "\n",
        "elif element == \"mint\":\n",
        "    nbm_core_valid_hour=\"12\"\n",
        "    nbm_qmd_valid_hour=\"18\"\n",
        "    valid_date_start = datetime.strptime(valid_date,'%Y-%m-%d')\n",
        "    valid_date_end = datetime.strptime(valid_date,'%Y-%m-%d')\n",
        "    obs_start_hour = \"0000\"\n",
        "    obs_end_hour = \"1800\"\n",
        "    ob_stat = \"minimum\"\n",
        "    valid_end_datetime = valid_date_end + timedelta(hours=(int(obs_end_hour)/100))\n",
        "    nbm_core_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_core_valid_hour))\n",
        "    nbm_qmd_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_qmd_valid_hour))\n",
        "    core_init = nbm_init + timedelta(hours = 7)\n",
        "    nbm_core_fhdelta = nbm_core_valid_end_datetime - core_init\n",
        "\n",
        "elif element == \"qpf\":\n",
        "    nbm_core_valid_hour = (str(qpf_valid_time)).zfill(2)\n",
        "    nbm_valid_hour = (str(qpf_valid_time)).zfill(2)\n",
        "    nbm_qmd_valid_hour=(str(qpf_valid_time)).zfill(2)\n",
        "    valid_date = datetime.strptime(valid_date,'%Y-%m-%d') + timedelta(hours=int(qpf_valid_time))\n",
        "    valid_date_start = valid_date - timedelta(hours=24)\n",
        "    valid_date_end = valid_date\n",
        "    obs_start_hour = (str(qpf_valid_time)).zfill(2)+\"00\"\n",
        "    obs_end_hour = (str(qpf_valid_time)).zfill(2)+\"00\"\n",
        "    ob_stat = \"total\"\n",
        "    valid_end_datetime = valid_date_end\n",
        "    core_init = nbm_init\n",
        "    nbm_core_valid_end_datetime = valid_date_end\n",
        "    nbm_qmd_valid_end_datetime = valid_date_end\n",
        "    nbm_core_fhdelta = nbm_core_valid_end_datetime - nbm_init\n",
        "\n",
        "elif element == \"maxwind\":\n",
        "    #nbm_core_valid_hour=\"06\"\n",
        "    #nbm_valid_hour=\"06\"\n",
        "    nbm_qmd_valid_hour=\"06\"\n",
        "    obs_start_hour=\"0600\"\n",
        "    obs_end_hour=\"0600\"\n",
        "    ob_stat=\"maximum\"\n",
        "    valid_date_start = datetime.strptime(valid_date,'%Y-%m-%d')\n",
        "    valid_date_end = datetime.strptime(valid_date,'%Y-%m-%d') + timedelta(days=1)\n",
        "    valid_end_datetime=valid_date_end + timedelta(hours=(int(obs_end_hour)/100))\n",
        "    core_init = nbm_init\n",
        "    nbm_core_valid_end_datetime = valid_date_end\n",
        "    nbm_qmd_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_qmd_valid_hour))\n",
        "    nbm_core_fhdelta = valid_end_datetime - nbm_init\n",
        "    #valid_date=date.strptime(valid_date,'') + timedelta(hours=)\n",
        "\n",
        "elif element == \"snow\":# or element == \"ice\":\n",
        "    nbm_core_valid_hour=(str(qpf_valid_time)).zfill(2)\n",
        "    nbm_qmd_valid_hour=(str(qpf_valid_time)).zfill(2)\n",
        "    obs_start_hour=(str(qpf_valid_time)).zfill(2)+\"00\"\n",
        "    obs_end_hour = (str(qpf_valid_time)).zfill(2)+\"00\"\n",
        "    valid_date = datetime.strptime(valid_date,'%Y-%m-%d') #+ timedelta(hours=int(qpf_valid_time))\n",
        "    valid_date_start = valid_date - timedelta(hours=24)\n",
        "    valid_date_end = valid_date\n",
        "    valid_end_datetime = valid_date_end + timedelta(hours=(int(obs_end_hour)/100))\n",
        "    ob_stat = \"total\"\n",
        "    core_init = nbm_init + timedelta(hours = 1)\n",
        "    nbm_qmd_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_qmd_valid_hour))\n",
        "    nbm_core_valid_end_datetime = nbm_qmd_valid_end_datetime\n",
        "    nbm_core_fhdelta = nbm_core_valid_end_datetime - core_init\n",
        "\n",
        "current_datetime=datetime.now()\n",
        "\n",
        "nbm_qmd_fhdelta = nbm_qmd_valid_end_datetime - nbm_init\n",
        "nbm_qmd_forecasthour = nbm_qmd_fhdelta.total_seconds() / 3600.\n",
        "if element == \"qpf\" or element == \"maxwind\" or element == \"maxgust\" or element == \"snow\" or element == \"ice24\":\n",
        "  nbm_qmd_forecasthour_start = nbm_qmd_forecasthour - 24\n",
        "else:\n",
        "  nbm_qmd_forecasthour_start = nbm_qmd_forecasthour - 18\n",
        "\n",
        "# Setup a dictionary for translating a form selection into a something we can pass to mesowest API\n",
        "network_dict = {\"NWS+RAWS+HADS\":\"&network=1,2,106\",\"NWS+RAWS\":\"&network=1,2\", \"NWS\":\"&network=1\", \"RAWS\": \"&network=2\", \"ALL\":\"\"}\n",
        "network_string = network_dict[network_selection]\n",
        "\n",
        "if element == \"qpf\":\n",
        "  cmap = get_cmap('BrBG')\n",
        "  cmap.set_under(color='black')\n",
        "  cmap.set_over(color='yellow')\n",
        "elif element == \"snow\":\n",
        "  cmap = get_cmap('cool_r')\n",
        "  cmap.set_under(color='black')\n",
        "  cmap.set_over(color='yellow')\n",
        "else:\n",
        "  #cmap = 'Spectral'\n",
        "  cmap = get_cmap('bwr')\n",
        "  cmap.set_under(color='yellow')\n",
        "  cmap.set_over(color='black')\n",
        "if use_stageiv and element==\"qpf\":\n",
        "  points_str = f'Stage IV @ {network_selection}'\n",
        "else:\n",
        "  points_str = network_selection\n",
        "\n",
        "if plot_style==\"light\":\n",
        "  background_color = '#f7f7f7'\n",
        "  text_color = '#121212'\n",
        "  map_land_color = '#FAFAF8'\n",
        "  map_water_color = '#D4DBDD'\n",
        "  map_border_color = 'grey'\n",
        "elif plot_style==\"dark\":\n",
        "  background_color = '#272727'\n",
        "  text_color = 'white'\n",
        "  map_land_color = '#414143'\n",
        "  map_water_color = '#272727'\n",
        "  #map_border_color = '#3B3B3D'\n",
        "  map_border_color = 'white'\n",
        "\n",
        "########################################################################################################################\n",
        "# Reusable functions section                                                                                           #\n",
        "########################################################################################################################\n",
        "\n",
        "def project3(lon, lat, prj):\n",
        "  lon = float(lon)\n",
        "  lat = float(lat)\n",
        "\n",
        "  outproj = prj\n",
        "  inproj = Proj(init='epsg:4326')\n",
        "  nbm_coords = transform(inproj, outproj, lon, lat)\n",
        "  coordX = nbm_coords[0]\n",
        "  coordY = nbm_coords[1]\n",
        "  #print(f'Lat: {lat}, Y: {coordY} | Lon: {lon}, X: {coordX}')\n",
        "  return(coordX, coordY)\n",
        "\n",
        "\n",
        "def ll_to_index(datalons, datalats, loclon, loclat):\n",
        "  abslat = np.abs(datalats-loclat)\n",
        "  abslon = np.abs(datalons-loclon)\n",
        "  c = np.maximum(abslon, abslat)\n",
        "  latlon_idx_flat = np.argmin(c)\n",
        "  latlon_idx = np.unravel_index(latlon_idx_flat, datalons.shape)\n",
        "  return(latlon_idx)\n",
        "\n",
        "\n",
        "def project_hrap(lon, lat, s4x, s4y):\n",
        "  lon = float(lon)\n",
        "  lat = float(lat)\n",
        "\n",
        "  globe = ccrs.Globe(semimajor_axis=6371200)\n",
        "  hrap_ccrs = proj = ccrs.Stereographic(central_latitude=90.0,\n",
        "                          central_longitude=255.0,\n",
        "                          true_scale_latitude=60.0, globe=globe)\n",
        "  latlon_ccrs = ccrs.PlateCarree()\n",
        "  hrap_coords = hrap_ccrs.transform_point(lon,lat,src_crs=latlon_ccrs)\n",
        "  hrap_idx = ll_to_index(s4x, s4y, hrap_coords[0], hrap_coords[1])\n",
        "\n",
        "  return hrap_idx\n",
        "\n",
        "def nohrsc_ll2ij(lon,lat,gridlons,gridlats):\n",
        "  #for a lat/lon grid\n",
        "  lon = float(lon)\n",
        "  lat = float(lat)\n",
        "  lonidx=(np.abs(lon-gridlons)).argmin()\n",
        "  latidx=(np.abs(lat-gridlats)).argmin()\n",
        "  return(latidx,lonidx)\n",
        "\n",
        "def get_stageiv():\n",
        "  siv_url = \"https://water.weather.gov/precip/downloads/\"+valid_date_end.strftime('%Y')+\"/\"+valid_date_end.strftime('%m')+\"/\"+valid_date_end.strftime('%d')+\"/nws_precip_1day_\"+valid_date_end.strftime('%Y%m%d')+\"_conus.nc\"\n",
        "  data = urlopen(siv_url).read()\n",
        "\n",
        "  nc = Dataset('data', memory=data)\n",
        "  #with Dataset(siv_file, 'r') as nc:\n",
        "  stageIV = nc.variables['observation']\n",
        "  s4x = nc.variables['x']\n",
        "  s4y = nc.variables['y']\n",
        "  return stageIV, s4x, s4y\n",
        "\n",
        "def get_nohrsc():\n",
        "  nohrsc_url = \"https://www.nohrsc.noaa.gov/snowfall_v2/data/\"+valid_date_end.strftime('%Y%m')+\"/sfav2_CONUS_24h_\"+valid_date_end.strftime('%Y%m%d%H')+\".nc\"\n",
        "  data = urlopen(nohrsc_url).read()\n",
        "\n",
        "  nc = Dataset('data',memory=data)\n",
        "  snow=np.asarray(nc.variables['Data']) #make lon by lat array (original lat by lon)\n",
        "  snowlat = np.asarray(nc.variables['lat'])\n",
        "  snowlon = np.asarray(nc.variables['lon'])\n",
        "  return snow,snowlon,snowlat\n",
        "\n",
        "def K_to_F(kelvin):\n",
        "  fahrenheit = 1.8*(kelvin-273)+32.\n",
        "  return fahrenheit\n",
        "\n",
        "def mps_to_kts(mps):\n",
        "  kts = mps * 1.94384\n",
        "  return kts\n",
        "\n",
        "def mm_to_in(millimeters):\n",
        "  inches = millimeters * 0.0393701\n",
        "  return inches\n",
        "\n",
        "def meters_to_in(meters):\n",
        "  inches = meters*39.3701\n",
        "  return inches\n",
        "\n",
        "def find_roots(x,y):\n",
        "  s = np.abs(np.diff(np.sign(y))).astype(bool)\n",
        "  return x[:-1][s] + np.diff(x)[s]/(np.abs(y[1:][s]/y[:-1][s])+1)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KHNSqQwLHqVj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Obs\n",
        "\n",
        "synoptic_token = \"62e9269f0a164da1b2415ddcf8f4f29e\"\n",
        "statistics_api = \"https://api.synopticlabs.org/v2/stations/statistics?\"\n",
        "precipitation_api = \"https://api.synopticdata.com/v2/stations/precipitation?\"\n",
        "metadata_api = \"https://api.synopticdata.com/v2/stations/metadata?\"\n",
        "\n",
        "print('Getting obs...')\n",
        "obs ={}\n",
        "\n",
        "for region in region_list:\n",
        "  if (valid_end_datetime <= current_datetime):\n",
        "    print(\"   > Grabbing obs for: \", region)\n",
        "    #print(\"List of CWAs: \", cwa_list(region) )\n",
        "    json_name = \"obs/Obs_\"+element+\"_\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour+\"_\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour+\"_\"+region+\".json\"\n",
        "    if os.path.exists(\"obs\"):\n",
        "      pass\n",
        "    else:\n",
        "      os.mkdir(\"obs\")\n",
        "    if element == \"mint\" or element == \"maxt\":\n",
        "      api_token = \"&token=\"+synoptic_token\n",
        "      station_query = \"&cwa=\"+cwa_list(region)\n",
        "      vars_query = \"&vars=air_temp\"\n",
        "      start_query = \"&start=\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour\n",
        "      end_query = \"&end=\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour\n",
        "      stat_type = \"&type=\"+ob_stat\n",
        "      network_query = network_string\n",
        "      api_extras = \"&units=temp%7Cf&within=1440&status=active\"\n",
        "      obs_url = statistics_api + api_token + station_query + vars_query + start_query + end_query + stat_type + network_query + api_extras\n",
        "    elif element == \"maxwind\":\n",
        "      api_token = \"&token=\"+synoptic_token\n",
        "      station_query = \"&cwa=\"+cwa_list(region)\n",
        "      vars_query = \"&vars=wind_speed\"\n",
        "      start_query = \"&start=\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour\n",
        "      end_query = \"&end=\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour\n",
        "      stat_type = \"&type=\"+ob_stat\n",
        "      network_query = network_string\n",
        "      obs_url = statistics_api + api_token + station_query + vars_query + start_query + end_query + stat_type + network_query\n",
        "    elif element == \"qpf\":\n",
        "      if use_stageiv:\n",
        "        api_token = \"&token=\"+synoptic_token\n",
        "        station_query = \"&cwa=\"+cwa_list(region)\n",
        "        api_extras = \"&fields=status,latitude,longitude,name,elevation\"\n",
        "        network_query = network_string\n",
        "        obs_url = metadata_api + api_token + station_query + network_query + api_extras\n",
        "        stageIV, s4xs, s4ys = get_stageiv()\n",
        "        s4xs, s4ys = np.meshgrid(s4xs, s4ys)\n",
        "      else:\n",
        "        api_token = \"&token=\"+synoptic_token\n",
        "        station_query = \"&cwa=\"+cwa_list(region)\n",
        "        api_extras = \"&fields=status,latitude,longitude,name,elevation&obtimezone=utc\"\n",
        "        network_query = network_string\n",
        "        vars_query = \"&pmode=totals\"\n",
        "        units_query = \"&units=precip|in\"\n",
        "        start_query = \"&start=\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour\n",
        "        end_query = \"&end=\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour\n",
        "        obs_url = precipitation_api + api_token + station_query + network_query + vars_query + start_query + end_query + units_query + api_extras\n",
        "    elif element == \"snow\":\n",
        "      if use_nohrsc:\n",
        "        api_token = \"&token=\"+synoptic_token\n",
        "        station_query = \"&cwa=\"+cwa_list(region)\n",
        "        api_extras = \"&fields=status,latitude,longitude,name,elevation\"\n",
        "        network_query = network_string\n",
        "        obs_url = metadata_api + api_token + station_query + network_query + api_extras\n",
        "        snow,snowlon,snowlat = get_nohrsc()\n",
        "        snowlons,snowlats = np.meshgrid(snowlon,snowlat)\n",
        "      else:\n",
        "        api_token = \"&token=\"+synoptic_token\n",
        "        station_query = \"&cwa=\"+cwa_list(region)\n",
        "        api_extras = \"&fields=status,latitude,longitude,name,elevation&obtimezone=utc\"\n",
        "        network_query = network_string\n",
        "        vars_query = \"&pmode=totals\"\n",
        "        units_query = \"&units=precip|in\"\n",
        "        start_query = \"&start=\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour\n",
        "        end_query = \"&end=\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour\n",
        "        obs_url = precipitation_api + api_token + station_query + network_query + vars_query + start_query + end_query + units_query + api_extras\n",
        "    print(\"Obs url: \" + obs_url)\n",
        "    if os.path.exists(json_name):\n",
        "      print (\"Skipping download since JSON file already exists\")\n",
        "      pass\n",
        "    else:\n",
        "      urlretrieve(obs_url, json_name)\n",
        "\n",
        "    if os.path.exists(json_name):\n",
        "        with open(json_name) as json_file:\n",
        "            obs_json = json.load(json_file)\n",
        "            print (\"Loaded Obs JSON file line 343: \" + json_name)\n",
        "            obs_lats = []\n",
        "            obs_lons = []\n",
        "            obs_value = []\n",
        "            obs_elev = []\n",
        "            obs_stid = []\n",
        "            obs_name = []\n",
        "            for stn in obs_json[\"STATION\"]:\n",
        "                if stn[\"STID\"] is None:\n",
        "                  stid = \"N0N3\"\n",
        "                else:\n",
        "                  stid = stn[\"STID\"]\n",
        "                name = stn[\"NAME\"]\n",
        "                if stn[\"ELEVATION\"] and stn[\"ELEVATION\"] is not None:\n",
        "                  elev = stn[\"ELEVATION\"]\n",
        "                else:\n",
        "                  elev = -999\n",
        "                lat = stn[\"LATITUDE\"]\n",
        "                lon = stn[\"LONGITUDE\"]\n",
        "                if float(lon) > -50:\n",
        "                  continue #bug fix to deal with errant synoptic labs obs in the file\n",
        "                if element == \"mint\" or element==\"maxt\":\n",
        "                  if 'air_temp_set_1' in stn['STATISTICS'] and stn['STATISTICS']['air_temp_set_1']:\n",
        "                    if ob_stat in stn['STATISTICS']['air_temp_set_1']: # and float(stn[\"LATITUDE\"]) != 0. and float(stn[\"LONGITUDE\"]) != 0.:\n",
        "                      stat = stn['STATISTICS']['air_temp_set_1'][ob_stat]\n",
        "                      obs_stid.append(str(stid))\n",
        "                      obs_name.append(str(name))\n",
        "                      obs_elev.append(float(elev))\n",
        "                      obs_lats.append(float(lat))\n",
        "                      obs_lons.append(float(lon))\n",
        "                      obs_value.append(float(stat))\n",
        "                elif element == \"maxwind\":\n",
        "                  if 'wind_speed_set_1' in stn['STATISTICS'] and stn['STATISTICS']['wind_speed_set_1']:\n",
        "                    if ob_stat in stn['STATISTICS']['wind_speed_set_1']: # and float(stn[\"LATITUDE\"]) != 0.:\n",
        "                      stat = stn['STATISTICS']['wind_speed_set_1'][ob_stat]\n",
        "                      obs_stid.append(str(stid))\n",
        "                      obs_name.append(str(name))\n",
        "                      obs_elev.append(float(elev))\n",
        "                      obs_lats.append(float(lat))\n",
        "                      obs_lons.append(float(lon))\n",
        "                      obs_value.append(mps_to_kts(float(stat)))\n",
        "                elif (element == \"qpf\"):\n",
        "                  if (stn[\"STATUS\"] == \"ACTIVE\"): # and float(stn[\"LATITUDE\"]) < 50.924 and float(stn[\"LATITUDE\"]) > 23.377 and float(stn[\"LONGITUDE\"]) > -125.650 and float(stn[\"LONGITUDE\"]) < -66.008:\n",
        "                    obs_stid.append(str(stid))\n",
        "                    obs_name.append(str(name))\n",
        "                    obs_elev.append(float(elev))\n",
        "                    obs_lats.append(float(lat))\n",
        "                    obs_lons.append(float(lon))\n",
        "                    if use_stageiv:\n",
        "                      coords = project_hrap(lon, lat, s4xs, s4ys)\n",
        "                      siv_value = float(stageIV[coords])\n",
        "                      if (siv_value >= 0.01):\n",
        "                        obs_value.append(siv_value)\n",
        "                      else:\n",
        "                        obs_value.append(0.0)\n",
        "                    else:\n",
        "                      if \"precipitation\" in stn[\"OBSERVATIONS\"]:\n",
        "                        if \"total\" in stn[\"OBSERVATIONS\"][\"precipitation\"][0]:\n",
        "                          ptotal = stn[\"OBSERVATIONS\"][\"precipitation\"][0][\"total\"]\n",
        "                          if ptotal >= 0.005:\n",
        "                            obs_value.append(ptotal)\n",
        "                          else:\n",
        "                            obs_value.append(0.0)\n",
        "                        else:\n",
        "                          obs_value.append(np.nan)\n",
        "                      else:\n",
        "                        obs_value.append(np.nan)\n",
        "                elif (element == \"snow\"):\n",
        "                  if stn[\"STATUS\"] == \"ACTIVE\": # and float(stn[\"LATITUDE\"]) < 50.924)and float(stn[\"LATITUDE\"]) > 23.377 and float(stn[\"LONGITUDE\"]) > -125.650 and float(stn[\"LONGITUDE\"]) < -66.008:\n",
        "                    obs_stid.append(str(stid))\n",
        "                    obs_name.append(str(name))\n",
        "                    obs_elev.append(float(elev))\n",
        "                    obs_lats.append(float(lat))\n",
        "                    obs_lons.append(float(lon))\n",
        "                    if use_nohrsc:\n",
        "                      coords = nohrsc_ll2ij(lon,lat,snowlon,snowlat)\n",
        "                      nohrsc_value = meters_to_in(float(snow[coords]))\n",
        "                      if nohrsc_value >= 0.005:\n",
        "                        obs_value.append(nohrsc_value)\n",
        "                      elif nohrsc_value < 0.0:\n",
        "                        obs_value.append(np.nan)\n",
        "                      else:\n",
        "                        obs_value.append(0.0)\n",
        "                    else:\n",
        "                      raise Exception(\"Still not able to process individual snow obs!\")\n",
        "            csv_name = \"obs_\"+element+\"_\"+region+\"_\"+valid_date_end.strftime('%Y%m%d')+\".csv\"\n",
        "            obs[region] = pd.DataFrame()\n",
        "            obs[region][\"stid\"] = obs_stid\n",
        "            obs[region][\"name\"] = obs_name\n",
        "            obs[region][\"elevation\"] = obs_elev\n",
        "            obs[region][\"lat\"] = obs_lats\n",
        "            obs[region][\"lon\"] = obs_lons\n",
        "            obs[region][\"ob_\"+element] = obs_value\n",
        "            obs[region].dropna(inplace=True)\n",
        "            obs[region].to_csv(csv_name)\n",
        "  else:\n",
        "    print(f'    > Valid Time in the future. Grabbing obs points only for: {region}')\n",
        "    json_name = \"obs/ObsPoints_\"+region+\"_wcoss.json\"\n",
        "    if os.path.exists(json_name):\n",
        "      pass\n",
        "    else:\n",
        "      if os.path.exists(\"obs\"):\n",
        "        pass\n",
        "      else:\n",
        "        os.mkdir(\"obs\")\n",
        "      obs_url = \"https://api.synopticdata.com/v2/stations/metadata?&token=\"+synoptic_token+\"&cwa=\"+cwa_list(region)+\"&fields=status,latitude,longitude,name,elevation\"+network_string\n",
        "      urlretrieve(obs_url, json_name)\n",
        "    if os.path.exists(json_name):\n",
        "      with open(json_name) as json_file:\n",
        "          obs_json = json.load(json_file)\n",
        "          print(\"Loaded Obs JSON file line 197!\")\n",
        "          obs_lats = []\n",
        "          obs_lons = []\n",
        "          obs_elev = []\n",
        "          obs_stid = []\n",
        "          obs_name = []\n",
        "          for stn in obs_json[\"STATION\"]:\n",
        "            # print(stn.encode('utf-8'))\n",
        "            if stn[\"STID\"] is None:\n",
        "              stid = \"N0N3\"\n",
        "            else:\n",
        "              stid = stn[\"STID\"]\n",
        "            #print(f'Processing {region} station {stid}')\n",
        "            name = stn[\"NAME\"]\n",
        "            if stn[\"ELEVATION\"] and stn[\"ELEVATION\"] is not None:\n",
        "              elev = stn[\"ELEVATION\"]\n",
        "            else:\n",
        "              elev = -999\n",
        "            lat = stn[\"LATITUDE\"]\n",
        "            lon = stn[\"LONGITUDE\"]\n",
        "            if stn[\"STATUS\"] == \"ACTIVE\": # and float(stn[\"LATITUDE\"]) != 0. and float(stn[\"LONGITUDE\"]) != 0.:\n",
        "              obs_stid.append(str(stid))\n",
        "              obs_name.append(str(name))\n",
        "              obs_elev.append(float(elev))\n",
        "              obs_lats.append(float(lat))\n",
        "              obs_lons.append(float(lon))\n",
        "          obs[region] = pd.DataFrame()\n",
        "          obs[region][\"stid\"] = obs_stid\n",
        "          obs[region][\"name\"] = obs_name\n",
        "          obs[region][\"elevation\"] = obs_elev\n",
        "          obs[region][\"lat\"] = obs_lats\n",
        "          obs[region][\"lon\"] = obs_lons\n",
        "          obs[region][\"ob_\"+element] = -999\n",
        "          obs[region].dropna(inplace=True)\n",
        "          #obs[region].to_csv(csv_name)\n",
        "\n",
        "#end up with one master frame of all obs\n",
        "masterobs=pd.concat(obs.values(),ignore_index=True)\n",
        "masterobs.to_csv(\"allobs.csv\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DX9ZfztpLfgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Gridded Data\n",
        "\n",
        "def getthresh(element):\n",
        "\tif element == \"gust\" or element == \"maxgust\":\n",
        "\t\t#kts:m/s\n",
        "\t\tidthresh = {\n",
        "\t\t\t22:11.32,\n",
        "\t\t\t34:17.49,\n",
        "\t\t\t41:21.09,\n",
        "\t\t\t48:24.69,\n",
        "\t\t\t56:28.81,\n",
        "\t\t\t64:32.92\n",
        "\t\t}\n",
        "\telif element == \"wind\" or element == \"maxwind\":\n",
        "\t\t#kts:m/s\n",
        "\t\tidthresh = {\n",
        "\t\t\t11:5.66,\n",
        "\t\t\t17:8.75,\n",
        "\t\t\t22:11.32,\n",
        "\t\t\t34:17.49,\n",
        "\t\t\t48:24.69,\n",
        "\t\t\t64:32.92\n",
        "\t\t}\n",
        "\telif element == \"maxt\":\n",
        "\t\t#F:K\n",
        "\t\tidthresh = {\n",
        "\t\t\t0:255,\n",
        "\t\t\t28:270,\n",
        "\t\t\t32:273,\n",
        "\t\t\t80:299,\n",
        "\t\t\t90:305,\n",
        "\t\t\t100:310,\n",
        "\t\t\t110:316,\n",
        "\t\t\t120:322\n",
        "\t\t}\n",
        "\telif element == \"mint\":\n",
        "\t\t#F:K\n",
        "\t\tidthresh = {\n",
        "\t\t\t-40:233,\n",
        "\t\t\t-20:244,\n",
        "\t\t\t0:255,\n",
        "\t\t\t10:260,\n",
        "\t\t\t28:270,\n",
        "\t\t\t32:273\n",
        "\t\t\t#80:299\n",
        "\t\t}\n",
        "\telif element == \"qpf\": #24-H QPF\n",
        "\t\t#in:in, no conversion necessary\n",
        "\t\tidthresh = {\n",
        "\t\t\t0.01:0.01,\n",
        "\t\t\t0.25:0.25,\n",
        "\t\t\t0.50:0.50,\n",
        "\t\t\t1.00:1.00,\n",
        "\t\t\t2.00:2.00,\n",
        "\t\t\t3.00:3.00,\n",
        "\t\t\t4.00:4.00,\n",
        "\t\t\t5.00:5.00,\n",
        "\t\t\t6.00:6.00,\n",
        "\t\t\t8.00:8.00\n",
        "\t\t}\n",
        "\telif element == \"qpf06\":\n",
        "\t\tidthresh = {\n",
        "\t\t\t0.1:0.1,\n",
        "\t\t\t0.25:0.25,\n",
        "\t\t\t0.50:0.50,\n",
        "\t\t\t0.75:0.75,\n",
        "\t\t\t1.00:1.00,\n",
        "\t\t\t1.50:1.50,\n",
        "\t\t\t2.00:2.00,\n",
        "\t\t\t2.50:2.50,\n",
        "\t\t\t3.00:3.00\n",
        "\t\t}\n",
        "\telif element == \"snow\": #snow 24\n",
        "\t\tidthresh = {\n",
        "\t\t\t0.1:0.1,\n",
        "\t\t\t0.3:0.3,\n",
        "\t\t\t0.5:0.5,\n",
        "\t\t\t0.7:0.7,\n",
        "\t\t\t1.0:1.0,\n",
        "\t\t\t1.5:1.5,\n",
        "\t\t\t2.0:2.0,\n",
        "\t\t\t2.5:2.5,\n",
        "\t\t\t3.0:3.0,\n",
        "\t\t\t4.0:4.0,\n",
        "\t\t\t6.0:6.0,\n",
        "\t\t\t8.0:8.0,\n",
        "\t\t\t10.0:10.0,\n",
        "\t\t\t12.0:12.0,\n",
        "\t\t\t18.0:18.0,\n",
        "\t\t\t24.0:24.0,\n",
        "\t\t\t30.0:30.0,\n",
        "\t\t\t36.0:36.0,\n",
        "\t\t\t48.0:48.0\n",
        "\t\t}\n",
        "\telif element == \"snow6\":\n",
        "\t\tidthresh = {\n",
        "\t\t\t0.1:0.1,\n",
        "\t\t\t0.3:0.3,\n",
        "\t\t\t0.5:0.5,\n",
        "\t\t\t0.7:0.7,\n",
        "\t\t\t1.0:1.0,\n",
        "\t\t\t1.5:1.5,\n",
        "\t\t\t2.0:2.0,\n",
        "\t\t\t2.5:2.5,\n",
        "\t\t\t3.0:3.0,\n",
        "\t\t\t4.0:4.0,\n",
        "\t\t\t5.0:5.0,\n",
        "\t\t\t6.0:6.0,\n",
        "\t\t\t8.0:8.0,\n",
        "\t\t\t10.0:10.0,\n",
        "\t\t\t12.0:12.0,\n",
        "\t\t\t18.0:18.0,\n",
        "\t\t\t24.0:24.0,\n",
        "\t\t\t30.0:30.0,\n",
        "\t\t\t36.0:36.0,\n",
        "\t\t\t48.0:48.0\n",
        "\t\t}\n",
        "\telif element in [\"ice6\",\"ice\"]:\n",
        "\t\tidthresh = {\n",
        "\t\t\t0.01:0.01,\n",
        "\t\t\t0.1:0.1,\n",
        "\t\t\t0.25:0.25,\n",
        "\t\t\t0.50:0.50,\n",
        "\t\t\t1.00:1.00\n",
        "\t\t}\n",
        "\telse:\n",
        "\t\traise Exception (\"No thresholds in place yet for element \" + element + \".  Look at convert.py to add thresholds as needed.\")\n",
        "\treturn idthresh\n",
        "\n",
        "\n",
        "def download_subset(remote_url, remote_file, local_filename):\n",
        "  print(\"   > Downloading a subset of NBM gribs\")\n",
        "  local_file = \"nbm/\"+local_filename\n",
        "  if \"qmd\" in remote_file:\n",
        "    if element == \"maxt\":\n",
        "      if (int(nbm_qmd_forecasthour_start) % 24 == 0) and (int(nbm_qmd_forecasthour) % 24 ==0):\n",
        "        search_string = f':TMP:2 m above ground:{str(int(int(nbm_qmd_forecasthour_start)/24))}-{str(int(int(nbm_qmd_forecasthour)/24))} day max fcst:'\n",
        "      else:\n",
        "        search_string = f':TMP:2 m above ground:{str(int(nbm_qmd_forecasthour_start))}-{str(int(nbm_qmd_forecasthour))} hour max fcst:'\n",
        "    elif element == \"mint\":\n",
        "      if (int(nbm_qmd_forecasthour_start) % 24 == 0) and (int(nbm_qmd_forecasthour) % 24 ==0):\n",
        "        search_string = f':TMP:2 m above ground:{str(int(int(nbm_qmd_forecasthour_start)/24))}-{str(int(int(nbm_qmd_forecasthour)/24))} day min fcst:'\n",
        "      else:\n",
        "        search_string = f':TMP:2 m above ground:{str(int(nbm_qmd_forecasthour_start))}-{str(int(nbm_qmd_forecasthour))} hour min fcst:'\n",
        "    elif element == \"qpf\":\n",
        "      if (int(nbm_qmd_forecasthour_start) % 24 == 0) and (int(nbm_qmd_forecasthour) % 24 ==0):\n",
        "        search_string = f':APCP:surface:{str(int(int(nbm_qmd_forecasthour_start)/24))}-{str(int(int(nbm_qmd_forecasthour)/24))} day acc fcst:'\n",
        "      else:\n",
        "        search_string = f':APCP:surface:{str(int(nbm_qmd_forecasthour_start))}-{str(int(nbm_qmd_forecasthour))} hour acc fcst:'\n",
        "    elif element == \"maxwind\":\n",
        "      if (int(nbm_qmd_forecasthour_start) % 24 == 0) and (int(nbm_qmd_forecasthour) % 24 == 0):\n",
        "        search_string = f':WIND:10 m above ground:{str(int(nbm_qmd_forecasthour_start/24))}-{str(int(nbm_qmd_forecasthour/24))} hour max fcst:'\n",
        "      else:\n",
        "        search_string = f':WIND:10 m above ground:{str(int(nbm_qmd_forecasthour_start))}-{str(int(nbm_qmd_forecasthour))} hour max fcst:'\n",
        "  elif \"core\" in remote_file:\n",
        "    if element == \"maxt\":\n",
        "      search_string = f':TMAX:2 m above ground:{str(int(nbm_core_forecasthour_start))}-{str(int(nbm_core_forecasthour))} hour max fcst:'\n",
        "    elif element == \"mint\":\n",
        "      search_string = f':TMIN:2 m above ground:{str(int(nbm_core_forecasthour_start))}-{str(int(nbm_core_forecasthour))} hour min fcst:'\n",
        "    elif element == \"snow\":\n",
        "      search_string = f':ASNOW:surface:{str(int(nbm_core_forecasthour_start))}-{str(int(nbm_core_forecasthour))} hour acc'\n",
        "  #print(\"Search string = \",search_string)\n",
        "  idx = remote_url+\".idx\"\n",
        "  #print(\"IDX file = \" + idx)\n",
        "  r = requests.get(idx)\n",
        "  if not r.ok:\n",
        "    print('     ❌ SORRY! Status Code:', r.status_code, r.reason)\n",
        "    print(f'      ❌ It does not look like the index file exists: {idx}')\n",
        "\n",
        "  lines = r.text.split('\\n')\n",
        "  expr = re.compile(search_string)\n",
        "  expr\n",
        "  byte_ranges = {}\n",
        "  for n, line in enumerate(lines, start=1):\n",
        "    # n is the line number (starting from 1) so that when we call for\n",
        "    # `lines[n]` it will give us the next line. (Clear as mud??)\n",
        "    # Use the compiled regular expression to search the line\n",
        "    #print(\">> Searching throgh this line: \" + line)\n",
        "    if expr.search(line):\n",
        "      # aka, if the line contains the string we are looking for...\n",
        "      # Get the beginning byte in the line we found\n",
        "      parts = line.split(':')\n",
        "      rangestart = int(parts[1])\n",
        "      # Get the beginning byte in the next line...\n",
        "      if n+1 < len(lines):\n",
        "        # ...if there is a next line\n",
        "        parts = lines[n].split(':')\n",
        "        rangeend = int(parts[1])\n",
        "      else:\n",
        "        # ...if there isn't a next line, then go to the end of the file.\n",
        "        rangeend = ''\n",
        "\n",
        "        # Store the byte-range string in our dictionary,\n",
        "        # and keep the line information too so we can refer back to it.\n",
        "      byte_ranges[f'{rangestart}-{rangeend}'] = line\n",
        "      #print(line)\n",
        "    #else:\n",
        "      #print(\">>>  Could not find search string!\")\n",
        "  #print(\">>  Number of items in byteRange:\" + str(len(byte_ranges)))\n",
        "  for i, (byteRange, line) in enumerate(byte_ranges.items()):\n",
        "\n",
        "    if i == 0:\n",
        "      # If we are working on the first item, overwrite the existing file.\n",
        "      curl = f'curl -s --range {byteRange} {remote_url} > {local_file}'\n",
        "      #print(\">>  Adding curl command: \" + curl)\n",
        "    else:\n",
        "      # If we are working on not the first item, append the existing file.\n",
        "      curl = f'curl -s --range {byteRange} {remote_url} >> {local_file}'\n",
        "      #print(\"Adding curl command: \" + curl)\n",
        "    #print('>>  Parsing line: ' + line)\n",
        "    try:\n",
        "      num, byte, date, var, level, forecast, _ = line.split(':')\n",
        "    except:\n",
        "      pass\n",
        "      #print(\">>>  Can't get num/byte/etc from this line, so skipping...\")\n",
        "\n",
        "    #print(f'  Downloading GRIB line [{num:>3}]: variable={var}, level={level}, forecast={forecast}')\n",
        "    #print(f'  Downloading GRIB line: variable={var}, level={level}, forecast={forecast}')\n",
        "    #print(\"Running the curl command...\")\n",
        "    os.system(curl)\n",
        "\n",
        "  if os.path.exists(local_file):\n",
        "    print(f'      ✅ Success! Searched for [{search_string}] and got [{len(byte_ranges)}] GRIB fields and saved as {local_file}')\n",
        "    return local_file\n",
        "  else:\n",
        "    print(print(f'      ❌ Unsuccessful! Searched for [{search_string}] and did not find anything!'))\n",
        "\n",
        "########################################################################################################################\n",
        "# This section downloads and processes the NBM.                                                                        #\n",
        "########################################################################################################################\n",
        "if \"AR\" in region_list or \"AJK\" in region_list or \"ARH\" in region_list or \"AFC\" in region_list:\n",
        "  rg=\"ak\"\n",
        "elif \"HFO\" in region_list:\n",
        "  rg=\"hi\"\n",
        "elif \"SJU\" in region_list:\n",
        "  rg=\"pr\"\n",
        "else:\n",
        "  rg=\"co\"\n",
        "\n",
        "print('Getting and processing NBM...')\n",
        "nbm_init_filen = nbm_init.strftime('%Y%m%d') + \"_\" + nbm_init.strftime('%H')\n",
        "nbm_init_filen_core = core_init.strftime('%Y%m%d') + \"_\" + core_init.strftime('%H')\n",
        "nbm_url_base = \"https://noaa-nbm-grib2-pds.s3.amazonaws.com/blend.\"+nbm_init.strftime('%Y%m%d') \\\n",
        "            +\"/\"+nbm_init.strftime('%H')+\"/\"\n",
        "nbm_url_base_core = \"https://noaa-nbm-grib2-pds.s3.amazonaws.com/blend.\"+core_init.strftime('%Y%m%d') \\\n",
        "            +\"/\"+core_init.strftime('%H')+\"/\"\n",
        "temp_vars = [\"maxt\",\"mint\"]\n",
        "\n",
        "prob_dict = {\"maxt\":\"maxt18p\", \"mint\":\"mint18p\", \"qpf\":\"qpf24p\", \"snow\":\"snow24p\"}\n",
        "if element == \"snow\":\n",
        "  prob_file=f\"blend.t{int(core_init.strftime('%H')):02}z.core.f{int(nbm_core_forecasthour):03}.{rg}.grib2\"\n",
        "  prob_url=nbm_url_base_core+\"core/\"+prob_file\n",
        "else:\n",
        "  prob_file = f'blend.t{int(nbm_init_hour):02}z.qmd.f{int(nbm_qmd_forecasthour):03}.{rg}.grib2'\n",
        "  prob_url = nbm_url_base+\"qmd/\"+prob_file\n",
        "prob_file_subset = f'blend.t{int(nbm_init_hour):02}z.qmd.{nbm_init_filen}{nbm_init_filen}f{int(nbm_qmd_forecasthour):03}.{rg}.{element}_subset.grib2'\n",
        "print(\"prob_url=\",prob_url)\n",
        "print(\"prob_file=\",prob_file)\n",
        "if os.path.exists(\"nbm\"):\n",
        "  pass\n",
        "else:\n",
        "  os.mkdir(\"nbm\")\n",
        "#Raise exception for missinge percentile data\n",
        "if rg in [\"pr\",\"hi\"] and element in [\"maxt\",\"mint\",\"maxwind\",\"snow\"]:\n",
        "  raise Exception (\"FATAL ERROR: Percentiles for \" + element + \"do not exist for PR or HI\")\n",
        "if os.path.exists(\"nbm/\"+prob_file_subset):\n",
        "  print(\"   > NBM probabilistic already exists\")\n",
        "else:\n",
        "  #urlretrieve(perc_url, \"nbm/\"+perc_file)\n",
        "  print(\"   > Getting NBM probabilistic\")\n",
        "  download_subset(prob_url, prob_file, prob_file_subset)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "BT6FB6N9QX-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Parse Downloaded Probabilities to Obs (without map)\n",
        "\n",
        "nbmprob = pygrib.open(\"nbm/\"+prob_file_subset)\n",
        "print('   > Extracting NBM Probabilistic')\n",
        "\n",
        "idthresh=getthresh(element)\n",
        "probfield={}\n",
        "point_lats=masterobs[\"lat\"]\n",
        "point_lons=masterobs[\"lon\"]\n",
        "\n",
        "#initial read of one random field for lat/lon info and identify index/grid values\n",
        "nbm_fidx=[]\n",
        "nbmprob.seek(2)\n",
        "tmpinv=nbmprob.read(2)[0]\n",
        "nbmlats,nbmlons=tmpinv.latlons()\n",
        "for i in range(0,len(point_lats)):\n",
        "  coords = ll_to_index(nbmlons,nbmlats,point_lons[i],point_lats[i])\n",
        "  nbm_fidx.append(coords)\n",
        "masterobs[\"NBM_fidx\"] = nbm_fidx\n",
        "\n",
        "#now read probabilities and add to dataframe\n",
        "for thresh in idthresh.keys():\n",
        "  tval=idthresh[thresh]\n",
        "  if element == \"maxt\":\n",
        "    limit=tval\n",
        "    if limit < 280:\n",
        "      probinv = nbmprob.select(name=\"Maximum temperature at 2 metres since previous post-processing\",probabilityTypeName=\"Probability of event below lower limit\",lowerLimit=limit)[0]\n",
        "      prob_name=\"Prob \" + element + \" < \" + str(thresh)\n",
        "    else:\n",
        "      probinv = nbmprob.select(name=\"Maximum temperature at 2 metres since previous post-processing\",probabilityTypeName=\"Probability of event above upper limit\",upperLimit=limit)[0]\n",
        "      prob_name=\"Prob \" + element + \" > \" + str(thresh)\n",
        "    probdata = probinv.values\n",
        "  elif element == \"mint\":\n",
        "    limit=tval\n",
        "    if limit < 280:\n",
        "      probinv = nbmprob.select(name=\"Minimum temperature at 2 metres since previous post-processing\",probabilityTypeName=\"Probability of event below lower limit\",lowerLimit=limit)[0]\n",
        "      prob_name=\"Prob \" + element + \" < \"+ str(thresh)\n",
        "    else:\n",
        "      probinv = nbmprob.select(name=\"Minimum temperature at 2 metres since previous post-processing\",probabilityTypeName=\"Probability of event above upper limit\",upperLimit=limit)[0]\n",
        "      prob_name=\"Prob \" + element + \" > \" + str(thresh)\n",
        "    probdata = probinv.values\n",
        "  elif element == \"qpf\":\n",
        "    limit=tval\n",
        "    probinv = nbmprob.select(name=\"Total Precipitation\",lengthOfTimeRange=24,probabilityTypeName=\"Probability of event above upper limit\",upperLimit=limit)[0]\n",
        "    probdata = probinv.values\n",
        "    prob_name=\"Prob \" + element + \" > \" + str(thresh)\n",
        "  elif element == \"maxwind\":\n",
        "    limit=tval\n",
        "    probinv = nbmprob.select(name=\"10 metre wind speed\",lengthOfTimeRange=24,probabilityTypeName=\"Probability of event above upper limit\",upperLimit=limit)[0]\n",
        "    probdata = probinv.values\n",
        "    prob_name=\"Prob \" + element + \" > \" + str(thresh)\n",
        "  elif element == \"snow\":\n",
        "    limit=tval\n",
        "    probinv = nbmprob.select(name=\"unknown\",lengthOfTimeRange=24,probabilityTypeName=\"Probability of event above upper limit\",upperLimit=limit)[0]\n",
        "    probdata = probinv.values\n",
        "    prob_name=\"Prob \" + element + \" > \" + str(thresh)\n",
        "  print(\"    >>  Extracted \" + prob_name)\n",
        "  nbm_coords = masterobs[\"NBM_fidx\"].values\n",
        "  prob_values = []\n",
        "  for i in range(0, len(nbm_coords)):\n",
        "    prob_value = probdata[nbm_coords[i]]\n",
        "    prob_values.append(prob_value)\n",
        "  masterobs[prob_name] = prob_values\n",
        "  probfield[thresh] = probdata\n",
        "nbmprob.close()\n",
        "masterobs.to_csv(\"allobs_withprobs.csv\")"
      ],
      "metadata": {
        "id": "O0SBkwpO6LTq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extract Probabilities and Make Maps\n",
        "\n",
        "try:\n",
        "  nbmprob\n",
        "except:\n",
        "  nbmprob = pygrib.open(\"nbm/\"+prob_file_subset)\n",
        "  print('   > Extracting NBM Probabilistic')\n",
        "  nbmprob.seek(2)\n",
        "  tmpinv=nbmprob.read(2)[0]\n",
        "  nbmlats,nbmlons=tmpinv.latlons()\n",
        "\n",
        "idthresh=getthresh(element)\n",
        "\n",
        "valdate=valid_end_datetime\n",
        "\n",
        "matplotlib.rc('axes',facecolor=background_color,edgecolor=text_color)\n",
        "fig_valid_date=valdate.strftime('%Y%m%d_%HZ')\n",
        "valid_title=valdate.strftime('%HZ %a %m-%d-%Y')\n",
        "nbm_init_title=nbm_init.strftime('%HZ %m-%d-%Y')\n",
        "nbm_init_string=nbm_init.strftime('%Y%m%d_%H') + \"Z\"\n",
        "\n",
        "print(\"Making plot for custom region!\")\n",
        "west = np.nanmin(masterobs[\"lon\"]) - 1.0\n",
        "south = np.nanmin(masterobs[\"lat\"]) - 1.0\n",
        "east = np.nanmax(masterobs[\"lon\"]) + 1.0\n",
        "north = np.nanmax(masterobs[\"lat\"]) + 1.0\n",
        "width, height = (16,9)\n",
        "ratioxy = 16./9.\n",
        "width_ratios = [ratioxy]#, 1]\n",
        "proj = ccrs.PlateCarree()\n",
        "print(\"Plot dimensions have been set up!\")\n",
        "plevels=[1,10,20,30,40,50,60,70,80,90,99]\n",
        "\n",
        "for thresh in idthresh.keys():\n",
        "  cust_labels=[]\n",
        "  print(\"Looking at threshold: \" + str(thresh))\n",
        "  if element == \"mint\":\n",
        "    fieldName=\"Minimum temperature at 2 metres since previous post-processing\"\n",
        "    if thresh <= 32:\n",
        "      obsplot=masterobs[masterobs[\"ob_mint\"] < thresh]\n",
        "      probName=\"Probability of event below lower limit\"\n",
        "      tdir=\"< \"\n",
        "    else:\n",
        "      obsplot=masterobs[masterobs[\"ob_mint\"] > thresh]\n",
        "      probName=\"Probability of event above upper limit\"\n",
        "      tdir=\"> \"\n",
        "  elif element == \"maxt\":\n",
        "    fieldName=\"Maximum temperature at 2 metres since previous post-processing\"\n",
        "    if thresh < 80:\n",
        "      obsplot=masterobs[masterobs[\"ob_maxt\"] < thresh]\n",
        "      probName=\"Probability of event below lower limit\"\n",
        "      tdir=\"< \"\n",
        "    else:\n",
        "      obsplot=masterobs[masterobs[\"ob_maxt\"] > thresh]\n",
        "      probName=\"Probability of event above upper limit\"\n",
        "      tdir=\"> \"\n",
        "  elif element == \"qpf\":\n",
        "    fieldName=\"Total Precipitation\"\n",
        "    obsplot=masterobs[masterobs[\"ob_qpf\"] > thresh]\n",
        "    probName=\"Probability of event above upper limit\"\n",
        "    tdir=\"> \"\n",
        "  elif element == \"snow\":\n",
        "    fieldName=\"unknown\"\n",
        "    obsplot=masterobs[masterobs[\"ob_snow\"] > thresh]\n",
        "    probName=\"Probability of event above upper limit\"\n",
        "    tdir=\"> \"\n",
        "  elif element == \"maxwind\":\n",
        "    fieldName=\"10 metre wind speed\"\n",
        "    obsplot=masterobs[masterobs[\"ob_maxwind\"] > thresh]\n",
        "    probName=\"Probability of event above upper limit\"\n",
        "    tdir=\"> \"\n",
        "  #if tdir == \"> \":\n",
        "  #  probfield = nbmprob.select(name=fieldName,probabilityTypeName=probName,upperLimit=thresh)[0]\n",
        "  #  probdata = probfield.values\n",
        "  #else:\n",
        "  #  probfield = nbmprob.select(name=fieldName,probabilityTypeName=probName,lowerLimit=thresh)[0]\n",
        "  #  probdata = probfield.values\n",
        "  fig = plt.figure(constrained_layout=True, figsize=(width,height), facecolor=background_color, frameon=True, dpi=150)\n",
        "  grid = fig.add_gridspec(1,1, hspace=0.1, width_ratios=width_ratios, height_ratios = [1], wspace=0.2)\n",
        "  ax1 = fig.add_subplot(grid[0,0], projection=ccrs.Mercator())\n",
        "  ax1.set_anchor('N')\n",
        "  ax1.set_facecolor(background_color)\n",
        "  ax1.set_extent([west, east, south, north], crs=proj)\n",
        "  ax1.add_feature(cfeature.OCEAN, edgecolor='none', facecolor=map_water_color, zorder=-2)\n",
        "  ax1.add_feature(cfeature.NaturalEarthFeature('physical', 'land', '50m', edgecolor='none', facecolor=map_land_color, zorder=-1))\n",
        "  #ax1.add_feature(cfeature.LAKES, edgecolor='none', facecolor='#272727', zorder=0)\n",
        "  ax1.add_feature(cfeature.NaturalEarthFeature('physical', 'lakes', '10m', edgecolor='none', facecolor=map_water_color, zorder=0))\n",
        "  ax1.add_feature(cfeature.BORDERS, edgecolor=map_border_color, facecolor='none', linewidth=2, zorder=1)\n",
        "  #ax1.add_feature(cfeature.NaturalEarthFeature('cultural', 'countries', '50m', edgecolor=map_border_color, facecolor='none', linewidth=2, zorder=2))\n",
        "  ax1.add_feature(cfeature.NaturalEarthFeature('cultural', 'admin_1_states_provinces_lines', '50m', edgecolor=map_border_color, facecolor='none', linewidth=1, zorder=2))\n",
        "  hits=ax1.scatter(obsplot[\"lon\"].values,obsplot[\"lat\"].values,color=\"white\",s=4,transform=proj,label=\"Observed Event\")\n",
        "  probs=ax1.contour(nbmlons,nbmlats,probfield[thresh],levels=plevels,cmap=plt.get_cmap('jet'),alpha=0.75,linewidths=1.5,linestyles='solid',transform=proj)\n",
        "  #plt.clabel(probs,inline=1,fontsize=10)\n",
        "  for i in range(len(plevels)):\n",
        "    probs.collections[i].set_label(str(plevels[i]) + \"% Probability\")\n",
        "    cust_labels.append(str(plevels[i]) + \"% Probability\")\n",
        "  #generate legend\n",
        "  legtitle=\"Prob \" + element + \" \" + tdir + str(thresh)\n",
        "  #for simplicity, we will append scatter plot artist to contour plot artists, then make one legend\n",
        "  artists,labels=probs.legend_elements()\n",
        "  artists.append(hits)\n",
        "  #note that we append to and use plabels, which are our custom labels.\n",
        "  cust_labels.append(\"Observed Event\")\n",
        "  legend2 = ax1.legend(artists,cust_labels,loc=\"best\",title=legtitle,fancybox=True)\n",
        "  #now, just formatting\n",
        "  for text in legend2.get_texts():\n",
        "    text.set_color(text_color)\n",
        "  title=legend2.get_title()\n",
        "  title.set_color(text_color)\n",
        "  ax1.add_artist(legend2)\n",
        "  ##add titles\n",
        "  fig.text(0.5,1.05,\"Obs and Probs: NBM 4.1 \" + element + \" \" + tdir + str(thresh),horizontalalignment='center', verticalalignment='bottom', weight='bold',fontsize=20,color=text_color)\n",
        "  fig.text(0.5,1.05,\"Valid ending at: \" + valid_title + \" | NBM Init: \" + nbm_init_title,horizontalalignment='center',verticalalignment='top', fontsize=16,color=text_color)\n",
        "  ##save figure\n",
        "  figname=\"obs_and_probs_v4.1_\"+region+\"_\"+element+\"_\"+str(thresh)+\"_\"+nbm_init_string+\"_\"+fig_valid_date+\".png\"\n",
        "  plt.savefig(figname,bbox_inches='tight',pad_inches=0.2,dpi='figure')\n",
        "  print(\"   >Done! Saved plot as \" + figname)\n",
        "\n",
        "#hitframe=obframe[obframe[\"Hit_1.0\"] == \"HIT\"]\n",
        "#hitscat=ax1.scatter(hitframe[\"lon\"].values,hitframe[\"lat\"].values,color=\"limegreen\",transform=proj)"
      ],
      "metadata": {
        "id": "4ppum_jM8jK7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "usgCFamy-5Yn"
      }
    }
  ]
}