{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StevenLevine-NOAA/NBM-Verif/blob/notebooks/MDL_Internal_NBM_Percentile_Context.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj77e3ZfajKO"
      },
      "source": [
        "**NBM Percentile in Context**\n",
        "A script to grab NBM percentile data, deterministic forecast and obs, and put the obs or deterministic forecast into the context of Probabilistic NBM space. This is based on code originally developed by Caleb Steele of Western Region.\n",
        "\n",
        "-*Steve Levine - NWS MDL/Statistical Modeling Division - 2-March-2023*\n",
        "<br/> <br/>\n",
        "Updates:\n",
        "> 28-Sept-2023: Corrected to indicate that some percentile fileds don't exist for HI and PR.\n",
        "\n",
        "> 20-Sept-2023: Included ability to work with HI/PR/AK domains (still no GU yet)\n",
        "\n",
        "> 14-Sep-2023: Updated snow plotting to work with 'unknown' name in snow total grib2 message\n",
        "\n",
        "> 10-Jul-2023: Corrected smoothling spline of percentiles so that now spline passes through relevant end points (set s = 0)\n",
        "\n",
        "> 20-June 2023: Added deterministic 24-hour snow forecasts, which include same caveats as 24-hour QPF forecasts (random bin assignment, etc.)\n",
        "\n",
        "> 4-Apr-2023: Added probabilistic (but not determinisitc) 24-hour snow forecasts based on NOHRSC analysis, added zero-padding for time strings where needed.  Set observed or deterministic qpf/snow to zero when value is < 0.005 inches.  Also added minor edits to plots (CWA's are now outlined in black).\n",
        "\n",
        "> 30-Mar-2023: Fixed bug where 0.0 qpf obs were being set to NaN.  Also, assigned such obs to a random percentile bin where percentile values was also 0.0.\n",
        "\n",
        "> 13-Mar-2023: Fixed bug with random extra obs showing up.  Also inserted exception for maximum winds, which do not exist in core/deterministic.  Fatal error will generate when attempting to work with core/deterministic max wind.\n",
        "\n",
        "> 2-Mar-2023: Added capability for maximum wind forecast.  Note that a determinsitic/core maximum wind forecast is not available.\n",
        "\n",
        ">>Previous development was from Caleb Steele\n",
        "\n",
        "> 12-Aug-2022: Pretty significant update that changed much of the underlying code. Files used are now grib files (instead of the geotiffs used before), which are larger and take longer to download and decode. The gribs include more percentiles, and a cubic spline is utilized vs linear interpolation now, so the NBM distribution is much better sampled. While all 99 percentiles are avaiable, trimmed it to use 13 (1st, 5th, 10th, 20th, 30th, etc.) to save some time. If you are more patient, you can dig into the code and swap out the lists (uncomment one, uncomment the other) that will use all 99, but it will take awhile. In my limited tests, it offers little improvement in the representation of the CDF/PDF. Also wrapped everything up behind a form and added light/dark mode option. Finally, added an option to generate a csv from the dataframe that is created (helpful to make sure it has done what you expect).\n",
        "\n",
        "> 7-Apr-2022: Added QPF, but still a lot of cleanup required. If you select Deterministic and QPF, it will really use the percentile mean. Will clean it up to use the deterministic, and add percentile mean as a separate option at some point.\n",
        "\n",
        "> 18-Feb-2022: Added option of adding CWA boundaries, cleaned up the directories (by actually making some), and more plot tweaks so they all look as expected.\n",
        "\n",
        "> 17-Feb-2022: fixed a hard coded reference that lead to the histogram always displaying the observation percentile distribution, even when deterministic was selected in regional plots.\n",
        "\n",
        "> 16-Feb-2022: added \"compare_to\" variable which lets you switch between comparing obs and NBM determinsitic to the ProbMaxT Percentiles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7inkOnFayzV"
      },
      "source": [
        "This first cell just imports everything we need. **NOTE: This cell will restart the notebook, which will prompt a crash popup in the lower left corner. This is safe to ignore and move on once the notebook comes back up** (you see RAM and Disk in the upper right corner)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initialize Notebook Part 1\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "id": "SAfL8UId5pUY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second cell should be run right after the first cell.  Note that these first two cells only need to be run once; then they will work for all other cases until you close or reset this notebook."
      ],
      "metadata": {
        "id": "wskTjNF1tCqC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwcXy34Yb3br",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Initialize Notebook Part 2\n",
        "!mamba install -q -c conda-forge cartopy contextily pyproj pyepsg pygrib netCDF4\n",
        "import numpy as np\n",
        "from scipy.interpolate import CubicSpline as cs, UnivariateSpline as us\n",
        "import pandas as pd\n",
        "from urllib.request import urlretrieve, urlopen\n",
        "import requests\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "from netCDF4 import Dataset\n",
        "import pygrib\n",
        "import pyproj\n",
        "from pyproj import Proj, transform\n",
        "import os, re, traceback\n",
        "\n",
        "import matplotlib\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "#from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.axes as maxes\n",
        "import matplotlib.patheffects as PathEffects\n",
        "from matplotlib.path import Path\n",
        "from matplotlib.textpath import TextToPath\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib.font_manager import FontProperties\n",
        "matplotlib.rcParams['font.sans-serif'] = 'Liberation Sans'\n",
        "matplotlib.rcParams['font.family'] = \"sans-serif\"\n",
        "from matplotlib.cm import get_cmap\n",
        "import seaborn as sns\n",
        "\n",
        "from cartopy import crs as ccrs, feature as cfeature\n",
        "from cartopy.io.shapereader import Reader\n",
        "from cartopy.feature import ShapelyFeature\n",
        "import contextily as cx\n",
        "import itertools\n",
        "\n",
        "import zipfile\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLyMTMsby8d8"
      },
      "outputs": [],
      "source": [
        "#@title Select Options and Download Obs { display-mode: \"form\" }\n",
        "element = \"maxt\" #@param [\"maxt\", \"mint\",\"qpf\",\"maxwind\",\"snow\"]\n",
        "valid_date = \"2023-11-20\" #@param {type:\"date\"}\n",
        "#@markdown QPF valid 24 hours ending time\n",
        "qpf_valid_time = 12 #@param {type:\"slider\", min:0, max:18, step:6}\n",
        "#@markdown Use StageIV at stations instead of obs?\n",
        "use_stageiv = True #@param {type:\"boolean\"}\n",
        "use_nohrsc = True #@param {type:\"boolean\"}\n",
        "#@markdown Pick NBM run time (note: add 1 hour for snow fcsts)\n",
        "nbm_init_date = \"2023-11-14\" #@param {type:\"date\"}\n",
        "nbm_init_hour = 0 #@param {type:\"slider\", min:0, max:18, step:6}\n",
        "#@markdown Where do you want to focus?\n",
        "region_selection = \"CWA\" #@param [\"WR\", \"SR\", \"CR\", \"ER\", \"AR\", \"CONUS\", \"CWA\"]\n",
        "#@markdown If CWA selected, which one? (i.e. \"SLC\" for Salt Lake City)\n",
        "cwa_id = \"AMA\" #@param {type:\"string\"}\n",
        "compare_to = \"obs\" #@param [\"obs\", \"deterministic\"]\n",
        "#@markdown Which obs?\n",
        "network_selection = \"NWS+RAWS+HADS\" #@param [\"NWS\", \"RAWS\", \"NWS+RAWS\", \"NWS+RAWS+HADS\", \"ALL\", \"CUSTOM\", \"LIST\"]\n",
        "#@markdown If Custom or List selected for network, enter comma separated network IDs (custom) or siteids (list)  WITH NO SPACES here. For help - https://developers.synopticdata.com/about/station-providers/\n",
        "network_input = \"\"#@param {type:\"string\"}\n",
        "cwa_outline = True #@param {type:\"boolean\"}\n",
        "county_outline = True #@param {type:\"boolean\"}\n",
        "#@markdown Do you want a CSV?\n",
        "export_csv = True #@param {type:\"boolean\"}\n",
        "#@markdown Light or dark theme plots?\n",
        "plot_style = \"dark\" #@param [\"light\", \"dark\"]\n",
        "\n",
        "if region_selection == \"CONUS\":\n",
        "  region_list = [\"WR\", \"CR\", \"SR\", \"ER\"]\n",
        "elif region_selection == \"CWA\":\n",
        "  region_list = [cwa_id]\n",
        "#elif region_selection == \"AR\":\n",
        "  #region_list=[\"AJK\",\"ARH\",\"AFC\"]\n",
        "else:\n",
        "  region_list = [region_selection]\n",
        "\n",
        "def cwa_list(input_region):\n",
        "  region_dict ={\"WR\":\"BYZ,BOI,LKN,EKA,FGZ,GGW,TFX,VEF,LOX,MFR,MTR,MSO,PDT,PSR,PIH,PQR,REV,STO,SLC,SGX,HNX,SEW,OTX,TWC\",\n",
        "              \"CR\":\"ABR,BIS,CYS,LOT,DVN,BOU,DMX,DTX,DDC,DLH,FGF,GLD,GJT,GRR,GRB,GID,IND,JKL,EAX,ARX,ILX,LMK,MQT,MKX,MPX,LBF,APX,IWX,OAX,PAH,PUB,UNR,RIW,FSD,SGF,LSX,TOP,ICT\",\n",
        "              \"ER\":\"ALY,LWX,BGM,BOX,BUF,BTV,CAR,CTP,RLX,CHS,ILN,CLE,CAE,GSP,MHX,OKX,PHI,PBZ,GYX,RAH,RNK,AKQ,ILM\",\n",
        "              \"SR\":\"ABQ,AMA,FFC,EWX,BMX,BRO,CRP,EPZ,FWD,HGX,HUN,JAN,JAX,KEY,MRX,LCH,LZK,LUB,MLB,MEG,MFL,MOB,MAF,OHX,LIX,OUN,SJT,SHV,TAE,TBW,TSA\",\n",
        "              \"AR\":\"AJK,ARH,AFC\"}\n",
        "  if (input_region in [\"WR\", \"CR\", \"SR\", \"ER\",\"AR\"]):\n",
        "    cwas_list = region_dict[input_region]\n",
        "  else:\n",
        "    cwas_list = input_region\n",
        "  return cwas_list\n",
        "\n",
        "nbm_init = datetime.strptime(nbm_init_date,'%Y-%m-%d') + timedelta(hours=int(nbm_init_hour))\n",
        "\n",
        "if element == \"maxt\":\n",
        "    nbm_core_valid_hour=\"00\"\n",
        "    nbm_qmd_valid_hour=\"06\"\n",
        "    valid_date_start = datetime.strptime(valid_date,'%Y-%m-%d')\n",
        "    valid_date_end = datetime.strptime(valid_date,'%Y-%m-%d') + timedelta(days=1)\n",
        "    obs_start_hour = \"1200\"\n",
        "    obs_end_hour = \"0600\"\n",
        "    ob_stat = \"maximum\"\n",
        "    valid_end_datetime = valid_date_end + timedelta(hours=(int(obs_end_hour)/100))\n",
        "    nbm_core_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_core_valid_hour))\n",
        "    nbm_qmd_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_qmd_valid_hour))\n",
        "    core_init = nbm_init + timedelta(hours = 7)\n",
        "    nbm_core_fhdelta = nbm_core_valid_end_datetime - core_init\n",
        "\n",
        "elif element == \"mint\":\n",
        "    nbm_core_valid_hour=\"12\"\n",
        "    nbm_qmd_valid_hour=\"18\"\n",
        "    valid_date_start = datetime.strptime(valid_date,'%Y-%m-%d')\n",
        "    valid_date_end = datetime.strptime(valid_date,'%Y-%m-%d')\n",
        "    obs_start_hour = \"0000\"\n",
        "    obs_end_hour = \"1800\"\n",
        "    ob_stat = \"minimum\"\n",
        "    valid_end_datetime = valid_date_end + timedelta(hours=(int(obs_end_hour)/100))\n",
        "    nbm_core_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_core_valid_hour))\n",
        "    nbm_qmd_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_qmd_valid_hour))\n",
        "    core_init = nbm_init + timedelta(hours = 7)\n",
        "    nbm_core_fhdelta = nbm_core_valid_end_datetime - core_init\n",
        "\n",
        "elif element == \"qpf\":\n",
        "    nbm_core_valid_hour = (str(qpf_valid_time)).zfill(2)\n",
        "    nbm_valid_hour = (str(qpf_valid_time)).zfill(2)\n",
        "    nbm_qmd_valid_hour=(str(qpf_valid_time)).zfill(2)\n",
        "    valid_date = datetime.strptime(valid_date,'%Y-%m-%d') + timedelta(hours=int(qpf_valid_time))\n",
        "    valid_date_start = valid_date - timedelta(hours=24)\n",
        "    valid_date_end = valid_date\n",
        "    obs_start_hour = (str(qpf_valid_time)).zfill(2)+\"00\"\n",
        "    obs_end_hour = (str(qpf_valid_time)).zfill(2)+\"00\"\n",
        "    ob_stat = \"total\"\n",
        "    valid_end_datetime = valid_date_end\n",
        "    core_init = nbm_init\n",
        "    nbm_core_valid_end_datetime = valid_date_end\n",
        "    nbm_qmd_valid_end_datetime = valid_date_end\n",
        "    nbm_core_fhdelta = nbm_core_valid_end_datetime - nbm_init\n",
        "\n",
        "elif element == \"maxwind\":\n",
        "    #nbm_core_valid_hour=\"06\"\n",
        "    #nbm_valid_hour=\"06\"\n",
        "    nbm_qmd_valid_hour=\"06\"\n",
        "    obs_start_hour=\"0600\"\n",
        "    obs_end_hour=\"0600\"\n",
        "    ob_stat=\"maximum\"\n",
        "    valid_date_start = datetime.strptime(valid_date,'%Y-%m-%d')\n",
        "    valid_date_end = datetime.strptime(valid_date,'%Y-%m-%d') + timedelta(days=1)\n",
        "    valid_end_datetime=valid_date_end + timedelta(hours=(int(obs_end_hour)/100))\n",
        "    core_init = nbm_init\n",
        "    nbm_core_valid_end_datetime = valid_date_end\n",
        "    nbm_qmd_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_qmd_valid_hour))\n",
        "    nbm_core_fhdelta = valid_end_datetime - nbm_init\n",
        "    if compare_to == \"deterministic\":\n",
        "      raise Exception(\"FATAL ERROR: You must compare to obs when looking at MAXWIND.  Deterministic data are not available!\")\n",
        "    #valid_date=date.strptime(valid_date,'') + timedelta(hours=)\n",
        "\n",
        "elif element == \"snow\":# or element == \"ice\":\n",
        "    nbm_core_valid_hour=(str(qpf_valid_time)).zfill(2)\n",
        "    nbm_qmd_valid_hour=(str(qpf_valid_time)).zfill(2)\n",
        "    obs_start_hour=(str(qpf_valid_time)).zfill(2)+\"00\"\n",
        "    obs_end_hour = (str(qpf_valid_time)).zfill(2)+\"00\"\n",
        "    valid_date = datetime.strptime(valid_date,'%Y-%m-%d') #+ timedelta(hours=int(qpf_valid_time))\n",
        "    valid_date_start = valid_date - timedelta(hours=24)\n",
        "    valid_date_end = valid_date\n",
        "    valid_end_datetime = valid_date_end + timedelta(hours=(int(obs_end_hour)/100))\n",
        "    ob_stat = \"total\"\n",
        "    core_init = nbm_init + timedelta(hours = 1)\n",
        "    nbm_qmd_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_qmd_valid_hour))\n",
        "    nbm_core_valid_end_datetime = nbm_qmd_valid_end_datetime\n",
        "    nbm_core_fhdelta = nbm_core_valid_end_datetime - core_init\n",
        "    #if compare_to == \"deterministic\":\n",
        "    #  raise Exception(\"FATAL ERROR: You must compare to obs when looking at snow/ice.  Determinsitic data are not avialable yet.\")\n",
        "\n",
        "current_datetime = datetime.now()\n",
        "\n",
        "nbm_core_forecasthour = nbm_core_fhdelta.total_seconds() / 3600.\n",
        "if element == \"snow\" or element == \"qpf\":\n",
        "  nbm_core_forecasthour_start = nbm_core_forecasthour - 24\n",
        "else:\n",
        "  nbm_core_forecasthour_start = nbm_core_forecasthour - 12\n",
        "nbm_qmd_fhdelta = nbm_qmd_valid_end_datetime - nbm_init\n",
        "nbm_qmd_forecasthour = nbm_qmd_fhdelta.total_seconds() / 3600.\n",
        "if element == \"qpf\" or element == \"maxwind\" or element == \"maxgust\" or element == \"snow\" or element == \"ice24\":\n",
        "  nbm_qmd_forecasthour_start = nbm_qmd_forecasthour - 24\n",
        "else:\n",
        "  nbm_qmd_forecasthour_start = nbm_qmd_forecasthour - 18\n",
        "\n",
        "synoptic_token = \"ea497aaa40464749b903ac1204fd8020\"\n",
        "statistics_api = \"https://api.synopticlabs.org/v2/stations/statistics?\"\n",
        "precipitation_api = \"https://api.synopticdata.com/v2/stations/precipitation?\"\n",
        "metadata_api = \"https://api.synopticdata.com/v2/stations/metadata?\"\n",
        "\n",
        "# Setup a dictionary for translating a form selection into a something we can pass to mesowest API\n",
        "network_dict = {\"NWS+RAWS+HADS\":\"&network=1,2,106\",\"NWS+RAWS\":\"&network=1,2\", \"NWS\":\"&network=1\", \"RAWS\": \"&network=2\", \"ALL\":\"\", \"CUSTOM\": \"&network=\"+network_input, \"LIST\": \"&stid=\"+network_input}\n",
        "network_string = network_dict[network_selection]\n",
        "\n",
        "if element == \"qpf\":\n",
        "  cmap = get_cmap('BrBG')\n",
        "  cmap.set_under(color='black')\n",
        "  cmap.set_over(color='yellow')\n",
        "elif element == \"snow\":\n",
        "  cmap = get_cmap('cool_r')\n",
        "  cmap.set_under(color='black')\n",
        "  cmap.set_over(color='yellow')\n",
        "else:\n",
        "  #cmap = 'Spectral'\n",
        "  cmap = get_cmap('bwr')\n",
        "  cmap.set_under(color='yellow')\n",
        "  cmap.set_over(color='black')\n",
        "if use_stageiv and element==\"qpf\":\n",
        "  points_str = f'Stage IV @ {network_selection}'\n",
        "else:\n",
        "  points_str = network_selection\n",
        "\n",
        "if plot_style==\"light\":\n",
        "  background_color = '#f7f7f7'\n",
        "  text_color = '#121212'\n",
        "  map_land_color = '#FAFAF8'\n",
        "  map_water_color = '#D4DBDD'\n",
        "  map_border_color = 'grey'\n",
        "elif plot_style==\"dark\":\n",
        "  background_color = '#272727'\n",
        "  text_color = 'white'\n",
        "  map_land_color = '#414143'\n",
        "  map_water_color = '#272727'\n",
        "  #map_border_color = '#3B3B3D'\n",
        "  map_border_color = 'white'\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "# Reusable functions section                                                                                           #\n",
        "########################################################################################################################\n",
        "\n",
        "def project3(lon, lat, prj):\n",
        "  lon = float(lon)\n",
        "  lat = float(lat)\n",
        "\n",
        "  outproj = prj\n",
        "  inproj = Proj(init='epsg:4326')\n",
        "  nbm_coords = transform(inproj, outproj, lon, lat)\n",
        "  coordX = nbm_coords[0]\n",
        "  coordY = nbm_coords[1]\n",
        "  #print(f'Lat: {lat}, Y: {coordY} | Lon: {lon}, X: {coordX}')\n",
        "  return(coordX, coordY)\n",
        "\n",
        "\n",
        "def ll_to_index(datalons, datalats, loclon, loclat):\n",
        "  abslat = np.abs(datalats-loclat)\n",
        "  abslon = np.abs(datalons-loclon)\n",
        "  c = np.maximum(abslon, abslat)\n",
        "  latlon_idx_flat = np.argmin(c)\n",
        "  latlon_idx = np.unravel_index(latlon_idx_flat, datalons.shape)\n",
        "  return(latlon_idx)\n",
        "\n",
        "\n",
        "def project_hrap(lon, lat, s4x, s4y):\n",
        "  lon = float(lon)\n",
        "  lat = float(lat)\n",
        "\n",
        "  globe = ccrs.Globe(semimajor_axis=6371200)\n",
        "  hrap_ccrs = proj = ccrs.Stereographic(central_latitude=90.0,\n",
        "                          central_longitude=255.0,\n",
        "                          true_scale_latitude=60.0, globe=globe)\n",
        "  latlon_ccrs = ccrs.PlateCarree()\n",
        "  hrap_coords = hrap_ccrs.transform_point(lon,lat,src_crs=latlon_ccrs)\n",
        "  hrap_idx = ll_to_index(s4x, s4y, hrap_coords[0], hrap_coords[1])\n",
        "\n",
        "  return hrap_idx\n",
        "\n",
        "def nohrsc_ll2ij(lon,lat,gridlons,gridlats):\n",
        "  #for a lat/lon grid\n",
        "  lon = float(lon)\n",
        "  lat = float(lat)\n",
        "  lonidx=(np.abs(lon-gridlons)).argmin()\n",
        "  latidx=(np.abs(lat-gridlats)).argmin()\n",
        "  return(latidx,lonidx)\n",
        "\n",
        "def get_stageiv():\n",
        "  siv_url = \"https://water.weather.gov/precip/downloads/\"+valid_date_end.strftime('%Y')+\"/\"+valid_date_end.strftime('%m')+\"/\"+valid_date_end.strftime('%d')+\"/nws_precip_1day_\"+valid_date_end.strftime('%Y%m%d')+\"_conus.nc\"\n",
        "  data = urlopen(siv_url).read()\n",
        "\n",
        "  nc = Dataset('data', memory=data)\n",
        "  #with Dataset(siv_file, 'r') as nc:\n",
        "  stageIV = nc.variables['observation']\n",
        "  s4x = nc.variables['x']\n",
        "  s4y = nc.variables['y']\n",
        "  return stageIV, s4x, s4y\n",
        "\n",
        "def get_nohrsc():\n",
        "  nohrsc_url = \"https://www.nohrsc.noaa.gov/snowfall_v2/data/\"+valid_date_end.strftime('%Y%m')+\"/sfav2_CONUS_24h_\"+valid_date_end.strftime('%Y%m%d%H')+\".nc\"\n",
        "  data = urlopen(nohrsc_url).read()\n",
        "\n",
        "  nc = Dataset('data',memory=data)\n",
        "  snow=np.asarray(nc.variables['Data']) #make lon by lat array (original lat by lon)\n",
        "  snowlat = np.asarray(nc.variables['lat'])\n",
        "  snowlon = np.asarray(nc.variables['lon'])\n",
        "  return snow,snowlon,snowlat\n",
        "\n",
        "def K_to_F(kelvin):\n",
        "  fahrenheit = 1.8*(kelvin-273)+32.\n",
        "  return fahrenheit\n",
        "\n",
        "def mps_to_kts(mps):\n",
        "  kts = mps * 1.94384\n",
        "  return kts\n",
        "\n",
        "def mm_to_in(millimeters):\n",
        "  inches = millimeters * 0.0393701\n",
        "  return inches\n",
        "\n",
        "def meters_to_in(meters):\n",
        "  inches = meters*39.3701\n",
        "  return inches\n",
        "\n",
        "def find_roots(x,y):\n",
        "  s = np.abs(np.diff(np.sign(y))).astype(bool)\n",
        "  return x[:-1][s] + np.diff(x)[s]/(np.abs(y[1:][s]/y[:-1][s])+1)\n",
        "\n",
        "\n",
        "def download_subset(remote_url, remote_file, local_filename):\n",
        "  print(\"   > Downloading a subset of NBM gribs\")\n",
        "  local_file = \"nbm/\"+local_filename\n",
        "  if \"qmd\" in remote_file:\n",
        "    if element == \"maxt\":\n",
        "      if (int(nbm_qmd_forecasthour_start) % 24 == 0) and (int(nbm_qmd_forecasthour) % 24 ==0):\n",
        "        search_string = f':TMP:2 m above ground:{str(int(int(nbm_qmd_forecasthour_start)/24))}-{str(int(int(nbm_qmd_forecasthour)/24))} day max fcst:'\n",
        "      else:\n",
        "        search_string = f':TMP:2 m above ground:{str(int(nbm_qmd_forecasthour_start))}-{str(int(nbm_qmd_forecasthour))} hour max fcst:'\n",
        "    elif element == \"mint\":\n",
        "      if (int(nbm_qmd_forecasthour_start) % 24 == 0) and (int(nbm_qmd_forecasthour) % 24 ==0):\n",
        "        search_string = f':TMP:2 m above ground:{str(int(int(nbm_qmd_forecasthour_start)/24))}-{str(int(int(nbm_qmd_forecasthour)/24))} day min fcst:'\n",
        "      else:\n",
        "        search_string = f':TMP:2 m above ground:{str(int(nbm_qmd_forecasthour_start))}-{str(int(nbm_qmd_forecasthour))} hour min fcst:'\n",
        "    elif element == \"qpf\":\n",
        "      if (int(nbm_qmd_forecasthour_start) % 24 == 0) and (int(nbm_qmd_forecasthour) % 24 ==0):\n",
        "        search_string = f':APCP:surface:{str(int(int(nbm_qmd_forecasthour_start)/24))}-{str(int(int(nbm_qmd_forecasthour)/24))} day acc fcst:'\n",
        "      else:\n",
        "        search_string = f':APCP:surface:{str(int(nbm_qmd_forecasthour_start))}-{str(int(nbm_qmd_forecasthour))} hour acc fcst:'\n",
        "    elif element == \"maxwind\":\n",
        "      if (int(nbm_qmd_forecasthour_start) % 24 == 0) and (int(nbm_qmd_forecasthour) % 24 == 0):\n",
        "        search_string = f':WIND:10 m above ground:{str(int(nbm_qmd_forecasthour_start/24))}-{str(int(nbm_qmd_forecasthour/24))} hour max fcst:'\n",
        "      else:\n",
        "        search_string = f':WIND:10 m above ground:{str(int(nbm_qmd_forecasthour_start))}-{str(int(nbm_qmd_forecasthour))} hour max fcst:'\n",
        "  elif \"core\" in remote_file:\n",
        "    if element == \"maxt\":\n",
        "      search_string = f':TMAX:2 m above ground:{str(int(nbm_core_forecasthour_start))}-{str(int(nbm_core_forecasthour))} hour max fcst:'\n",
        "    elif element == \"mint\":\n",
        "      search_string = f':TMIN:2 m above ground:{str(int(nbm_core_forecasthour_start))}-{str(int(nbm_core_forecasthour))} hour min fcst:'\n",
        "    elif element == \"snow\":\n",
        "      search_string = f':ASNOW:surface:{str(int(nbm_core_forecasthour_start))}-{str(int(nbm_core_forecasthour))} hour acc'\n",
        "  #print(\"Search string = \",search_string)\n",
        "  idx = remote_url+\".idx\"\n",
        "  #print(\"IDX file = \" + idx)\n",
        "  r = requests.get(idx)\n",
        "  if not r.ok:\n",
        "    print('     ❌ SORRY! Status Code:', r.status_code, r.reason)\n",
        "    print(f'      ❌ It does not look like the index file exists: {idx}')\n",
        "\n",
        "  lines = r.text.split('\\n')\n",
        "  expr = re.compile(search_string)\n",
        "  expr\n",
        "  byte_ranges = {}\n",
        "  for n, line in enumerate(lines, start=1):\n",
        "    # n is the line number (starting from 1) so that when we call for\n",
        "    # `lines[n]` it will give us the next line. (Clear as mud??)\n",
        "    # Use the compiled regular expression to search the line\n",
        "    #print(\">> Searching throgh this line: \" + line)\n",
        "    if expr.search(line):\n",
        "      # aka, if the line contains the string we are looking for...\n",
        "      # Get the beginning byte in the line we found\n",
        "      parts = line.split(':')\n",
        "      rangestart = int(parts[1])\n",
        "      # Get the beginning byte in the next line...\n",
        "      if n+1 < len(lines):\n",
        "        # ...if there is a next line\n",
        "        parts = lines[n].split(':')\n",
        "        rangeend = int(parts[1])\n",
        "      else:\n",
        "        # ...if there isn't a next line, then go to the end of the file.\n",
        "        rangeend = ''\n",
        "\n",
        "        # Store the byte-range string in our dictionary,\n",
        "        # and keep the line information too so we can refer back to it.\n",
        "      byte_ranges[f'{rangestart}-{rangeend}'] = line\n",
        "      #print(line)\n",
        "    #else:\n",
        "      #print(\">>>  Could not find search string!\")\n",
        "  #print(\">>  Number of items in byteRange:\" + str(len(byte_ranges)))\n",
        "  for i, (byteRange, line) in enumerate(byte_ranges.items()):\n",
        "\n",
        "    if i == 0:\n",
        "      # If we are working on the first item, overwrite the existing file.\n",
        "      curl = f'curl -s --range {byteRange} {remote_url} > {local_file}'\n",
        "      #print(\">>  Adding curl command: \" + curl)\n",
        "    else:\n",
        "      # If we are working on not the first item, append the existing file.\n",
        "      curl = f'curl -s --range {byteRange} {remote_url} >> {local_file}'\n",
        "      #print(\"Adding curl command: \" + curl)\n",
        "    #print('>>  Parsing line: ' + line)\n",
        "    try:\n",
        "      num, byte, date, var, level, forecast, _ = line.split(':')\n",
        "    except:\n",
        "      pass\n",
        "      #print(\">>>  Can't get num/byte/etc from this line, so skipping...\")\n",
        "\n",
        "    #print(f'  Downloading GRIB line [{num:>3}]: variable={var}, level={level}, forecast={forecast}')\n",
        "    #print(f'  Downloading GRIB line: variable={var}, level={level}, forecast={forecast}')\n",
        "    #print(\"Running the curl command...\")\n",
        "    os.system(curl)\n",
        "\n",
        "  if os.path.exists(local_file):\n",
        "    print(f'      ✅ Success! Searched for [{search_string}] and got [{len(byte_ranges)}] GRIB fields and saved as {local_file}')\n",
        "    return local_file\n",
        "  else:\n",
        "    print(print(f'      ❌ Unsuccessful! Searched for [{search_string}] and did not find anything!'))\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "# This section for downloading and processing obs                                                                      #\n",
        "########################################################################################################################\n",
        "print('Getting obs...')\n",
        "obs ={}\n",
        "for region in region_list:\n",
        "  if (valid_end_datetime <= current_datetime):\n",
        "    print(\"   > Grabbing obs for: \", region)\n",
        "    #print(\"List of CWAs: \", cwa_list(region) )\n",
        "    json_name = \"obs/Obs_\"+element+\"_\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour+\"_\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour+\"_\"+region+\".json\"\n",
        "    if os.path.exists(\"obs\"):\n",
        "      pass\n",
        "    else:\n",
        "      os.mkdir(\"obs\")\n",
        "\n",
        "    #if element != \"qpf\":\n",
        "    if element == \"mint\" or element == \"maxt\":\n",
        "      api_token = \"&token=\"+synoptic_token\n",
        "      station_query = \"&cwa=\"+cwa_list(region)\n",
        "      vars_query = \"&vars=air_temp\"\n",
        "      start_query = \"&start=\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour\n",
        "      end_query = \"&end=\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour\n",
        "      stat_type = \"&type=\"+ob_stat\n",
        "      network_query = network_string\n",
        "      api_extras = \"&units=temp%7Cf&within=1440&status=active\"\n",
        "      obs_url = statistics_api + api_token + station_query + vars_query + start_query + end_query + stat_type + network_query + api_extras\n",
        "    elif element == \"maxwind\":\n",
        "      api_token = \"&token=\"+synoptic_token\n",
        "      station_query = \"&cwa=\"+cwa_list(region)\n",
        "      vars_query = \"&vars=wind_speed\"\n",
        "      start_query = \"&start=\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour\n",
        "      end_query = \"&end=\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour\n",
        "      stat_type = \"&type=\"+ob_stat\n",
        "      network_query = network_string\n",
        "      #api_extras =\n",
        "      obs_url = statistics_api + api_token + station_query + vars_query + start_query + end_query + stat_type + network_query\n",
        "    elif element == \"qpf\":\n",
        "      if use_stageiv:\n",
        "        api_token = \"&token=\"+synoptic_token\n",
        "        station_query = \"&cwa=\"+cwa_list(region)\n",
        "        api_extras = \"&fields=status,latitude,longitude,name,elevation\"\n",
        "        network_query = network_string\n",
        "        obs_url = metadata_api + api_token + station_query + network_query + api_extras\n",
        "        stageIV, s4xs, s4ys = get_stageiv()\n",
        "        s4xs, s4ys = np.meshgrid(s4xs, s4ys)\n",
        "      else:\n",
        "        api_token = \"&token=\"+synoptic_token\n",
        "        station_query = \"&cwa=\"+cwa_list(region)\n",
        "        api_extras = \"&fields=status,latitude,longitude,name,elevation&obtimezone=utc\"\n",
        "        network_query = network_string\n",
        "        vars_query = \"&pmode=totals\"\n",
        "        units_query = \"&units=precip|in\"\n",
        "        start_query = \"&start=\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour\n",
        "        end_query = \"&end=\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour\n",
        "        obs_url = precipitation_api + api_token + station_query + network_query + vars_query + start_query + end_query + units_query + api_extras\n",
        "    elif element == \"snow\":\n",
        "      if use_nohrsc:\n",
        "        api_token = \"&token=\"+synoptic_token\n",
        "        station_query = \"&cwa=\"+cwa_list(region)\n",
        "        api_extras = \"&fields=status,latitude,longitude,name,elevation\"\n",
        "        network_query = network_string\n",
        "        obs_url = metadata_api + api_token + station_query + network_query + api_extras\n",
        "        snow,snowlon,snowlat = get_nohrsc()\n",
        "        snowlons,snowlats = np.meshgrid(snowlon,snowlat)\n",
        "      else:\n",
        "        api_token = \"&token=\"+synoptic_token\n",
        "        station_query = \"&cwa=\"+cwa_list(region)\n",
        "        api_extras = \"&fields=status,latitude,longitude,name,elevation&obtimezone=utc\"\n",
        "        network_query = network_string\n",
        "        vars_query = \"&pmode=totals\"\n",
        "        units_query = \"&units=precip|in\"\n",
        "        start_query = \"&start=\"+valid_date_start.strftime('%Y%m%d')+obs_start_hour\n",
        "        end_query = \"&end=\"+valid_date_end.strftime('%Y%m%d')+obs_end_hour\n",
        "        obs_url = precipitation_api + api_token + station_query + network_query + vars_query + start_query + end_query + units_query + api_extras\n",
        "    print(\"Obs url: \" + obs_url)\n",
        "    if os.path.exists(json_name):\n",
        "      print (\"Skipping download since JSON file already exists\")\n",
        "      pass\n",
        "    else:\n",
        "      urlretrieve(obs_url, json_name)\n",
        "\n",
        "    if os.path.exists(json_name):\n",
        "        with open(json_name) as json_file:\n",
        "            obs_json = json.load(json_file)\n",
        "            print (\"Loaded Obs JSON file line 343: \" + json_name)\n",
        "            obs_lats = []\n",
        "            obs_lons = []\n",
        "            obs_value = []\n",
        "            obs_elev = []\n",
        "            obs_stid = []\n",
        "            obs_name = []\n",
        "            for stn in obs_json[\"STATION\"]:\n",
        "                # print(stn.encode('utf-8'))\n",
        "                if stn[\"STID\"] is None:\n",
        "                  stid = \"N0N3\"\n",
        "                else:\n",
        "                  stid = stn[\"STID\"]\n",
        "                #print(f'Processing {region} station {stid}')\n",
        "                name = stn[\"NAME\"]\n",
        "                if stn[\"ELEVATION\"] and stn[\"ELEVATION\"] is not None:\n",
        "                  elev = stn[\"ELEVATION\"]\n",
        "                else:\n",
        "                  elev = -999\n",
        "                lat = stn[\"LATITUDE\"]\n",
        "                lon = stn[\"LONGITUDE\"]\n",
        "                if float(lon) > -50:\n",
        "                  continue #bug fix to deal with errant synoptic labs obs in the file\n",
        "                if element == \"mint\" or element==\"maxt\":\n",
        "                  if 'air_temp_set_1' in stn['STATISTICS'] and stn['STATISTICS']['air_temp_set_1']:\n",
        "                    if ob_stat in stn['STATISTICS']['air_temp_set_1']: # and float(stn[\"LATITUDE\"]) != 0. and float(stn[\"LONGITUDE\"]) != 0.:\n",
        "                      stat = stn['STATISTICS']['air_temp_set_1'][ob_stat]\n",
        "                      obs_stid.append(str(stid))\n",
        "                      obs_name.append(str(name))\n",
        "                      obs_elev.append(float(elev))\n",
        "                      obs_lats.append(float(lat))\n",
        "                      obs_lons.append(float(lon))\n",
        "                      obs_value.append(float(stat))\n",
        "                elif element == \"maxwind\":\n",
        "                  if 'wind_speed_set_1' in stn['STATISTICS'] and stn['STATISTICS']['wind_speed_set_1']:\n",
        "                    if ob_stat in stn['STATISTICS']['wind_speed_set_1']: # and float(stn[\"LATITUDE\"]) != 0.:\n",
        "                      stat = stn['STATISTICS']['wind_speed_set_1'][ob_stat]\n",
        "                      obs_stid.append(str(stid))\n",
        "                      obs_name.append(str(name))\n",
        "                      obs_elev.append(float(elev))\n",
        "                      obs_lats.append(float(lat))\n",
        "                      obs_lons.append(float(lon))\n",
        "                      obs_value.append(mps_to_kts(float(stat)))\n",
        "                elif (element == \"qpf\"):\n",
        "                  if (stn[\"STATUS\"] == \"ACTIVE\"): # and float(stn[\"LATITUDE\"]) < 50.924 and float(stn[\"LATITUDE\"]) > 23.377 and float(stn[\"LONGITUDE\"]) > -125.650 and float(stn[\"LONGITUDE\"]) < -66.008:\n",
        "                    obs_stid.append(str(stid))\n",
        "                    obs_name.append(str(name))\n",
        "                    obs_elev.append(float(elev))\n",
        "                    obs_lats.append(float(lat))\n",
        "                    obs_lons.append(float(lon))\n",
        "                    if use_stageiv:\n",
        "                      coords = project_hrap(lon, lat, s4xs, s4ys)\n",
        "                      siv_value = float(stageIV[coords])\n",
        "                      if (siv_value >= 0.01):\n",
        "                        obs_value.append(siv_value)\n",
        "                      else:\n",
        "                        obs_value.append(0.0)\n",
        "                    else:\n",
        "                      if \"precipitation\" in stn[\"OBSERVATIONS\"]:\n",
        "                        if \"total\" in stn[\"OBSERVATIONS\"][\"precipitation\"][0]:\n",
        "                          ptotal = stn[\"OBSERVATIONS\"][\"precipitation\"][0][\"total\"]\n",
        "                          if ptotal >= 0.005:\n",
        "                            obs_value.append(ptotal)\n",
        "                          else:\n",
        "                            obs_value.append(0.0)\n",
        "                        else:\n",
        "                          obs_value.append(np.nan)\n",
        "                      else:\n",
        "                        obs_value.append(np.nan)\n",
        "                elif (element == \"snow\"):\n",
        "                  if stn[\"STATUS\"] == \"ACTIVE\": # and float(stn[\"LATITUDE\"]) < 50.924)and float(stn[\"LATITUDE\"]) > 23.377 and float(stn[\"LONGITUDE\"]) > -125.650 and float(stn[\"LONGITUDE\"]) < -66.008:\n",
        "                    obs_stid.append(str(stid))\n",
        "                    obs_name.append(str(name))\n",
        "                    obs_elev.append(float(elev))\n",
        "                    obs_lats.append(float(lat))\n",
        "                    obs_lons.append(float(lon))\n",
        "                    if use_nohrsc:\n",
        "                      coords = nohrsc_ll2ij(lon,lat,snowlon,snowlat)\n",
        "                      nohrsc_value = meters_to_in(float(snow[coords]))\n",
        "                      if nohrsc_value >= 0.005:\n",
        "                        obs_value.append(nohrsc_value)\n",
        "                      elif nohrsc_value < 0.0:\n",
        "                        obs_value.append(np.nan)\n",
        "                      else:\n",
        "                        obs_value.append(0.0)\n",
        "                    else:\n",
        "                      raise Exception(\"Still not able to process individual snow obs!\")\n",
        "            csv_name = \"obs_\"+element+\"_\"+region+\".csv\"\n",
        "            obs[region] = pd.DataFrame()\n",
        "            obs[region][\"stid\"] = obs_stid\n",
        "            obs[region][\"name\"] = obs_name\n",
        "            obs[region][\"elevation\"] = obs_elev\n",
        "            obs[region][\"lat\"] = obs_lats\n",
        "            obs[region][\"lon\"] = obs_lons\n",
        "            obs[region][\"ob_\"+element] = obs_value\n",
        "            obs[region].dropna(inplace=True)\n",
        "            obs[region].to_csv(csv_name)\n",
        "  else:\n",
        "    print(f'    > Valid Time in the future. Grabbing obs points only for: {region}')\n",
        "    json_name = \"obs/ObsPoints_\"+region+\"_wcoss.json\"\n",
        "    if os.path.exists(json_name):\n",
        "      pass\n",
        "    else:\n",
        "      if os.path.exists(\"obs\"):\n",
        "        pass\n",
        "      else:\n",
        "        os.mkdir(\"obs\")\n",
        "      obs_url = \"https://api.synopticdata.com/v2/stations/metadata?&token=\"+synoptic_token+\"&cwa=\"+cwa_list(region)+\"&fields=status,latitude,longitude,name,elevation\"+network_string\n",
        "      urlretrieve(obs_url, json_name)\n",
        "    if os.path.exists(json_name):\n",
        "      with open(json_name) as json_file:\n",
        "          obs_json = json.load(json_file)\n",
        "          print(\"Loaded Obs JSON file line 427!\")\n",
        "          obs_lats = []\n",
        "          obs_lons = []\n",
        "          obs_elev = []\n",
        "          obs_stid = []\n",
        "          obs_name = []\n",
        "          for stn in obs_json[\"STATION\"]:\n",
        "            # print(stn.encode('utf-8'))\n",
        "            if stn[\"STID\"] is None:\n",
        "              stid = \"N0N3\"\n",
        "            else:\n",
        "              stid = stn[\"STID\"]\n",
        "            #print(f'Processing {region} station {stid}')\n",
        "            name = stn[\"NAME\"]\n",
        "            if stn[\"ELEVATION\"] and stn[\"ELEVATION\"] is not None:\n",
        "              elev = stn[\"ELEVATION\"]\n",
        "            else:\n",
        "              elev = -999\n",
        "            lat = stn[\"LATITUDE\"]\n",
        "            lon = stn[\"LONGITUDE\"]\n",
        "            if stn[\"STATUS\"] == \"ACTIVE\": # and float(stn[\"LATITUDE\"]) != 0. and float(stn[\"LONGITUDE\"]) != 0.:\n",
        "              obs_stid.append(str(stid))\n",
        "              obs_name.append(str(name))\n",
        "              obs_elev.append(float(elev))\n",
        "              obs_lats.append(float(lat))\n",
        "              obs_lons.append(float(lon))\n",
        "          obs[region] = pd.DataFrame()\n",
        "          obs[region][\"stid\"] = obs_stid\n",
        "          obs[region][\"name\"] = obs_name\n",
        "          obs[region][\"elevation\"] = obs_elev\n",
        "          obs[region][\"lat\"] = obs_lats\n",
        "          obs[region][\"lon\"] = obs_lons\n",
        "          obs[region][\"ob_\"+element] = -999\n",
        "          obs[region].dropna(inplace=True)\n",
        "          #obs[region].to_csv(csv_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0BRckH750J8",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Download Gribs and Interpolate\n",
        "########################################################################################################################\n",
        "# This section downloads and processes the NBM.                                                                        #\n",
        "########################################################################################################################\n",
        "if \"AR\" in region_list or \"AJK\" in region_list or \"ARH\" in region_list or \"AFC\" in region_list:\n",
        "  rg=\"ak\"\n",
        "elif \"HFO\" in region_list:\n",
        "  rg=\"hi\"\n",
        "elif \"SJU\" in region_list:\n",
        "  rg=\"pr\"\n",
        "else:\n",
        "  rg=\"co\"\n",
        "\n",
        "print('Getting and processing NBM...')\n",
        "nbm_init_filen = nbm_init.strftime('%Y%m%d') + \"_\" + nbm_init.strftime('%H')\n",
        "nbm_init_filen_core = core_init.strftime('%Y%m%d') + \"_\" + core_init.strftime('%H')\n",
        "nbm_url_base = \"https://noaa-nbm-grib2-pds.s3.amazonaws.com/blend.\"+nbm_init.strftime('%Y%m%d') \\\n",
        "            +\"/\"+nbm_init.strftime('%H')+\"/\"\n",
        "nbm_url_base_core = \"https://noaa-nbm-grib2-pds.s3.amazonaws.com/blend.\"+core_init.strftime('%Y%m%d') \\\n",
        "            +\"/\"+core_init.strftime('%H')+\"/\"\n",
        "temp_vars = [\"maxt\",\"mint\"]\n",
        "if (element == \"qpf\"):\n",
        "  detr_file = f'blend.t{int(nbm_init_hour):02}z.qmd.f{int(nbm_qmd_forecasthour):03}.{rg}.grib2'\n",
        "  detr_file_subset = f'blend.t{int(nbm_init_hour):02}z.qmd.{nbm_init_filen}{nbm_init_filen}f{int(nbm_qmd_forecasthour):03}.{rg}.{element}_subset.grib2'\n",
        "  detr_url = nbm_url_base+\"qmd/\"+detr_file\n",
        "\n",
        "elif any(te in element for te in temp_vars):\n",
        "  detr_file = f\"blend.t{int(core_init.strftime('%H')):02}z.core.f{int(nbm_core_forecasthour):03}.{rg}.grib2\"\n",
        "  detr_file_subset = f\"blend.t{int(core_init.strftime('%H')):02}z.core.{nbm_init_filen_core}f{int(nbm_core_forecasthour):03}.{rg}.{element}_subset.grib2\"\n",
        "  detr_url = nbm_url_base_core+\"core/\"+detr_file\n",
        "  #if rg in [\"pr\",\"hi\"]:\n",
        "  #  raise exception (\"Min/maxT Percentiles/Probabilities not available for HI/PR domains\")\n",
        "\n",
        "elif element == \"maxwind\" or element == \"maxgust\":\n",
        "  print(\"MAXWIND: No Core data available!\")\n",
        "  #core_init = nbm_init\n",
        "  #nbm_qmd_valid_end_datetime = valid_date_end + timedelta(hours=int(nbm_qmd_valid_hour))\n",
        "\n",
        "elif element == \"snow\":\n",
        "  #for snow, even for prob/perecntiles, we are dealing with a core file\n",
        "  detr_file=f\"blend.t{int(core_init.strftime('%H')):02}z.core.f{int(nbm_core_forecasthour):03}.{rg}.grib2\"\n",
        "  detr_file_subset = f\"blend.t{int(core_init.strftime('%H')):02}z.core.{nbm_init_filen_core}f{int(nbm_core_forecasthour):03}.{rg}.{element}_subset.grib2\"\n",
        "  detr_url = nbm_url_base_core+\"core/\"+detr_file\n",
        "  #raise Exception (\"Not ready to deal with snow deterministic yet!\")\n",
        "  #print(\"SNOW: No Core data avaialable!\")\n",
        "  #if rg in [\"pr\",\"hi\"]:\n",
        "  #  raise Exception (\"Snow Percentiles not available for PR or HI\")\n",
        "\n",
        "if element != \"maxwind\" and element != \"maxgust\": # and element != \"snow\":\n",
        "  if os.path.exists(\"nbm/\"+detr_file_subset):\n",
        "    print(\"   > NBM deterministic already exists\")\n",
        "  else:\n",
        "    print(\"   > Getting NBM deterministic\")\n",
        "    if os.path.exists(\"nbm\"):\n",
        "      pass\n",
        "    else:\n",
        "      os.mkdir(\"nbm\")\n",
        "    #urlretrieve(detr_url, \"nbm/\"+detr_file)\n",
        "    download_subset(detr_url, detr_file, detr_file_subset)\n",
        "  #print(detr_url)\n",
        "  nbmd = pygrib.open(\"nbm/\"+detr_file_subset)\n",
        "  if element == \"maxt\":\n",
        "    deterministic = nbmd.select(name=\"Maximum temperature\",lengthOfTimeRange=12, stepTypeInternal=\"max\")[0]\n",
        "    deterministic_array = K_to_F(deterministic.values)\n",
        "  elif element == \"mint\":\n",
        "    deterministic = nbmd.select(name=\"Minimum temperature\",lengthOfTimeRange=12, stepTypeInternal=\"min\")[0]\n",
        "    deterministic_array = K_to_F(deterministic.values)\n",
        "  elif element == \"qpf\":\n",
        "    deterministic = nbmd.select(name=\"Total Precipitation\",lengthOfTimeRange=24)[-1]\n",
        "    deterministic_array = mm_to_in(deterministic.values)\n",
        "  if element == \"snow\":\n",
        "    deterministic = nbmd.select(name=\"unknown\",lengthOfTimeRange=24)[7] #look at 8th record\n",
        "    deterministic_array = mm_to_in(deterministic.values)\n",
        "  nbmlats, nbmlons = deterministic.latlons()\n",
        "  nbmd.close()\n",
        "\n",
        "  for region in region_list:\n",
        "    print(\"     >> Extracting NBM deterministic\")\n",
        "    point_lats = obs[region][\"lat\"].values\n",
        "    point_lons = obs[region][\"lon\"].values\n",
        "    detr_values = []\n",
        "    nbm_fidx = []\n",
        "    for i in range(0, len(point_lats)):\n",
        "      coords = ll_to_index(nbmlons, nbmlats, point_lons[i], point_lats[i])\n",
        "      detr_value = deterministic_array[coords]\n",
        "      nbm_fidx.append(coords)\n",
        "      if (element == \"qpf\") and detr_value < 0.005: #set very light values to zero\n",
        "        detr_value = 0.0\n",
        "      detr_values.append(detr_value)\n",
        "    obs[region][\"NBM_fidx\"] = nbm_fidx\n",
        "    obs[region][\"NBM_D\"] = detr_values\n",
        "else:\n",
        "  #set up forecast dataframe for wind/gust, but make determinstic values nan\n",
        "  print(\"MAXWIND: Putting station locations into dataframe\")\n",
        "  #fcst[cwa]=pd.DataFrame()\n",
        "  point_lats = obs[region][\"lat\"].values\n",
        "  point_lons = obs[region][\"lon\"].values\n",
        "  nbm_fidx = []\n",
        "  nbmlats=None\n",
        "  nbmlons=None\n",
        "  stations = obs[region][\"stid\"]\n",
        "  detr_values=np.empty(np.shape(stations))\n",
        "  detr_values.fill(np.nan)\n",
        "  obs[region][\"NBM_D\"] = detr_values\n",
        "\n",
        "\n",
        "if element == \"snow\":\n",
        "  perc_list = [5,10,25,50,75,90,95]\n",
        "else:\n",
        "  perc_list = [1,5,10,20,30,40,50,60,70,80,90,95,99]\n",
        "#perc_list = range(1,100,1)\n",
        "perc_dict = {\"maxt\":\"maxt18p\", \"mint\":\"mint18p\", \"qpf\":\"qpf24p\", \"snow\":\"snow24p\"}\n",
        "if element == \"snow\":\n",
        "  perc_file=f\"blend.t{int(core_init.strftime('%H')):02}z.core.f{int(nbm_core_forecasthour):03}.{rg}.grib2\"\n",
        "  perc_url=nbm_url_base_core+\"core/\"+perc_file\n",
        "else:\n",
        "  perc_file = f'blend.t{int(nbm_init_hour):02}z.qmd.f{int(nbm_qmd_forecasthour):03}.{rg}.grib2'\n",
        "  perc_url = nbm_url_base+\"qmd/\"+perc_file\n",
        "perc_file_subset = f'blend.t{int(nbm_init_hour):02}z.qmd.{nbm_init_filen}{nbm_init_filen}f{int(nbm_qmd_forecasthour):03}.{rg}.{element}_subset.grib2'\n",
        "print(\"perc_url=\",perc_url)\n",
        "print(\"perc_file=\",perc_file)\n",
        "if os.path.exists(\"nbm\"):\n",
        "  pass\n",
        "else:\n",
        "  os.mkdir(\"nbm\")\n",
        "#Raise exception for missinge percentile data\n",
        "if rg in [\"pr\",\"hi\"] and element in [\"maxt\",\"mint\",\"maxwind\",\"snow\"]:\n",
        "  raise Exception (\"FATAL ERROR: Percentiles for \" + element + \"do not exist for PR or HI\")\n",
        "if os.path.exists(\"nbm/\"+perc_file_subset):\n",
        "  print(\"   > NBM probabilistic already exists\")\n",
        "else:\n",
        "  #urlretrieve(perc_url, \"nbm/\"+perc_file)\n",
        "  print(\"   > Getting NBM probabilistic\")\n",
        "  download_subset(perc_url, perc_file, perc_file_subset)\n",
        "\n",
        "nbmperc = pygrib.open(\"nbm/\"+perc_file_subset)\n",
        "print('   > Extracting NBM Probabilistic')\n",
        "for perc in perc_list:\n",
        "  print(f'     >> Extracting NBM P{int(perc):01}')\n",
        "  perc_name = \"NBM_P\"+str(perc)\n",
        "  if element == \"maxt\":\n",
        "    percdata = K_to_F(nbmperc.select(name=\"Maximum temperature at 2 metres since previous post-processing\", stepTypeInternal=\"max\", percentileValue=perc)[0].values)\n",
        "  elif element == \"mint\":\n",
        "    percdata = K_to_F(nbmperc.select(name=\"Maximum temperature at 2 metres since previous post-processing\", stepTypeInternal=\"min\", percentileValue=perc)[0].values)\n",
        "  elif element == \"qpf\":\n",
        "    percdata = mm_to_in(nbmperc.select(name=\"Total Precipitation\",lengthOfTimeRange=24, percentileValue=perc)[0].values)\n",
        "  elif element == \"maxwind\":\n",
        "    percinv = nbmperc.select(name=\"10 metre wind speed\",lengthOfTimeRange=24,stepTypeInternal=\"max\",percentileValue=perc)[0]\n",
        "    percdata = mps_to_kts(percinv.values)\n",
        "  elif element == \"snow\":\n",
        "    percinv = nbmperc.select(name=\"unknown\",lengthOfTimeRange=24,percentileValue=perc)[0]\n",
        "    percdata=meters_to_in(percinv.values)\n",
        "  if nbmlats is None:\n",
        "    nbmlats,nbmlons=percinv.latlons()\n",
        "    for i in range(0,len(point_lats)):\n",
        "      coords = ll_to_index(nbmlons,nbmlats,point_lons[i],point_lats[i])\n",
        "      nbm_fidx.append(coords)\n",
        "    obs[region][\"NBM_fidx\"] = nbm_fidx\n",
        "  for region in region_list:\n",
        "    nbm_coords = obs[region][\"NBM_fidx\"].values\n",
        "    perc_values = []\n",
        "    for i in range(0, len(nbm_coords)):\n",
        "      perc_value = percdata[nbm_coords[i]]\n",
        "      perc_values.append(perc_value)\n",
        "    obs[region][perc_name] = perc_values\n",
        "nbmperc.close()\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "# This section creates a distribution curve at each site, and interpolates ob and deterministic to percentile space    #\n",
        "########################################################################################################################\n",
        "print('Creating point distribution curves and interpolating...')\n",
        "for region in region_list:\n",
        "  if element == \"snow\":\n",
        "    perc_start = obs[region].columns.get_loc(\"NBM_P5\")\n",
        "    perc_end = obs[region].columns.get_loc(\"NBM_P95\")\n",
        "    all_percs = obs[region].iloc[:, perc_start:perc_end+1].values\n",
        "  else:\n",
        "    perc_start = obs[region].columns.get_loc(\"NBM_P1\")\n",
        "    perc_end = obs[region].columns.get_loc(\"NBM_P99\")\n",
        "    all_percs = obs[region].iloc[:, perc_start:perc_end+1].values\n",
        "  var_string = \"ob_\"+element\n",
        "  all_obs = obs[region][[var_string]].values\n",
        "  all_nbmd = obs[region][['NBM_D']].values\n",
        "  obs_percs = []\n",
        "  nbmd_percs = []\n",
        "  for i in range(0,len(all_obs)):\n",
        "    udf = us(perc_list, all_percs[i,:], bbox=[0,100], ext=0, s=0)\n",
        "    if np.isnan(all_obs[i]):\n",
        "      ob_perc = np.nan\n",
        "    elif all_obs[i] == 0.0 and (element == \"qpf\" or element == \"snow\"):\n",
        "        zo=len(perc_list)-np.count_nonzero(all_percs[i,:])\n",
        "        if zo == 0:\n",
        "          ob_perc = -10\n",
        "        elif zo == 1:\n",
        "          ob_perc = 10\n",
        "        else:\n",
        "          ob_perc = np.random.randint(low=1,high=perc_list[zo-1])\n",
        "    elif all_obs[i] < udf(0):\n",
        "      ob_perc = -10\n",
        "    elif all_obs[i] > udf(100):\n",
        "      ob_perc = 110\n",
        "    else:\n",
        "      ob_perc = find_roots(np.arange(0,101,1), udf(np.arange(0,101,1)) - all_obs[i])\n",
        "      ob_perc = ob_perc[0].round(1)\n",
        "\n",
        "    if np.isnan(all_nbmd[i]):\n",
        "      nbm_perc = np.nan\n",
        "    elif all_nbmd[i] == 0.0 and (element == \"qpf\" or element == \"snow\"):\n",
        "      zn=len(perc_list)-np.count_nonzero(all_percs[i,:])\n",
        "      if zn == 0:\n",
        "        nbm_perc = -10\n",
        "      elif zn == 1:\n",
        "        nbm_perc = 10\n",
        "      else:\n",
        "        nbm_perc = np.random.randint(low=1,high=perc_list[zo-1])\n",
        "    elif all_nbmd[i] < udf(0):\n",
        "      nbm_perc = -10\n",
        "    elif all_nbmd[i] > udf(100):\n",
        "      nbm_perc = 110\n",
        "    else:\n",
        "      nbm_perc = find_roots(np.arange(0,101,1), udf(np.arange(0,101,1)) - all_nbmd[i])\n",
        "      nbm_perc = nbm_perc[0].round(1)\n",
        "\n",
        "    if np.isnan(ob_perc):\n",
        "      obs_percs.append(ob_perc)\n",
        "    else:\n",
        "      obs_percs.append(int(ob_perc))\n",
        "    if np.isnan(nbm_perc):\n",
        "      nbmd_percs.append(nbm_perc)\n",
        "    else:\n",
        "      nbmd_percs.append(int(nbm_perc))\n",
        "  obs[region][\"ob_perc\"] = obs_percs\n",
        "  obs[region][\"NBMd_perc\"] = nbmd_percs\n",
        "  if export_csv:\n",
        "    csv_name = \"obs_and_percs_\"+element+\"_\"+nbm_init.strftime('%Y%m%d')+\"_\"+valid_end_datetime.strftime('%Y%m%d')+\"_\"+region+\".csv\"\n",
        "    obs[region].to_csv(csv_name)\n",
        "    print(f'   > Created and saved {csv_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aC8a2EWZ5j_s",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Generate Plot\n",
        "########################################################################################################################\n",
        "# Finally, this section makes our plot.                                                                                #\n",
        "########################################################################################################################\n",
        "print(\"Making plot (almost done!)...\")\n",
        "if compare_to ==\"obs\":\n",
        "  compare_var = \"ob_perc\"\n",
        "  compare_element = \"Obs\"\n",
        "elif compare_to == \"deterministic\":\n",
        "  compare_var = \"NBMd_perc\"\n",
        "  if element ==\"qpf\":\n",
        "    compare_element = \"pMean\"\n",
        "  else:\n",
        "    compare_element = \"Detr\"\n",
        "\n",
        "title_dict = {\"maxt\":[\"Max T\",\"PMaxT\"],\"mint\":[\"Min T\",\"PMinT\"], \"qpf\":[\"QPF\",\"PQPF\"], \"maxwind\":[\"Max Wind\",\"Prob Max Wind\"], \"snow\":[\"Snow Acc\", \"Prob Snow Acc\"]}\n",
        "matplotlib.rc('axes',facecolor=background_color, edgecolor=text_color)\n",
        "if (element == \"qpf\" or element == \"snow\"):\n",
        "  valid_datetime = valid_date\n",
        "  fig_valid_date = nbm_qmd_valid_end_datetime.strftime('%Y%m%d_%HZ')\n",
        "  valid_title = nbm_qmd_valid_end_datetime.strftime('%HZ %a %m-%d-%Y')\n",
        "else:\n",
        "  valid_datetime = datetime.strptime(valid_date,'%Y-%m-%d')\n",
        "  fig_valid_date = valid_datetime.strftime('%Y%m%d')\n",
        "  valid_title = valid_datetime.strftime('%a %m-%d-%Y')\n",
        "if (element == \"snow\"):\n",
        "  nbm_init_title = core_init.strftime('%HZ %m-%d-%Y')\n",
        "else:\n",
        "  nbm_init_title = nbm_init.strftime('%HZ %m-%d-%Y')\n",
        "\n",
        "def flip(items, ncol):\n",
        "    return itertools.chain(*[items[i::ncol] for i in range(ncol)])\n",
        "\n",
        "if region_selection == \"CONUS\":\n",
        "  dataframeid = \"CONUS\"\n",
        "  #set up multipanel plot\n",
        "  west =-125.650\n",
        "  south = 23.377\n",
        "  east = -66.008\n",
        "  north = 50.924\n",
        "  width_ratios = [7,3,3,3]\n",
        "  lloc = \"lower right\"\n",
        "  fig = plt.figure(constrained_layout=True, figsize=(16,9), facecolor=background_color, frameon=True, dpi=150)\n",
        "  grid = fig.add_gridspec(4,4, width_ratios=width_ratios, hspace=0.2, wspace=0.2, left=0.1, right=0.9)\n",
        "  fig.text(0.30, 0.885,f'{region_selection} {title_dict[element][0]} {compare_element} in NBM {title_dict[element][1]} Percentile Space',horizontalalignment='center',weight='bold',fontsize=25,color=text_color)\n",
        "  fig.text(0.30, 0.855,f'Valid: {valid_title}  |  NBM Init: {nbm_init_title}  |  Points: {points_str}',horizontalalignment='center',fontsize=16,color=text_color)\n",
        "\n",
        "  ax1 = fig.add_subplot(grid[:,:-2], projection=ccrs.Mercator(globe=None))\n",
        "  ax2 = fig.add_subplot(grid[0,2])\n",
        "  ax3 = fig.add_subplot(grid[0,3])\n",
        "  ax4 = fig.add_subplot(grid[1,2])\n",
        "  ax5 = fig.add_subplot(grid[1,3])\n",
        "  ax6 = fig.add_subplot(grid[2:,2:])\n",
        "\n",
        "  conus_df = pd.concat([obs[\"WR\"], obs[\"CR\"], obs[\"ER\"],obs[\"SR\"]])\n",
        "  lats = conus_df[\"lat\"].values\n",
        "  lons = conus_df[\"lon\"].values\n",
        "  point_data = conus_df[compare_var].values\n",
        "  mean = conus_df[compare_var].mean()\n",
        "  median = conus_df[compare_var].median()\n",
        "  mode = conus_df[compare_var].mode().values[0]\n",
        "\n",
        "  proj = ccrs.PlateCarree()\n",
        "\n",
        "  ax1.set_anchor('S')\n",
        "  ax1.set_extent([west, east, south, north], crs=proj)\n",
        "  #ax1.add_feature(cfeature.LAND, edgecolor='none', facecolor='#414143', zorder=-1)\n",
        "  ax1.add_feature(cfeature.OCEAN, edgecolor='none', facecolor=map_water_color, zorder=-2)\n",
        "  ax1.add_feature(cfeature.NaturalEarthFeature('physical', 'land', '50m', edgecolor='none', facecolor=map_land_color, zorder=-1))\n",
        "  #ax1.add_feature(cfeature.LAKES, edgecolor='none', facecolor='#272727', zorder=0)\n",
        "  ax1.add_feature(cfeature.NaturalEarthFeature('physical', 'lakes', '10m', edgecolor='none', facecolor=map_water_color, zorder=0))\n",
        "  ax1.add_feature(cfeature.BORDERS, edgecolor=map_border_color, facecolor='none', linewidth=2, zorder=1)\n",
        "  #ax1.add_feature(cfeature.NaturalEarthFeature('cultural', 'countries', '50m', edgecolor=map_border_color, facecolor='none', linewidth=2, zorder=2))\n",
        "  ax1.add_feature(cfeature.NaturalEarthFeature('cultural', 'admin_1_states_provinces_lines', '50m', edgecolor=map_border_color, facecolor='none', linewidth=1, zorder=2))\n",
        "  #cx.add_basemap(ax1, source='https://server.arcgisonline.com/ArcGIS/rest/services/Canvas/World_Dark_Gray_Base/MapServer/tile/{z}/{y}/{x}', attribution=False)\n",
        "  scatter = ax1.scatter(lons, lats, c= point_data, cmap=cmap, s=45, transform=proj, vmin=0.0, vmax=100.0)\n",
        "  #handles, labels = scatter.legend_elements(num=10)\n",
        "  #legend1 = ax1.legend(flip(handles, 6), flip(labels, 6), ncol=6,loc=lloc, title=f'{compare_element} in NBM Percentile Space', fancybox=True)\n",
        "  numcols=abs(np.amax(point_data) - np.amin(point_data))//10\n",
        "  legend1 = ax1.legend(*scatter.legend_elements(num=numcols), loc=lloc, title=f'{compare_element} \\n Rank', fancybox=True)\n",
        "  plt.setp(legend1.get_title(), multialignment='center', color=text_color)\n",
        "  for text in legend1.get_texts():\n",
        "    text.set_color(text_color)\n",
        "  ax1.add_artist(legend1)\n",
        "  ax1.add_feature(cfeature.NaturalEarthFeature(\n",
        "    'cultural', 'admin_1_states_provinces_lines', '110m',\n",
        "    edgecolor='gray', facecolor='none'))\n",
        "  if cwa_outline:\n",
        "    try:\n",
        "      if os.path.exists(\"shp/w_22mr22.shp\"):\n",
        "        pass\n",
        "      else:\n",
        "        cwa_url = \"https://www.weather.gov/source/gis/Shapefiles/WSOM/w_22mr22.zip\"\n",
        "        os.mkdir(\"shp\")\n",
        "        urlretrieve(cwa_url, \"shp/nws_cwa_outlines.zip\")\n",
        "        #!unzip shp/nws_cwa_outlines.zip -d shp\n",
        "        with zipfile.ZipFile(\"shp/nws_cwa_outlines.zip\", 'r') as zip_ref:\n",
        "          zip_ref.extractall(\"shp\")\n",
        "      cwa_feature = ShapelyFeature(Reader(\"shp/w_22mr22.shp\").geometries(),ccrs.PlateCarree(), edgecolor='grey', facecolor='none', linewidth=0.5, linestyle=':', zorder=3)\n",
        "      ax1.add_feature(cwa_feature)\n",
        "    except:\n",
        "      print(\"   > Aw shucks, no CWA boundaries for you. Sorry bout that.\")\n",
        "  if county_outline:\n",
        "    try:\n",
        "      if os.path.exists(\"shp/c_08mr23.zip\"):\n",
        "        pass\n",
        "      else:\n",
        "        county_url = \"https://www.weather.gov/source/gis/Shapefiles/County/c_08mr23.zip\"\n",
        "        os.mkdir(\"shp\")\n",
        "        urlretrieve(county_url, \"shp/counties.zip\")\n",
        "        with zipfile.ZipFile(\"shp/counties.zip\",'r') as zip_ref:\n",
        "          zip_ref.extractall(\"shp\")\n",
        "      cty_feature = ShapelyFeature(Reader(\"shp/c_08mr23.shp\").geometries().ccrs.PlateCarree(), edgecolor='white',facecolor='none',linewidth=1.0,linestyle=\"--\",zorder=3)\n",
        "      ax1.add_feature(cty_feature)\n",
        "    except:\n",
        "      print(\"   > Cannot plot county boundaries.\")\n",
        "\n",
        "  mean_wr = obs[\"WR\"][compare_var].mean()\n",
        "  median_wr = obs[\"WR\"][compare_var].median()\n",
        "  #mode_wr = obs[\"WR\"][compare_var].mode().values[0]\n",
        "  ax2.set_anchor('N')\n",
        "  sns.histplot(data=obs[\"WR\"], x=compare_var, ax=ax2, kde=True, bins=range(0,110,10),color='steelblue',edgecolor='lightgrey')\n",
        "  ax2.set_xlabel(\"Western Region\", color=text_color, fontsize=12)\n",
        "  ax2.axvline(mean_wr, color='salmon', linestyle='--', label=\"Mean\")\n",
        "  ax2.axvline(median_wr, color='mediumaquamarine', linestyle='-', label=\"Median\")\n",
        "  #ax2.axvline(mode_wr, color='lightskyblue', linestyle='-', label=\"Mode\")\n",
        "  ax2.grid(False)\n",
        "  for tick in ax2.get_xticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  for tick in ax2.get_yticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  ax2.tick_params(axis='y',labelsize=8, color=text_color)\n",
        "  legend2 = ax2.legend()\n",
        "  for text in legend2.get_texts():\n",
        "    text.set_color(text_color)\n",
        "  ax2.set(ylabel=None)\n",
        "\n",
        "  mean_cr = obs[\"CR\"][compare_var].mean()\n",
        "  median_cr = obs[\"CR\"][compare_var].median()\n",
        "  #mode_cr = obs[\"CR\"][compare_var].mode().values[0]\n",
        "  ax3.set_anchor('N')\n",
        "  sns.histplot(data=obs[\"CR\"], x=compare_var, ax=ax3, kde=True, bins=range(0,110,10),color='steelblue',edgecolor='lightgrey')\n",
        "  ax3.set_xlabel(\"Central Region\", color=text_color, fontsize=12)\n",
        "  ax3.axvline(mean_cr, color='salmon', linestyle='--', label=\"Mean\")\n",
        "  ax3.axvline(median_cr, color='mediumaquamarine', linestyle='-', label=\"Median\")\n",
        "  #ax3.axvline(mode_cr, color='lightskyblue', linestyle='-', label=\"Mode\")\n",
        "  ax3.grid(False)\n",
        "  for tick in ax3.get_xticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  for tick in ax3.get_yticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  ax3.tick_params(axis='y',labelsize=8, color=text_color)\n",
        "  legend3 = ax3.legend()\n",
        "  for text in legend3.get_texts():\n",
        "    text.set_color(text_color)\n",
        "  ax3.set(ylabel=None)\n",
        "\n",
        "\n",
        "  mean_er = obs[\"ER\"][compare_var].mean()\n",
        "  median_er = obs[\"ER\"][compare_var].median()\n",
        "  #mode_er = obs[\"ER\"][compare_var].mode().values[0]\n",
        "  ax4.set_anchor('N')\n",
        "  sns.histplot(data=obs[\"ER\"], x=compare_var, ax=ax4, kde=True, bins=range(0,110,10),color='steelblue',edgecolor='lightgrey')\n",
        "  ax4.set_xlabel(\"Eastern Region\", color=text_color, fontsize=12)\n",
        "  ax4.axvline(mean_er, color='salmon', linestyle='--', label=\"Mean\")\n",
        "  ax4.axvline(median_er, color='mediumaquamarine', linestyle='-', label=\"Median\")\n",
        "  #ax4.axvline(mode_er, color='lightskyblue', linestyle='-', label=\"Mode\")\n",
        "  ax4.grid(False)\n",
        "  for tick in ax4.get_xticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  for tick in ax4.get_yticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  ax4.tick_params(axis='y',labelsize=8, color=text_color)\n",
        "  legend4 = ax4.legend()\n",
        "  for text in legend4.get_texts():\n",
        "    text.set_color(text_color)\n",
        "  ax4.set(ylabel=None)\n",
        "\n",
        "\n",
        "  mean_sr = obs[\"SR\"][compare_var].mean()\n",
        "  median_sr = obs[\"SR\"][compare_var].median()\n",
        "  #mode_sr = obs[\"SR\"][compare_var].mode().values[0]\n",
        "  ax5.set_anchor('N')\n",
        "  sns.histplot(data=obs[\"SR\"], x=compare_var, ax=ax5, kde=True, bins=range(0,110,10),color='steelblue',edgecolor='lightgrey')\n",
        "  ax5.set_xlabel(\"Southern Region\", color=text_color, fontsize=12)\n",
        "  ax5.axvline(mean_sr, color='salmon', linestyle='--', label=\"Mean\")\n",
        "  ax5.axvline(median_sr, color='mediumaquamarine', linestyle='-', label=\"Median\")\n",
        "  #ax5.axvline(mode_sr, color='lightskyblue', linestyle='-', label=\"Mode\")\n",
        "  ax5.grid(False)\n",
        "  for tick in ax5.get_xticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  for tick in ax5.get_yticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  ax5.tick_params(axis='y',labelsize=8, color=text_color)\n",
        "  legend5 = ax5.legend()\n",
        "  for text in legend5.get_texts():\n",
        "    text.set_color(text_color)\n",
        "  ax5.set(ylabel=None)\n",
        "\n",
        "  ax6.set_anchor('NC')\n",
        "  sns.histplot(data=point_data, ax=ax6, kde=True, bins=range(0,110,10),color='steelblue',edgecolor='lightgrey')\n",
        "  ax6.set_xlabel(f'{compare_element} in NBM {title_dict[element][1]} Percentile Bins', color=text_color, fontsize=12)\n",
        "  ax6.axvline(mean, color='salmon', linestyle='--', label=\"Mean\")\n",
        "  ax6.axvline(median, color='mediumaquamarine', linestyle='-', label=\"Median\")\n",
        "  #ax6.axvline(mode, color='lightskyblue', linestyle='-', label=\"Mode\")\n",
        "  ax6.grid(False)\n",
        "  for tick in ax6.get_xticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  for tick in ax6.get_yticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  ax6.tick_params(axis='y',labelsize=8, color=text_color)\n",
        "  legend6 = ax6.legend()\n",
        "  for text in legend6.get_texts():\n",
        "    text.set_color(text_color)\n",
        "  ax6.set(ylabel=None)\n",
        "\n",
        "  #figname=region_selection+\"_\"+element+\"_\"+valid_date+\".png\"\n",
        "  #plt.savefig(figname, facecolor=fig.get_facecolor(), bbox_inches=None, pad_inches=0.2, dpi='figure')\n",
        "\n",
        "else:\n",
        "  #set up two panel plot\n",
        "  if (region_selection == \"WR\"):\n",
        "      west = -126.917\n",
        "      south = 30.586\n",
        "      east = -102.740\n",
        "      north = 49.755\n",
        "      width, height = (16,9)\n",
        "      width_ratios = [9,8]\n",
        "      lloc = \"lower right\"\n",
        "  if (region_selection == \"CR\"):\n",
        "      west = -111.534\n",
        "      south = 33.295\n",
        "      east = -81.723\n",
        "      north = 49.755\n",
        "      width, height = (16,7)\n",
        "      width_ratios = [9,7]\n",
        "      lloc = \"lower center\"\n",
        "  if (region_selection == \"ER\"):\n",
        "      west = -86.129\n",
        "      south = 31.223\n",
        "      east = -66.465\n",
        "      north = 47.676\n",
        "      width, height = (16,7.25)\n",
        "      width_ratios = [6.9,9.5]\n",
        "      lloc = \"lower right\"\n",
        "  if (region_selection == \"SR\"):\n",
        "      west = -109.758\n",
        "      south = 23.313\n",
        "      east = -79.247\n",
        "      north = 36.899\n",
        "      width, height = (16,5.6)\n",
        "      width_ratios = [10,6]\n",
        "      lloc = \"lower center\"\n",
        "  if (region_selection == \"CWA\"):\n",
        "      west = np.min(obs[region][\"lon\"]) - 0.5\n",
        "      south = np.min(obs[region][\"lat\"]) - 0.5\n",
        "      east = np.max(obs[region][\"lon\"]) + 1.0\n",
        "      north = np.max(obs[region][\"lat\"]) + 0.5\n",
        "      width, height = (16,9)\n",
        "      ratioxy = 16./9.\n",
        "      width_ratios = [ratioxy, 1]\n",
        "      lloc = \"center right\"\n",
        "\n",
        "  #width, height = (16,9)\n",
        "  fig = plt.figure(constrained_layout=True, figsize=(width,height), facecolor=background_color, frameon=True, dpi=150)\n",
        "  if (region_selection == \"CWA\"):\n",
        "    dataframeid = cwa_id\n",
        "  else:\n",
        "    dataframeid = region_selection\n",
        "  #ratioxy = 16./9.\n",
        "  #width_ratios = [ratioxy, 1]\n",
        "  grid = fig.add_gridspec(1,2, hspace=0.2, width_ratios=width_ratios, height_ratios = [1], wspace=0.2)\n",
        "  ax1 = fig.add_subplot(grid[0,0], projection=ccrs.Mercator())\n",
        "  #ax1 = fig.add_subplot(grid[0,0], projection=ccrs.LambertConformal(central_latitude=25, central_longitude=265, standard_parallels=(25,25)))\n",
        "  ax2 = fig.add_subplot(grid[0,1], )\n",
        "  fig.text(0.5, 1.05,f'{dataframeid} {title_dict[element][0]} {compare_element} in NBM v4.1 {title_dict[element][1]} Percentile Space',\\\n",
        "           horizontalalignment='center', verticalalignment='bottom', weight='bold',fontsize=20,color=text_color)\n",
        "  fig.text(0.5, 1.05,f'Valid: {valid_title} | NBM Init: {nbm_init_title} | Points: {points_str}', \\\n",
        "           horizontalalignment='center',verticalalignment='top', fontsize=16,color=text_color)\n",
        "\n",
        "  lats = obs[dataframeid][\"lat\"].values\n",
        "  lons = obs[dataframeid][\"lon\"].values\n",
        "  point_data = obs[dataframeid][compare_var].values\n",
        "  mean = obs[dataframeid][compare_var].mean()\n",
        "  median = obs[dataframeid][compare_var].median()\n",
        "  #mode = obs[dataframeid][compare_var].mode().values[0]\n",
        "  proj = ccrs.PlateCarree()\n",
        "  numcols=(abs(np.amax(point_data) - np.amin(point_data))//10) + 1\n",
        "\n",
        "  ax1.set_anchor('N')\n",
        "  ax1.set_facecolor(background_color)\n",
        "  ax1.set_extent([west, east, south, north], crs=proj)\n",
        "  #ax1.add_feature(cfeature.LAND, edgecolor='none', facecolor='#414143', zorder=-1)\n",
        "  ax1.add_feature(cfeature.OCEAN, edgecolor='none', facecolor=map_water_color, zorder=-2)\n",
        "  ax1.add_feature(cfeature.NaturalEarthFeature('physical', 'land', '50m', edgecolor='none', facecolor=map_land_color, zorder=-1))\n",
        "  #ax1.add_feature(cfeature.LAKES, edgecolor='none', facecolor='#272727', zorder=0)\n",
        "  ax1.add_feature(cfeature.NaturalEarthFeature('physical', 'lakes', '10m', edgecolor='none', facecolor=map_water_color, zorder=0))\n",
        "  ax1.add_feature(cfeature.BORDERS, edgecolor=map_border_color, facecolor='none', linewidth=2, zorder=2)\n",
        "  #ax1.add_feature(cfeature.NaturalEarthFeature('cultural', 'countries', '50m', edgecolor=map_border_color, facecolor='none', linewidth=2, zorder=2))\n",
        "  ax1.add_feature(cfeature.NaturalEarthFeature('cultural', 'admin_1_states_provinces_lines', '50m', edgecolor=map_border_color, facecolor='none', linewidth=1, zorder=5))\n",
        "  #ax1.add_feature(cfeature.NaturalEarthFeature('cultural', 'admin_1_states_provinces_lines', '50m', edgecolor=map_border, facecolor='none', linewidth=1, zorder=5))\n",
        "  #cx.add_basemap(ax1, source='https://server.arcgisonline.com/ArcGIS/rest/services/Canvas/World_Dark_Gray_Base/MapServer/tile/{z}/{y}/{x}', attribution=False)\n",
        "  scatter = ax1.scatter(lons, lats, c= point_data, cmap=cmap, s=45, transform=proj, zorder=2, vmin=0.0, vmax=100.0)\n",
        "\n",
        "  if region_selection in (\"CR\",\"SR\"):\n",
        "    handles, labels = scatter.legend_elements(num=numcols)\n",
        "    legend1 = ax1.legend(flip(handles, 6), flip(labels, 6), ncol=6,loc=lloc, title=f'{compare_element} in NBM Percentile Space', fancybox=True)\n",
        "  else:\n",
        "    legend1 = ax1.legend(*scatter.legend_elements(num=numcols),\n",
        "                      loc=lloc, title=f'{compare_element} \\n Rank', fancybox=True)\n",
        "  plt.setp(legend1.get_title(), multialignment='center', color=text_color)\n",
        "  for text in legend1.get_texts():\n",
        "    text.set_color(text_color)\n",
        "  ax1.add_artist(legend1)\n",
        "  #ax1.set(aspect='equal', adjustable='box')\n",
        "  if cwa_outline:\n",
        "    try:\n",
        "      if os.path.exists(\"shp/w_22mr22.shp\"):\n",
        "        pass\n",
        "      else:\n",
        "        cwa_url = \"https://www.weather.gov/source/gis/Shapefiles/WSOM/w_22mr22.zip\"\n",
        "        os.mkdir(\"shp\")\n",
        "        urlretrieve(cwa_url, \"shp/nws_cwa_outlines.zip\")\n",
        "        #!unzip shp/nws_cwa_outlines.zip -d shp\n",
        "      with zipfile.ZipFile(\"shp/nws_cwa_outlines.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"shp\")\n",
        "        cwa_feature = ShapelyFeature(Reader(\"shp/w_22mr22.shp\").geometries(),ccrs.PlateCarree(), edgecolor='black', facecolor='none', linewidth=1, linestyle='-', zorder=4)\n",
        "        ax1.add_feature(cwa_feature)\n",
        "    except:\n",
        "      print(\"Aw shucks, no CWA boundaries for you. Sorry bout that.\")\n",
        "  if county_outline:\n",
        "    try:\n",
        "      if os.path.exists(\"shp/c_08mr23.shp\"):\n",
        "        pass\n",
        "      else:\n",
        "        county_url = \"https://www.weather.gov/source/gis/Shapefiles/County/c_08mr23.zip\"\n",
        "        if os.path.exists(\"shp/counties.zip\"):\n",
        "          pass\n",
        "        else:\n",
        "          if os.path.exists(\"shp\"):\n",
        "            pass\n",
        "          else:\n",
        "            os.mkdir(\"shp\")\n",
        "          urlretrieve(county_url, \"shp/counties.zip\")\n",
        "          print(\"   >> Downloaded county zip file\")\n",
        "\n",
        "      with zipfile.ZipFile(\"shp/counties.zip\",'r') as cty_ref:\n",
        "        cty_ref.extractall(\"shp\")\n",
        "        print(\"   >> Extracted county shape files\")\n",
        "        cty_feature = ShapelyFeature(Reader(\"shp/c_08mr23.shp\").geometries(),ccrs.PlateCarree(), edgecolor='grey', facecolor='none', linewidth=0.5, linestyle=':', zorder=3)\n",
        "        ax1.add_feature(cty_feature)\n",
        "    except:\n",
        "      print(\"   >> Cannot plot county boundaries.\")\n",
        "\n",
        "  #if region_selection == \"SR\":\n",
        "  #ax2.set(aspect=1)\n",
        "  ax2.set_anchor('C')\n",
        "  sns.histplot(data=obs[dataframeid], x=compare_var, ax=ax2, kde=True, bins=range(-10,115,10),color='steelblue',edgecolor='lightgrey')\n",
        "  ax2.set_xlabel(f'{compare_element} in NBM {title_dict[element][1]} Percentile Bins', color=text_color, fontsize=12)\n",
        "  ax2.axvline(mean, color='salmon', linestyle='--', label=\"Mean\")\n",
        "  ax2.axvline(median, color='mediumaquamarine', linestyle='-', label=\"Median\")\n",
        "\n",
        "  ax2.grid(False)\n",
        "  for tick in ax2.get_xticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  for tick in ax2.get_yticklabels():\n",
        "    tick.set_color(text_color)\n",
        "  ax2.tick_params(axis='y',labelsize=8, color=text_color)\n",
        "  legend2 = ax2.legend()\n",
        "  for text in legend2.get_texts():\n",
        "    text.set_color(text_color)\n",
        "  ax2.set(ylabel=None)\n",
        "\n",
        "\n",
        "figname=\"map_\"+dataframeid+\"_\"+compare_element+\"_\"+element+\"_\"+nbm_init.strftime('%Y%m%d')+\"_\"+valid_end_datetime.strftime('%Y%m%d')+\".png\"\n",
        "plt.savefig(figname, facecolor=fig.get_facecolor(), bbox_inches='tight', pad_inches=0.2, dpi='figure')\n",
        "print(f'   > Done! Saved plot as {figname}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}